{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8595a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ RAPIDS未安裝，使用CPU版本\n",
      "PyTorch CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "GPU Memory: 25.30 GB\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Cell 1: 導入套件（GPU版本）\n",
    "# ===========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# GPU加速的資料處理 (如果有安裝RAPIDS)\n",
    "try:\n",
    "    import cudf\n",
    "    import cupy as cp\n",
    "    from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "    from cuml.linear_model import LogisticRegression as cuLR\n",
    "    from cuml.preprocessing import StandardScaler as cuScaler\n",
    "    from cuml.model_selection import train_test_split as cu_train_test_split\n",
    "    USE_RAPIDS = True\n",
    "    print(\"✅ RAPIDS GPU加速已啟用！\")\n",
    "except ImportError:\n",
    "    USE_RAPIDS = False\n",
    "    print(\"⚠️ RAPIDS未安裝，使用CPU版本\")\n",
    "\n",
    "# GPU加速的模型\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# PyTorch for Neural Networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1a46313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 2: GPU記憶體監控\n",
    "# ===========================\n",
    "def print_gpu_memory():\n",
    "    \"\"\"顯示GPU記憶體使用情況\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU記憶體使用: {torch.cuda.memory_allocated()/1e9:.2f} GB / {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37216f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "載入資料...\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Cell 3: 使用GPU加速的資料處理（RAPIDS）\n",
    "# ===========================\n",
    "def load_data_gpu(file_path, use_rapids=USE_RAPIDS):\n",
    "    \"\"\"使用GPU加速載入和處理資料\"\"\"\n",
    "    print(\"載入資料...\")\n",
    "    \n",
    "    if use_rapids:\n",
    "        # 使用cuDF載入（GPU）\n",
    "        df = cudf.read_csv(file_path)\n",
    "        print(\"✅ 使用cuDF (GPU)載入資料\")\n",
    "    else:\n",
    "        # 使用pandas載入（CPU）\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        print(\"使用pandas (CPU)載入資料\")\n",
    "    \n",
    "    print(f\"資料集大小: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# 執行載入資料\n",
    "file_path = 'us-accidents/US_Accidents_March23.csv'\n",
    "df = load_data_gpu(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd2d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 3: 處理日期時間欄位\n",
    "# ===========================\n",
    "def process_datetime_columns(df):\n",
    "    \"\"\"處理日期時間欄位，解決格式問題\"\"\"\n",
    "    print(\"\\n處理日期時間欄位...\")\n",
    "    \n",
    "    # 使用更寬鬆的日期解析方式\n",
    "    date_columns = ['Start_Time', 'End_Time', 'Weather_Timestamp']\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            print(f\"處理 {col}...\")\n",
    "            # 嘗試多種格式\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', format='mixed')\n",
    "            \n",
    "            # 檢查並報告無法解析的數量\n",
    "            null_count = df[col].isnull().sum()\n",
    "            if null_count > 0:\n",
    "                print(f\"  警告: {col} 有 {null_count} 筆無法解析的日期\")\n",
    "    \n",
    "    # 計算持續時間（分鐘）\n",
    "    if 'Start_Time' in df.columns and 'End_Time' in df.columns:\n",
    "        df['Duration_minutes'] = (df['End_Time'] - df['Start_Time']).dt.total_seconds() / 60\n",
    "        \n",
    "        # 過濾異常的持續時間\n",
    "        print(f\"\\n持續時間統計:\")\n",
    "        print(f\"  最小值: {df['Duration_minutes'].min():.2f} 分鐘\")\n",
    "        print(f\"  最大值: {df['Duration_minutes'].max():.2f} 分鐘\")\n",
    "        print(f\"  平均值: {df['Duration_minutes'].mean():.2f} 分鐘\")\n",
    "        print(f\"  中位數: {df['Duration_minutes'].median():.2f} 分鐘\")\n",
    "        \n",
    "        # 移除持續時間異常的記錄\n",
    "        original_len = len(df)\n",
    "        df = df[(df['Duration_minutes'] > 0) & (df['Duration_minutes'] < 1440)]  # 小於24小時\n",
    "        print(f\"\\n移除異常持續時間後，資料從 {original_len} 筆減少到 {len(df)} 筆\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 執行日期處理\n",
    "df = process_datetime_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51de2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 4: 特徵工程\n",
    "# ===========================\n",
    "def feature_engineering(df):\n",
    "    \"\"\"特徵工程：創建新的特徵\"\"\"\n",
    "    print(\"\\n執行特徵工程...\")\n",
    "    \n",
    "    # 檢查Start_Time是否存在且不為null\n",
    "    if 'Start_Time' in df.columns:\n",
    "        # 移除Start_Time為null的記錄\n",
    "        df = df[df['Start_Time'].notna()]\n",
    "        \n",
    "        # 時間特徵\n",
    "        df['Hour'] = df['Start_Time'].dt.hour\n",
    "        df['DayOfWeek'] = df['Start_Time'].dt.dayofweek\n",
    "        df['Month'] = df['Start_Time'].dt.month\n",
    "        df['Year'] = df['Start_Time'].dt.year\n",
    "        df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)\n",
    "        \n",
    "        # 時段分類\n",
    "        df['TimeOfDay'] = pd.cut(df['Hour'], \n",
    "                                 bins=[-1, 6, 12, 18, 24], \n",
    "                                 labels=['Night', 'Morning', 'Afternoon', 'Evening'])\n",
    "        \n",
    "        # 季節\n",
    "        df['Season'] = pd.cut(df['Month'], \n",
    "                              bins=[0, 3, 6, 9, 12], \n",
    "                              labels=['Winter', 'Spring', 'Summer', 'Fall'])\n",
    "    \n",
    "    # 天氣條件簡化\n",
    "    if 'Weather_Condition' in df.columns:\n",
    "        weather_keywords = {\n",
    "            'Clear': ['Clear', 'Fair'],\n",
    "            'Cloudy': ['Cloud', 'Overcast'],\n",
    "            'Rain': ['Rain', 'Drizzle', 'Shower'],\n",
    "            'Snow': ['Snow', 'Sleet', 'Hail'],\n",
    "            'Fog': ['Fog', 'Mist'],\n",
    "            'Storm': ['Storm', 'Thunder']\n",
    "        }\n",
    "        \n",
    "        def categorize_weather(condition):\n",
    "            if pd.isna(condition):\n",
    "                return 'Unknown'\n",
    "            condition = str(condition)\n",
    "            for category, keywords in weather_keywords.items():\n",
    "                if any(keyword in condition for keyword in keywords):\n",
    "                    return category\n",
    "            return 'Other'\n",
    "        \n",
    "        df['Weather_Category'] = df['Weather_Condition'].apply(categorize_weather)\n",
    "    \n",
    "    print(f\"特徵工程完成，目前資料集大小: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 執行特徵工程\n",
    "df = feature_engineering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62346784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 5: 缺失值分析與處理\n",
    "# ===========================\n",
    "def analyze_missing_values(df):\n",
    "    \"\"\"分析缺失值\"\"\"\n",
    "    print(\"\\n分析缺失值...\")\n",
    "    \n",
    "    # 計算每個欄位的缺失值\n",
    "    missing_stats = df.isnull().sum()\n",
    "    missing_percentage = (missing_stats / len(df)) * 100\n",
    "    \n",
    "    # 建立缺失值報告\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': missing_stats.index,\n",
    "        'Missing_Count': missing_stats.values,\n",
    "        'Percentage': missing_percentage.values\n",
    "    })\n",
    "    \n",
    "    # 只顯示有缺失值的欄位\n",
    "    missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Percentage', ascending=False)\n",
    "    \n",
    "    print(\"\\n缺失值統計（前20個）:\")\n",
    "    print(missing_df.head(20))\n",
    "    \n",
    "    # 視覺化缺失值\n",
    "    if len(missing_df) > 0:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(missing_df.head(20)['Column'], missing_df.head(20)['Percentage'])\n",
    "        plt.xlabel('Missing Percentage (%)')\n",
    "        plt.title('Top 20 Columns with Missing Values')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "# 分析缺失值\n",
    "missing_df = analyze_missing_values(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec73c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 6: 處理缺失值\n",
    "# ===========================\n",
    "def handle_missing_values(df, missing_threshold=60):\n",
    "    \"\"\"處理缺失值\"\"\"\n",
    "    print(f\"\\n處理缺失值（閾值: {missing_threshold}%）...\")\n",
    "    \n",
    "    # 計算缺失值百分比\n",
    "    missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "    \n",
    "    # 刪除缺失值比例過高的欄位\n",
    "    high_missing_cols = missing_percentage[missing_percentage > missing_threshold].index.tolist()\n",
    "    \n",
    "    if len(high_missing_cols) > 0:\n",
    "        print(f\"\\n刪除高缺失率欄位 ({len(high_missing_cols)} 個):\")\n",
    "        print(high_missing_cols[:10])  # 只顯示前10個\n",
    "        df = df.drop(columns=high_missing_cols)\n",
    "    \n",
    "    # 對剩餘的缺失值進行填補\n",
    "    # 數值型欄位\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "    # 類別型欄位\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna(df[col].mode()[0] if len(df[col].mode()) > 0 else 'Unknown', inplace=True)\n",
    "    \n",
    "    print(f\"\\n處理後資料集大小: {df.shape}\")\n",
    "    print(f\"剩餘缺失值總數: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 處理缺失值\n",
    "df = handle_missing_values(df, missing_threshold=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb7b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 7: 特徵選擇與準備\n",
    "# ===========================\n",
    "def select_features(df):\n",
    "    \"\"\"選擇模型訓練所需的特徵\"\"\"\n",
    "    print(\"\\n選擇特徵...\")\n",
    "    \n",
    "    # 定義要使用的特徵\n",
    "    numeric_features = [\n",
    "        'Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)',\n",
    "        'Wind_Speed(mph)', 'Distance(mi)', 'Hour', 'DayOfWeek', 'Month', \n",
    "        'Year', 'IsWeekend'\n",
    "    ]\n",
    "    \n",
    "    categorical_features = [\n",
    "        'Side', 'State', 'Weather_Category', 'TimeOfDay', 'Season',\n",
    "        'Sunrise_Sunset'\n",
    "    ]\n",
    "    \n",
    "    boolean_features = [\n",
    "        'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit',\n",
    "        'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', \n",
    "        'Traffic_Signal'\n",
    "    ]\n",
    "    \n",
    "    # 如果有Duration_minutes，也加入\n",
    "    if 'Duration_minutes' in df.columns:\n",
    "        numeric_features.append('Duration_minutes')\n",
    "    \n",
    "    # 確保特徵存在於資料集中\n",
    "    numeric_features = [f for f in numeric_features if f in df.columns]\n",
    "    categorical_features = [f for f in categorical_features if f in df.columns]\n",
    "    boolean_features = [f for f in boolean_features if f in df.columns]\n",
    "    \n",
    "    print(f\"\\n選擇的特徵:\")\n",
    "    print(f\"  數值型特徵: {len(numeric_features)} 個\")\n",
    "    print(f\"  類別型特徵: {len(categorical_features)} 個\")  \n",
    "    print(f\"  布林型特徵: {len(boolean_features)} 個\")\n",
    "    \n",
    "    # 將布林值轉換為整數\n",
    "    for col in boolean_features:\n",
    "        df[col] = df[col].astype(int)\n",
    "    \n",
    "    # 對類別變數進行編碼\n",
    "    label_encoders = {}\n",
    "    encoded_features = numeric_features + boolean_features\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        encoded_col_name = col + '_encoded'\n",
    "        df[encoded_col_name] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        encoded_features.append(encoded_col_name)\n",
    "    \n",
    "    print(f\"\\n總特徵數: {len(encoded_features)}\")\n",
    "    \n",
    "    return df, encoded_features, label_encoders\n",
    "\n",
    "# 選擇特徵\n",
    "df, selected_features, label_encoders = select_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3684da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 8: 準備訓練資料\n",
    "# ===========================\n",
    "def prepare_training_data(df, features, target_col='Severity'):\n",
    "    \"\"\"準備訓練資料\"\"\"\n",
    "    print(\"\\n準備訓練資料...\")\n",
    "    \n",
    "    # 確保沒有缺失值\n",
    "    df_clean = df[features + [target_col]].dropna()\n",
    "    \n",
    "    print(f\"清理後的資料大小: {df_clean.shape}\")\n",
    "    \n",
    "    # 準備特徵和目標變數\n",
    "    X = df_clean[features]\n",
    "    y = df_clean[target_col]\n",
    "    \n",
    "    # 將嚴重度轉換為從0開始的索引（sklearn要求）\n",
    "    y = y - 1\n",
    "    \n",
    "    # 顯示類別分布\n",
    "    print(\"\\n調整後的目標變數分布:\")\n",
    "    print(pd.Series(y).value_counts().sort_index())\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 準備資料\n",
    "X, y = prepare_training_data(df, selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6662385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 9: 處理資料不平衡\n",
    "# ===========================\n",
    "def handle_imbalanced_data(X, y, method='smote'):\n",
    "    \"\"\"處理資料不平衡問題\"\"\"\n",
    "    print(f\"\\n使用 {method} 處理資料不平衡...\")\n",
    "    \n",
    "    print(\"原始類別分布:\")\n",
    "    print(pd.Series(y).value_counts().sort_index())\n",
    "    \n",
    "    if method == 'smote':\n",
    "        # 使用SMOTE過採樣\n",
    "        smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    elif method == 'undersample':\n",
    "        # 使用隨機欠採樣\n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "    elif method == 'combine':\n",
    "        # 結合過採樣和欠採樣\n",
    "        smote_enn = SMOTEENN(random_state=42)\n",
    "        X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "    else:\n",
    "        # 不處理\n",
    "        X_resampled, y_resampled = X, y\n",
    "    \n",
    "    print(\"\\n處理後類別分布:\")\n",
    "    print(pd.Series(y_resampled).value_counts().sort_index())\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# 分割資料集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"訓練集大小:\", X_train.shape)\n",
    "print(\"測試集大小:\", X_test.shape)\n",
    "\n",
    "# 處理訓練集的不平衡\n",
    "X_train_balanced, y_train_balanced = handle_imbalanced_data(X_train, y_train, method='smote')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 9: GPU加速的模型訓練\n",
    "# ===========================\n",
    "\n",
    "# 1. XGBoost GPU版本\n",
    "def train_xgboost_gpu(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"使用XGBoost GPU版本訓練\"\"\"\n",
    "    print(\"\\n訓練 XGBoost (GPU)...\")\n",
    "    \n",
    "    # 轉換為DMatrix格式（XGBoost的優化格式）\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    # GPU參數設定\n",
    "    params = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 4,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 100,\n",
    "        'tree_method': 'gpu_hist',  # GPU加速\n",
    "        'predictor': 'gpu_predictor',  # GPU預測\n",
    "        'gpu_id': 0,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # 訓練模型\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 預測\n",
    "    y_pred_proba = model.predict(dtest)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # 評估\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"  訓練時間: {train_time:.2f} 秒\")\n",
    "    print(f\"  準確率: {accuracy:.4f}\")\n",
    "    print(f\"  F1分數: {f1:.4f}\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    return model, accuracy, f1\n",
    "\n",
    "# 2. LightGBM GPU版本\n",
    "def train_lightgbm_gpu(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"使用LightGBM GPU版本訓練\"\"\"\n",
    "    print(\"\\n訓練 LightGBM (GPU)...\")\n",
    "    \n",
    "    # GPU參數設定\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 4,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'multi_logloss',\n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 100,\n",
    "        'random_state': 42,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # 訓練模型\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 預測\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 評估\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"  訓練時間: {train_time:.2f} 秒\")\n",
    "    print(f\"  準確率: {accuracy:.4f}\")\n",
    "    print(f\"  F1分數: {f1:.4f}\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    return model, accuracy, f1\n",
    "\n",
    "# 3. CatBoost GPU版本\n",
    "def train_catboost_gpu(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"使用CatBoost GPU版本訓練\"\"\"\n",
    "    print(\"\\n訓練 CatBoost (GPU)...\")\n",
    "    \n",
    "    # GPU參數設定\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=100,\n",
    "        learning_rate=0.1,\n",
    "        depth=6,\n",
    "        loss_function='MultiClass',\n",
    "        task_type='GPU',\n",
    "        devices='0',\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # 訓練模型\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 預測\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 評估\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"  訓練時間: {train_time:.2f} 秒\")\n",
    "    print(f\"  準確率: {accuracy:.4f}\")\n",
    "    print(f\"  F1分數: {f1:.4f}\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    return model, accuracy, f1\n",
    "\n",
    "# 4. PyTorch神經網路 (GPU)\n",
    "class AccidentSeverityNN(nn.Module):\n",
    "    \"\"\"事故嚴重度預測神經網路\"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes=[256, 128, 64], num_classes=4, dropout_rate=0.3):\n",
    "        super(AccidentSeverityNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_pytorch_gpu(X_train, X_test, y_train, y_test, epochs=50, batch_size=1024):\n",
    "    \"\"\"使用PyTorch GPU訓練深度神經網路\"\"\"\n",
    "    print(\"\\n訓練 PyTorch Neural Network (GPU)...\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"使用設備: {device}\")\n",
    "    \n",
    "    # 資料標準化\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 轉換為PyTorch張量\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    y_test_tensor = torch.LongTensor(y_test).to(device)\n",
    "    \n",
    "    # 建立資料載入器\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # 建立模型\n",
    "    input_size = X_train.shape[1]\n",
    "    model = AccidentSeverityNN(input_size).to(device)\n",
    "    \n",
    "    # 損失函數和優化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "    \n",
    "    # 訓練\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"  Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "            scheduler.step(avg_loss)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 評估\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred = predicted.cpu().numpy()\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"  訓練時間: {train_time:.2f} 秒\")\n",
    "    print(f\"  準確率: {accuracy:.4f}\")\n",
    "    print(f\"  F1分數: {f1:.4f}\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    return model, scaler, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb8005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 10: RAPIDS GPU加速的隨機森林\n",
    "# ===========================\n",
    "def train_rapids_rf(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"使用RAPIDS cuML的GPU隨機森林\"\"\"\n",
    "    if not USE_RAPIDS:\n",
    "        print(\"RAPIDS未安裝，跳過此模型\")\n",
    "        return None, 0, 0\n",
    "    \n",
    "    print(\"\\n訓練 Random Forest (RAPIDS GPU)...\")\n",
    "    \n",
    "    # 轉換為GPU陣列\n",
    "    X_train_gpu = cp.asarray(X_train)\n",
    "    y_train_gpu = cp.asarray(y_train)\n",
    "    X_test_gpu = cp.asarray(X_test)\n",
    "    \n",
    "    # 建立GPU隨機森林\n",
    "    model = cuRF(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 訓練\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_gpu, y_train_gpu)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 預測\n",
    "    y_pred_gpu = model.predict(X_test_gpu)\n",
    "    y_pred = cp.asnumpy(y_pred_gpu)\n",
    "    \n",
    "    # 評估\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"  訓練時間: {train_time:.2f} 秒\")\n",
    "    print(f\"  準確率: {accuracy:.4f}\")\n",
    "    print(f\"  F1分數: {f1:.4f}\")\n",
    "    \n",
    "    return model, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ddc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 11: 主要執行函數\n",
    "# ===========================\n",
    "def main_gpu_training(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"執行所有GPU加速的模型訓練\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. XGBoost GPU\n",
    "    xgb_model, xgb_acc, xgb_f1 = train_xgboost_gpu(X_train, X_test, y_train, y_test)\n",
    "    results['XGBoost_GPU'] = {'accuracy': xgb_acc, 'f1': xgb_f1}\n",
    "    \n",
    "    # 2. LightGBM GPU\n",
    "    lgb_model, lgb_acc, lgb_f1 = train_lightgbm_gpu(X_train, X_test, y_train, y_test)\n",
    "    results['LightGBM_GPU'] = {'accuracy': lgb_acc, 'f1': lgb_f1}\n",
    "    \n",
    "    # 3. CatBoost GPU\n",
    "    cat_model, cat_acc, cat_f1 = train_catboost_gpu(X_train, X_test, y_train, y_test)\n",
    "    results['CatBoost_GPU'] = {'accuracy': cat_acc, 'f1': cat_f1}\n",
    "    \n",
    "    # 4. PyTorch Neural Network\n",
    "    nn_model, nn_scaler, nn_acc, nn_f1 = train_pytorch_gpu(X_train, X_test, y_train, y_test)\n",
    "    results['PyTorch_NN'] = {'accuracy': nn_acc, 'f1': nn_f1}\n",
    "    \n",
    "    # 5. RAPIDS Random Forest (如果可用)\n",
    "    if USE_RAPIDS:\n",
    "        rf_model, rf_acc, rf_f1 = train_rapids_rf(X_train, X_test, y_train, y_test)\n",
    "        results['RAPIDS_RF'] = {'accuracy': rf_acc, 'f1': rf_f1}\n",
    "    \n",
    "    # 顯示結果比較\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"模型性能比較（GPU加速版）:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"{model_name:15} - 準確率: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}\")\n",
    "    \n",
    "    # 找出最佳模型\n",
    "    best_model = max(results.items(), key=lambda x: x[1]['accuracy'])\n",
    "    print(f\"\\n最佳模型: {best_model[0]} (準確率: {best_model[1]['accuracy']:.4f})\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 12: 特徵重要性分析\n",
    "# ===========================\n",
    "def analyze_feature_importance(model, feature_names, top_n=20):\n",
    "    \"\"\"分析特徵重要性\"\"\"\n",
    "    print(\"\\n分析特徵重要性...\")\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # 顯示前N個重要特徵\n",
    "        print(f\"\\nTop {top_n} 重要特徵:\")\n",
    "        print(importances.head(top_n))\n",
    "        \n",
    "        # 視覺化\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(importances.head(top_n)['feature'][::-1], \n",
    "                importances.head(top_n)['importance'][::-1])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'Top {top_n} Feature Importances')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return importances\n",
    "    else:\n",
    "        print(\"此模型不支援特徵重要性分析\")\n",
    "        return None\n",
    "\n",
    "# 如果最佳模型是樹模型，分析特徵重要性\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "    feature_importance = analyze_feature_importance(best_model, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513bff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 13: 超參數調整（選擇性執行）\n",
    "# ===========================\n",
    "def hyperparameter_tuning(X_train, y_train, model_type='rf'):\n",
    "    \"\"\"使用網格搜索進行超參數調整\"\"\"\n",
    "    print(f\"\\n進行{model_type}超參數調整（這可能需要一些時間）...\")\n",
    "    \n",
    "    if model_type == 'rf':\n",
    "        # Random Forest 參數網格\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 15, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "        model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    elif model_type == 'gb':\n",
    "        # Gradient Boosting 參數網格\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "        model = GradientBoostingClassifier(random_state=42)\n",
    "    \n",
    "    # 使用分層k折交叉驗證\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grid, \n",
    "        cv=cv, \n",
    "        scoring='f1_weighted',\n",
    "        verbose=1, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\n最佳參數: {grid_search.best_params_}\")\n",
    "    print(f\"最佳交叉驗證分數: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# 如果需要，可以執行超參數調整\n",
    "# best_rf_tuned = hyperparameter_tuning(X_train_balanced, y_train_balanced, model_type='rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e194fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 14: 儲存模型\n",
    "# ===========================\n",
    "import joblib\n",
    "\n",
    "def save_model(model, scaler, label_encoders, feature_names, output_dir='./model_output/'):\n",
    "    \"\"\"儲存模型和相關物件\"\"\"\n",
    "    import os\n",
    "    \n",
    "    # 建立輸出目錄\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 儲存模型\n",
    "    joblib.dump(model, f'{output_dir}best_model.pkl')\n",
    "    \n",
    "    # 儲存標準化器\n",
    "    joblib.dump(scaler, f'{output_dir}scaler.pkl')\n",
    "    \n",
    "    # 儲存標籤編碼器\n",
    "    joblib.dump(label_encoders, f'{output_dir}label_encoders.pkl')\n",
    "    \n",
    "    # 儲存特徵名稱\n",
    "    joblib.dump(feature_names, f'{output_dir}feature_names.pkl')\n",
    "    \n",
    "    print(f\"模型已儲存到 {output_dir}\")\n",
    "\n",
    "# 儲存最佳模型\n",
    "save_model(best_model, scaler, label_encoders, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a33a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 15: 預測函數\n",
    "# ===========================\n",
    "def predict_severity(model, scaler, label_encoders, feature_names, input_data):\n",
    "    \"\"\"使用訓練好的模型進行預測\"\"\"\n",
    "    \n",
    "    # 確保輸入資料包含所有必要的特徵\n",
    "    for feature in feature_names:\n",
    "        if feature not in input_data:\n",
    "            print(f\"警告: 缺少特徵 {feature}\")\n",
    "            return None\n",
    "    \n",
    "    # 準備特徵向量\n",
    "    X_new = pd.DataFrame([input_data])[feature_names]\n",
    "    \n",
    "    # 如果模型需要標準化\n",
    "    if isinstance(model, (LogisticRegression, MLPClassifier)):\n",
    "        X_new = scaler.transform(X_new)\n",
    "    \n",
    "    # 進行預測\n",
    "    prediction = model.predict(X_new)[0]\n",
    "    prediction_proba = model.predict_proba(X_new)[0]\n",
    "    \n",
    "    # 將預測結果轉換回原始的嚴重度級別\n",
    "    severity_level = prediction + 1\n",
    "    \n",
    "    print(f\"\\n預測結果:\")\n",
    "    print(f\"嚴重度級別: {severity_level}\")\n",
    "    print(f\"各級別機率:\")\n",
    "    for i, prob in enumerate(prediction_proba):\n",
    "        print(f\"  級別 {i+1}: {prob:.4f}\")\n",
    "    \n",
    "    return severity_level, prediction_proba\n",
    "\n",
    "# 範例預測（需要根據實際特徵調整）\n",
    "# example_input = {\n",
    "#     'Temperature(F)': 70.0,\n",
    "#     'Humidity(%)': 80.0,\n",
    "#     # ... 其他特徵\n",
    "# }\n",
    "# predict_severity(best_model, scaler, label_encoders, selected_features, example_input)\n",
    "\n",
    "print(\"\\n模型訓練完成！\")\n",
    "print(\"您可以使用 predict_severity 函數進行新的預測。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
