{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b086c8e",
   "metadata": {},
   "source": [
    "### US Accidents 資料分析：模型比較與時空風險預測\n",
    "## 實驗設計\n",
    "1. 比較有無資料前處理的影響\n",
    "2. 比較有無混合採樣策略的影響\n",
    "3. 使用三個模型：LightGBM, XGBoost, CatBoost（GPU加速版）\n",
    "4. 包含交叉驗證和進度顯示\n",
    "5. 創建時空風險預測數據供 Kepler.gl 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f029f3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "環境檢查\n",
      "============================================================\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "GPU Memory: 23.56 GB\n",
      "XGBoost version: 3.0.2\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Cell 1: 導入套件和設定\n",
    "# ===========================\n",
    "import os, time, json, gc, warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# Imbalanced-learn（保留混合採樣）\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# 只保留 XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU 檢查\n",
    "print(\"=\"*60)\n",
    "print(\"環境檢查\")\n",
    "print(\"=\"*60)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da381a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 2: 記憶體優化函數\n",
    "# ===========================\n",
    "\n",
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"通過改變數據類型來減少DataFrame的記憶體使用\"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f'記憶體使用減少了 {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "        print(f'{start_mem:.2f} MB --> {end_mem:.2f} MB')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_memory():\n",
    "    \"\"\"清理記憶體\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739ddefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "載入資料: us-accidents/US_Accidents_March23.csv\n",
      "載入 100% 的資料...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "載入資料大小: (7728394, 27)\n",
      "記憶體使用: 1031.85 MB\n",
      "\n",
      "目標變數分布:\n",
      "Severity 1: 67,366 (0.87%)\n",
      "Severity 2: 6,156,981 (79.67%)\n",
      "Severity 3: 1,299,337 (16.81%)\n",
      "Severity 4: 204,710 (2.65%)\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Cell 3: 載入資料（優化版）\n",
    "# ===========================\n",
    "\n",
    "def load_data_optimized(file_path, sample_frac=1):  # 使用10%資料做實驗\n",
    "    \"\"\"優化的資料載入\"\"\"\n",
    "    print(f\"\\n載入資料: {file_path}\")\n",
    "    \n",
    "    # 定義需要的欄位\n",
    "    important_cols = [\n",
    "        'Severity', 'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng',\n",
    "        'Distance(mi)', 'Temperature(F)', 'Humidity(%)', 'Pressure(in)',\n",
    "        'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)',\n",
    "        'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way',\n",
    "        'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop',\n",
    "        'Traffic_Calming', 'Traffic_Signal', 'Sunrise_Sunset', 'State'\n",
    "    ]\n",
    "    \n",
    "    # 載入資料\n",
    "    print(f\"載入 {sample_frac*100}% 的資料...\")\n",
    "    df = pd.read_csv(file_path, usecols=lambda x: x in important_cols)\n",
    "    df = df.sample(frac=sample_frac, random_state=42)\n",
    "    \n",
    "    print(f\"載入資料大小: {df.shape}\")\n",
    "    print(f\"記憶體使用: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # 顯示目標變數分布\n",
    "    print(\"\\n目標變數分布:\")\n",
    "    severity_counts = df['Severity'].value_counts().sort_index()\n",
    "    for sev, count in severity_counts.items():\n",
    "        print(f\"Severity {sev}: {count:,} ({count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 執行載入\n",
    "file_path = 'us-accidents/US_Accidents_March23.csv'\n",
    "df = load_data_optimized(file_path, sample_frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e1219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 4: 基礎特徵工程函數\n",
    "# ===========================\n",
    "\n",
    "def basic_preprocessing(df):\n",
    "    \"\"\"基礎前處理：只處理缺失值和基本轉換\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # 處理日期時間\n",
    "    df_copy['Start_Time'] = pd.to_datetime(df_copy['Start_Time'], errors='coerce')\n",
    "    df_copy['End_Time'] = pd.to_datetime(df_copy['End_Time'], errors='coerce')\n",
    "    \n",
    "    # 計算持續時間\n",
    "    df_copy['Duration_minutes'] = (df_copy['End_Time'] - df_copy['Start_Time']).dt.total_seconds() / 60\n",
    "    \n",
    "    # 過濾異常值\n",
    "    df_copy = df_copy[(df_copy['Duration_minutes'] > 0) & (df_copy['Duration_minutes'] < 1440*7)]\n",
    "    df_copy = df_copy.dropna(subset=['Start_Time'])\n",
    "    \n",
    "    # 提取基本時間特徵\n",
    "    df_copy['Hour'] = df_copy['Start_Time'].dt.hour\n",
    "    df_copy['DayOfWeek'] = df_copy['Start_Time'].dt.dayofweek\n",
    "    df_copy['Month'] = df_copy['Start_Time'].dt.month\n",
    "    \n",
    "    # 處理缺失值（簡單填充）\n",
    "    numeric_cols = df_copy.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col != 'Severity':\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
    "    \n",
    "    # 類別變數填充\n",
    "    categorical_cols = ['Weather_Condition', 'State', 'Sunrise_Sunset']\n",
    "    for col in categorical_cols:\n",
    "        if col in df_copy.columns:\n",
    "            df_copy[col] = df_copy[col].fillna('Unknown')\n",
    "    \n",
    "    # 布林型欄位轉換\n",
    "    bool_cols = df_copy.select_dtypes(include=['bool']).columns\n",
    "    for col in bool_cols:\n",
    "        df_copy[col] = df_copy[col].astype(int)\n",
    "    \n",
    "    # 刪除不需要的欄位\n",
    "    df_copy = df_copy.drop(['Start_Time', 'End_Time'], axis=1, errors='ignore')\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def advanced_preprocessing(df):\n",
    "    \"\"\"進階前處理：包含所有特徵工程\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # 先做基礎處理\n",
    "    df_copy = basic_preprocessing(df_copy)\n",
    "    \n",
    "    # 額外的特徵工程\n",
    "    # 1. 是否週末\n",
    "    df_copy['IsWeekend'] = (df_copy['DayOfWeek'] >= 5).astype(int)\n",
    "    \n",
    "    # 2. 是否尖峰時段\n",
    "    df_copy['IsRushHour'] = df_copy['Hour'].apply(\n",
    "        lambda x: 1 if (6 <= x <= 9) or (16 <= x <= 19) else 0\n",
    "    )\n",
    "    \n",
    "    # 3. 時段分類\n",
    "    df_copy['TimeOfDay'] = pd.cut(df_copy['Hour'], \n",
    "                                  bins=[-1, 6, 12, 18, 24], \n",
    "                                  labels=[0, 1, 2, 3]).astype(int)\n",
    "    \n",
    "    # 4. 季節\n",
    "    df_copy['Season'] = pd.cut(df_copy['Month'], \n",
    "                               bins=[0, 3, 6, 9, 12], \n",
    "                               labels=[0, 1, 2, 3]).astype(int)\n",
    "    \n",
    "    # 5. 天氣分類（如果有天氣條件）\n",
    "    if 'Weather_Condition' in df_copy.columns:\n",
    "        def categorize_weather(condition):\n",
    "            if pd.isna(condition):\n",
    "                return 0\n",
    "            condition = str(condition).lower()\n",
    "            if any(word in condition for word in ['clear', 'fair']):\n",
    "                return 1\n",
    "            elif any(word in condition for word in ['cloud', 'overcast']):\n",
    "                return 2\n",
    "            elif any(word in condition for word in ['rain', 'drizzle']):\n",
    "                return 3\n",
    "            elif any(word in condition for word in ['snow', 'sleet']):\n",
    "                return 4\n",
    "            elif any(word in condition for word in ['fog', 'mist']):\n",
    "                return 5\n",
    "            elif any(word in condition for word in ['storm', 'thunder']):\n",
    "                return 6\n",
    "            else:\n",
    "                return 7\n",
    "        \n",
    "        df_copy['Weather_Category'] = df_copy['Weather_Condition'].apply(categorize_weather)\n",
    "        df_copy = df_copy.drop('Weather_Condition', axis=1)\n",
    "    \n",
    "    # 6. 對類別變數進行標籤編碼\n",
    "    label_encoders = {}\n",
    "    categorical_cols = ['State', 'Sunrise_Sunset']\n",
    "    for col in categorical_cols:\n",
    "        if col in df_copy.columns:\n",
    "            le = LabelEncoder()\n",
    "            df_copy[col] = le.fit_transform(df_copy[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    return df_copy, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "217b93e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 5: 混合採樣策略\n",
    "# ===========================\n",
    "\n",
    "def apply_mixed_sampling(X_train, y_train):\n",
    "    \"\"\"應用混合採樣策略\"\"\"\n",
    "    print(\"\\n應用混合採樣策略...\")\n",
    "    \n",
    "    # 計算各類別數量\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    class_counts = dict(zip(unique, counts))\n",
    "    print(\"原始分布:\", class_counts)\n",
    "    \n",
    "    # 混合策略：對多數類欠採樣，對少數類過採樣\n",
    "    median_count = int(np.median(counts))\n",
    "    target_count = int(median_count * 1.5)\n",
    "    \n",
    "    # 第一步：欠採樣\n",
    "    undersample_strategy = {}\n",
    "    for cls, cnt in class_counts.items():\n",
    "        if cnt > target_count:\n",
    "            undersample_strategy[cls] = target_count\n",
    "        else:\n",
    "            undersample_strategy[cls] = cnt\n",
    "    \n",
    "    if len(undersample_strategy) > 0:\n",
    "        rus = RandomUnderSampler(sampling_strategy=undersample_strategy, random_state=42)\n",
    "        X_temp, y_temp = rus.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        X_temp, y_temp = X_train, y_train\n",
    "    \n",
    "    # 第二步：過採樣\n",
    "    temp_unique, temp_counts = np.unique(y_temp, return_counts=True)\n",
    "    temp_class_counts = dict(zip(temp_unique, temp_counts))\n",
    "    \n",
    "    oversample_strategy = {}\n",
    "    for cls, cnt in temp_class_counts.items():\n",
    "        if cnt < target_count:\n",
    "            oversample_strategy[cls] = target_count\n",
    "        else:\n",
    "            oversample_strategy[cls] = cnt\n",
    "    \n",
    "    if len(oversample_strategy) > 0:\n",
    "        ros = RandomOverSampler(sampling_strategy=oversample_strategy, random_state=42)\n",
    "        X_resampled, y_resampled = ros.fit_resample(X_temp, y_temp)\n",
    "    else:\n",
    "        X_resampled, y_resampled = X_temp, y_temp\n",
    "    \n",
    "    # 顯示新分布\n",
    "    unique_new, counts_new = np.unique(y_resampled, return_counts=True)\n",
    "    print(\"採樣後分布:\", dict(zip(unique_new, counts_new)))\n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaddd219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 6: XGBoost GPU 訓練器\n",
    "# ===========================\n",
    "def train_xgboost_gpu(\n",
    "        X_train, X_test, y_train, y_test,\n",
    "        X_val=None, y_val=None,\n",
    "        *,\n",
    "        objective='multi:softprob',\n",
    "        num_class=4\n",
    "    ):\n",
    "    \"\"\"\n",
    "    通用 XGBoost GPU 訓練器\n",
    "      - 多分類: objective='multi:softprob'，num_class=類別總數\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'objective': objective,\n",
    "        # ==== 3090 GPU 最佳化 ====\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'predictor':   'gpu_predictor',\n",
    "        'gpu_id': 0,\n",
    "        'max_bin': 256,\n",
    "        'sampling_method': 'gradient_based',\n",
    "        # ==== 常用超參 ====\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.08,\n",
    "        'n_estimators': 2000,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 1,\n",
    "        'gamma': 0.15,\n",
    "        'lambda': 1.0,\n",
    "        'alpha': 0.0,\n",
    "        'n_jobs': os.cpu_count()\n",
    "    }\n",
    "\n",
    "    # 只有在多分類時才加入 num_class & eval_metric\n",
    "    if objective.startswith('multi'):\n",
    "        params['num_class'] = num_class\n",
    "        params['eval_metric'] = ['mlogloss', 'merror']\n",
    "\n",
    "    Model = xgb.XGBClassifier\n",
    "    model = Model(**params)\n",
    "\n",
    "    eval_set = [(X_test, y_test)]\n",
    "    if X_val is not None:\n",
    "        eval_set.append((X_val, y_val))\n",
    "\n",
    "    start = time.time()\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=eval_set,\n",
    "        # early_stopping_rounds=80,\n",
    "        verbose=200\n",
    "    )\n",
    "    train_time = time.time() - start\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    return model, preds, train_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43aa7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 7: 交叉驗證函數\n",
    "# ===========================\n",
    "\n",
    "def cross_validate_model(model_func, X, y, cv_folds=5):\n",
    "    \"\"\"執行交叉驗證\"\"\"\n",
    "    print(f\"\\n執行 {cv_folds} 折交叉驗證...\")\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    cv_scores = {\n",
    "        'accuracy': [],\n",
    "        'f1_score': [],\n",
    "        'balanced_accuracy': []\n",
    "    }\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tqdm(skf.split(X, y), total=cv_folds, desc=\"CV Progress\")):\n",
    "        X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
    "        y_train_cv, y_val_cv = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # 訓練模型\n",
    "        model, y_pred, _ = model_func(X_train_cv, X_val_cv, y_train_cv, y_val_cv)\n",
    "        \n",
    "        # 計算指標\n",
    "        cv_scores['accuracy'].append(accuracy_score(y_val_cv, y_pred))\n",
    "        cv_scores['f1_score'].append(f1_score(y_val_cv, y_pred, average='weighted'))\n",
    "        cv_scores['balanced_accuracy'].append(balanced_accuracy_score(y_val_cv, y_pred))\n",
    "        \n",
    "        # 清理GPU記憶體\n",
    "        clean_memory()\n",
    "    \n",
    "    # 計算平均值和標準差\n",
    "    results = {}\n",
    "    for metric, scores in cv_scores.items():\n",
    "        results[f'{metric}_mean'] = np.mean(scores)\n",
    "        results[f'{metric}_std'] = np.std(scores)\n",
    "        print(f\"{metric}: {results[f'{metric}_mean']:.4f} (+/- {results[f'{metric}_std']:.4f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e5d8780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "開始執行嚴重度分類實驗...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "應用混合採樣策略...\n",
      "原始分布: {0: 40419, 1: 3690866, 2: 779551, 3: 122104}\n",
      "採樣後分布: {0: 676240, 1: 676240, 2: 676240, 3: 676240}\n",
      "\n",
      "--- 訓練【嚴重度】XGBoost 分類 ---\n",
      "[0]\tvalidation_0-mlogloss:1.34685\tvalidation_0-merror:0.47085\tvalidation_1-mlogloss:1.34691\tvalidation_1-merror:0.47134\n",
      "[200]\tvalidation_0-mlogloss:0.74675\tvalidation_0-merror:0.35052\tvalidation_1-mlogloss:0.74750\tvalidation_1-merror:0.35163\n",
      "[400]\tvalidation_0-mlogloss:0.68755\tvalidation_0-merror:0.31481\tvalidation_1-mlogloss:0.68800\tvalidation_1-merror:0.31545\n",
      "[600]\tvalidation_0-mlogloss:0.65548\tvalidation_0-merror:0.29669\tvalidation_1-mlogloss:0.65575\tvalidation_1-merror:0.29702\n",
      "[800]\tvalidation_0-mlogloss:0.63234\tvalidation_0-merror:0.28407\tvalidation_1-mlogloss:0.63246\tvalidation_1-merror:0.28423\n",
      "[1000]\tvalidation_0-mlogloss:0.61322\tvalidation_0-merror:0.27398\tvalidation_1-mlogloss:0.61325\tvalidation_1-merror:0.27411\n",
      "[1200]\tvalidation_0-mlogloss:0.59786\tvalidation_0-merror:0.26584\tvalidation_1-mlogloss:0.59781\tvalidation_1-merror:0.26595\n",
      "[1400]\tvalidation_0-mlogloss:0.58635\tvalidation_0-merror:0.25979\tvalidation_1-mlogloss:0.58622\tvalidation_1-merror:0.25994\n",
      "[1600]\tvalidation_0-mlogloss:0.57906\tvalidation_0-merror:0.25628\tvalidation_1-mlogloss:0.57892\tvalidation_1-merror:0.25630\n",
      "[1800]\tvalidation_0-mlogloss:0.57427\tvalidation_0-merror:0.25392\tvalidation_1-mlogloss:0.57409\tvalidation_1-merror:0.25395\n",
      "[1999]\tvalidation_0-mlogloss:0.57006\tvalidation_0-merror:0.25201\tvalidation_1-mlogloss:0.56982\tvalidation_1-merror:0.25200\n",
      "Severity 分類結果 → acc: 0.7480, f1: 0.7853, bal_acc: 0.8180, time: 81.4s\n",
      "\n",
      "=== Experiment Summary ===\n",
      "嚴重度 (分類):   acc=0.7480, f1=0.7853, bal_acc=0.8180, time=81.4s\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Cell 8: run_experiment（僅嚴重度分類）\n",
    "# ===========================\n",
    "def run_experiment(df):\n",
    "    results = {}\n",
    "\n",
    "    # ---------- Ⅰ. 嚴重度（分類） ----------\n",
    "    df_cls, _ = advanced_preprocessing(df)\n",
    "    df_cls = df_cls[df_cls['Severity'].isin([1,2,3,4])].dropna()\n",
    "\n",
    "    obj_cols = df_cls.select_dtypes(include='object').columns\n",
    "    df_cls[obj_cols] = df_cls[obj_cols].astype('category').apply(lambda s: s.cat.codes)\n",
    "\n",
    "    X_cls = df_cls.drop('Severity', axis=1).values\n",
    "    y_cls = df_cls['Severity'].values - 1      # 0~3\n",
    "\n",
    "    X_tmp, X_test, y_tmp, y_test = train_test_split(\n",
    "        X_cls, y_cls, test_size=0.20, random_state=42, stratify=y_cls\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_tmp, y_tmp, test_size=0.25, random_state=42, stratify=y_tmp\n",
    "    )\n",
    "\n",
    "    X_train_s, y_train_s = apply_mixed_sampling(X_train, y_train)\n",
    "\n",
    "    print(\"\\n--- 訓練【嚴重度】XGBoost 分類 ---\")\n",
    "    sev_model, sev_pred, sev_time = train_xgboost_gpu(\n",
    "        X_train_s, X_test, y_train_s, y_test,\n",
    "        X_val=X_val, y_val=y_val,\n",
    "        objective='multi:softprob', num_class=4\n",
    "    )\n",
    "    sev_acc     = accuracy_score(y_test, sev_pred)\n",
    "    sev_f1      = f1_score(y_test, sev_pred, average='weighted')\n",
    "    sev_bal_acc = balanced_accuracy_score(y_test, sev_pred)\n",
    "    print(f\"Severity 分類結果 → acc: {sev_acc:.4f}, f1: {sev_f1:.4f}, bal_acc: {sev_bal_acc:.4f}, time: {sev_time:.1f}s\")\n",
    "\n",
    "    results['severity'] = {\n",
    "        'acc':        float(sev_acc),\n",
    "        'f1':         float(sev_f1),\n",
    "        'bal_acc':    float(sev_bal_acc),\n",
    "        'train_time': float(sev_time),\n",
    "        'model':      sev_model\n",
    "    }\n",
    "\n",
    "    # ---------- 打印最終結果 ----------\n",
    "    print(\"\\n=== Experiment Summary ===\")\n",
    "    print(f\"嚴重度 (分類):   acc={results['severity']['acc']:.4f}, \"\n",
    "          f\"f1={results['severity']['f1']:.4f}, \"\n",
    "          f\"bal_acc={results['severity']['bal_acc']:.4f}, \"\n",
    "          f\"time={results['severity']['train_time']:.1f}s\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# 執行實驗\n",
    "print(\"\\n開始執行嚴重度分類實驗...\")\n",
    "results = run_experiment(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "904804e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===========================\n",
    "# # Cell 9: 結果視覺化\n",
    "# # ===========================\n",
    "\n",
    "# def visualize_results(results):\n",
    "#     \"\"\"視覺化實驗結果\"\"\"\n",
    "#     # 準備數據\n",
    "#     metrics = ['accuracy', 'f1_score', 'balanced_accuracy']\n",
    "#     models = ['LightGBM', 'XGBoost', 'CatBoost']\n",
    "#     experiments = list(results.keys())\n",
    "    \n",
    "#     # 創建比較圖表\n",
    "#     fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "#     axes = axes.flatten()\n",
    "    \n",
    "#     for idx, metric in enumerate(metrics):\n",
    "#         ax = axes[idx]\n",
    "        \n",
    "#         # 準備數據\n",
    "#         data = []\n",
    "#         for exp in experiments:\n",
    "#             row = []\n",
    "#             for model in models:\n",
    "#                 row.append(results[exp][model][metric])\n",
    "#             data.append(row)\n",
    "        \n",
    "#         # 繪製熱力圖\n",
    "#         sns.heatmap(data, annot=True, fmt='.4f', \n",
    "#                    xticklabels=models, yticklabels=experiments,\n",
    "#                    cmap='YlOrRd', ax=ax, cbar_kws={'label': metric})\n",
    "#         ax.set_title(f'{metric.replace(\"_\", \" \").title()} 比較')\n",
    "    \n",
    "#     # 訓練時間比較\n",
    "#     ax = axes[3]\n",
    "#     time_data = []\n",
    "#     for exp in experiments:\n",
    "#         row = []\n",
    "#         for model in models:\n",
    "#             row.append(results[exp][model]['training_time'])\n",
    "#         time_data.append(row)\n",
    "    \n",
    "#     sns.heatmap(time_data, annot=True, fmt='.2f', \n",
    "#                xticklabels=models, yticklabels=experiments,\n",
    "#                cmap='Blues', ax=ax, cbar_kws={'label': '秒'})\n",
    "#     ax.set_title('訓練時間比較 (秒)')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('model_comparison_results.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "    \n",
    "#     # 打印總結表格\n",
    "#     print(\"\\n\" + \"=\"*100)\n",
    "#     print(\"實驗結果總結\")\n",
    "#     print(\"=\"*100)\n",
    "#     print(f\"{'實驗':<20} {'模型':<10} {'準確率':<10} {'F1分數':<10} {'平衡準確率':<12} {'訓練時間(秒)':<12}\")\n",
    "#     print(\"-\"*100)\n",
    "    \n",
    "#     for exp in experiments:\n",
    "#         for model in models:\n",
    "#             metrics = results[exp][model]\n",
    "#             print(f\"{exp:<20} {model:<10} {metrics['accuracy']:<10.4f} \"\n",
    "#                   f\"{metrics['f1_score']:<10.4f} {metrics['balanced_accuracy']:<12.4f} \"\n",
    "#                   f\"{metrics['training_time']:<12.2f}\")\n",
    "    \n",
    "#     # 找出最佳組合\n",
    "#     best_score = 0\n",
    "#     best_combo = None\n",
    "#     for exp in experiments:\n",
    "#         for model in models:\n",
    "#             score = results[exp][model]['balanced_accuracy']\n",
    "#             if score > best_score:\n",
    "#                 best_score = score\n",
    "#                 best_combo = (exp, model)\n",
    "    \n",
    "#     print(f\"\\n最佳組合: {best_combo[0]} - {best_combo[1]}\")\n",
    "#     print(f\"平衡準確率: {best_score:.4f}\")\n",
    "    \n",
    "#     return best_combo\n",
    "\n",
    "# best_combo = visualize_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cd4e2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Kepler] 以真實格點分布抽樣，開始 Batch 推論 30 天 …\n",
      "→ 完成 999,960 筆預測，花費 7.8 秒\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid format specifier",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m csv_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccident_severity_forecast_kepler_30days_realistic.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    115\u001b[0m pred_df\u001b[38;5;241m.\u001b[39mto_csv(csv_out, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKepler CSV 已存：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_out\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  （檔案大小約 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;250m \u001b[39m(pred_df\u001b[38;5;241m.\u001b[39mmemory_usage(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB）\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid format specifier"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Cell 10: Kepler.gl Data (真實分布抽樣 + Batch 推論，30 天預測、預估 60 萬筆)\n",
    "# ===========================\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_kepler_predictions_realistic(\n",
    "    df_geo,                # 原始全部資料，需包含 Start_Lat, Start_Lng, Start_Time, State\n",
    "    df_full_processed,     # advanced_preprocessing(df) → drop('Severity') 後的 DataFrame\n",
    "    sev_model,             # 已訓練好的 XGBClassifier\n",
    "    horizon_days=30,       # 預測 30 天\n",
    "    daily_times=[0, 6, 12, 18],  # 每天四個時段\n",
    "    total_predictions=600000     # 目標輸出筆數 (約 0.6M)\n",
    "):\n",
    "    \"\"\"\n",
    "    步驟概覽：\n",
    "    1. 以經緯度四捨五入至 2 位 (lat_bin, lng_bin)，計算每個格點的事故計數。\n",
    "    2. 計算每個格點的抽樣權重 = cnt / sum(cnt)，\n",
    "       依此從所有格點中抽取 n_locs 個格點，n_locs = total_predictions / (horizon_days * len(daily_times))。\n",
    "    3. 生成 (lat, lng, timestamp, Hour, DayOfWeek, Month) 的 Cartesian product → big_df。\n",
    "    4. 其餘所有訓練時用過的特徵用「整體平均值」填入 big_df。\n",
    "    5. 一次性呼叫 sev_model.predict_proba(batch_features)，塞回 risk_score, risk_level。\n",
    "    6. 回傳 big_df。\n",
    "    \"\"\"\n",
    "    # 1. 建立格點 (lat_bin, lng_bin) 並計算計數與權重\n",
    "    df_geo['lat_bin'] = df_geo['Start_Lat'].round(2)\n",
    "    df_geo['lng_bin'] = df_geo['Start_Lng'].round(2)\n",
    "    grid_counts = (\n",
    "        df_geo.groupby(['lat_bin', 'lng_bin'])\n",
    "              .size()\n",
    "              .reset_index(name='cnt')\n",
    "    )\n",
    "    grid_counts['weight'] = grid_counts['cnt'] / grid_counts['cnt'].sum()\n",
    "\n",
    "    # 2. 計算要抽取的格點數量\n",
    "    time_points = horizon_days * len(daily_times)  # 30 * 4 = 120\n",
    "    n_locs = int(total_predictions / time_points)\n",
    "    n_locs = min(n_locs, len(grid_counts))\n",
    "\n",
    "    # 3. 依照權重隨機抽 n_locs 個格點 (無放回)\n",
    "    sampled_idxs = np.random.choice(\n",
    "        grid_counts.index,\n",
    "        size=n_locs,\n",
    "        replace=False,\n",
    "        p=grid_counts['weight'].values\n",
    "    )\n",
    "    sample_locs = grid_counts.loc[sampled_idxs, ['lat_bin', 'lng_bin']].reset_index(drop=True)\n",
    "    sample_locs = sample_locs.rename(columns={'lat_bin':'lat','lng_bin':'lng'})\n",
    "\n",
    "    # 4. 構造所有 (lat, lng, timestamp, Hour, DayOfWeek, Month) 組合\n",
    "    latest_day = df_geo['Start_Time'].max().normalize()\n",
    "    rows = []\n",
    "    for lat, lng in sample_locs.values:\n",
    "        for d in range(1, horizon_days + 1):\n",
    "            ts_base = latest_day + pd.Timedelta(days=d)\n",
    "            dow = ts_base.weekday()\n",
    "            for hr in daily_times:\n",
    "                ts = ts_base + pd.Timedelta(hours=hr)\n",
    "                rows.append((lat, lng, ts, hr, dow, ts.month))\n",
    "    big_df = pd.DataFrame(rows, columns=['lat','lng','timestamp','Hour','DayOfWeek','Month'])\n",
    "\n",
    "    # 5. 其餘訓練特徵以平均值填充\n",
    "    feature_cols = df_full_processed.columns.tolist()\n",
    "    # 檢查常用時間特徵一定存在\n",
    "    for c in ['Hour','DayOfWeek','Month']:\n",
    "        if c not in feature_cols:\n",
    "            raise ValueError(f\"訓練特徵缺少 '{c}'，請先確認 advanced_preprocessing 有生成這三個欄。\")\n",
    "    template = df_full_processed[feature_cols].mean().round(4)\n",
    "    for col in feature_cols:\n",
    "        if col in ['Hour','DayOfWeek','Month']:\n",
    "            continue\n",
    "        big_df[col] = float(template[col])\n",
    "\n",
    "    # 6. Batch 推論：一次性呼叫 predict_proba\n",
    "    X_all = big_df[feature_cols].values  # shape=(n_locs*120, len(feature_cols))\n",
    "    all_probs = sev_model.predict_proba(X_all)  # shape=(n_rows, num_class)\n",
    "    risk_scores = all_probs.max(axis=1)  # 取最大機率作為風險分數\n",
    "    risk_levels = np.where(risk_scores > 0.7, 'High',\n",
    "                  np.where(risk_scores > 0.4, 'Medium', 'Low'))\n",
    "    big_df['risk_score'] = risk_scores\n",
    "    big_df['risk_level'] = risk_levels\n",
    "\n",
    "    return big_df\n",
    "\n",
    "\n",
    "# ——— 重新讀取完整地理資料，這次要帶 State，用 grid_counts 時其實不需要 State，但不影響抽樣分布 —— \n",
    "df_geo_full = pd.read_csv(\n",
    "    file_path,\n",
    "    usecols=['Start_Lat','Start_Lng','Start_Time','State'],\n",
    "    parse_dates=['Start_Time']\n",
    ")\n",
    "df_geo = df_geo_full.copy()\n",
    "\n",
    "# ——— 取得與訓練時相同的完整特徵 DataFrame —— \n",
    "df_full_processed, _ = advanced_preprocessing(df.copy())\n",
    "df_full_processed = df_full_processed.drop('Severity', axis=1)\n",
    "\n",
    "# ——— 執行 Batch 預測，預測 30 天，共約 600k 筆資料 —— \n",
    "print(\"\\n[Kepler] 以真實格點分布抽樣，開始 Batch 推論 30 天 …\")\n",
    "start_time = time.time()\n",
    "pred_df = create_kepler_predictions_realistic(\n",
    "    df_geo,\n",
    "    df_full_processed,\n",
    "    results['severity']['model'],\n",
    "    horizon_days=30,         # 預測未來 30 天\n",
    "    daily_times=[0,6,12,18],  # 每天 4 個時段\n",
    "    total_predictions=1000000  # 共約 600k 筆輸出\n",
    ")\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"→ 完成 {len(pred_df):,} 筆預測，花費 {elapsed:.1f} 秒\")\n",
    "\n",
    "# ——— 將結果存為 CSV，方便 Kepler.gl 使用 —— \n",
    "csv_out = 'accident_severity_forecast_kepler_30days_realistic.csv'\n",
    "pred_df.to_csv(csv_out, index=False)\n",
    "print(f\"Kepler CSV 已存：{csv_out}  （檔案大小約 { (pred_df.memory_usage(deep=True).sum()/1024**2):.1f } MB）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5df4c599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Kepler.gl 使用教學 (針對 accident_severity_forecast_kepler_30days_realistic.csv)\n",
      "================================================================================\n",
      "\n",
      "### 如何使用 Kepler.gl 視覺化 「accident_severity_forecast_kepler_30days_realistic.csv」：\n",
      "\n",
      "1. **下載或確認 CSV 檔案路徑**\n",
      "   - 請確定「accident_severity_forecast_kepler_30days_realistic.csv」已經存在於本機硬碟上，並且你能夠在瀏覽器或檔案總管中找到它。\n",
      "\n",
      "2. **打開 Kepler.gl**\n",
      "   - 在瀏覽器中輸入: https://kepler.gl/\n",
      "   - 等待 Kepler.gl 網頁載入完成。\n",
      "\n",
      "3. **上傳資料**\n",
      "   - 點擊右上角的「Add Data」按鈕，或直接把 `accident_severity_forecast_kepler_30days_realistic.csv` 檔案拖放到網頁上。\n",
      "   - Kepler.gl 會自動解析 CSV 中的標題列，並將數據加載到地圖中。\n",
      "\n",
      "4. **檢查數據欄位**\n",
      "   - 確認以下欄位都已正確匯入：\n",
      "     - `lat`、`lng`：地理緯度與經度（必須）\n",
      "     - `timestamp`：時間戳\n",
      "     - `risk_score`：風險分數 (0~1)\n",
      "     - `risk_level`：風險等級（High / Medium / Low）\n",
      "     - `Hour`：小時\n",
      "     - `DayOfWeek`：星期幾 (0=週一, …, 6=週日)\n",
      "     - `Month`：月份 (1~12)\n",
      "     - **以及其他模型訓練時所需的數值特徵（如果需要在 Kepler 篩選，也可一併查看）**\n",
      "\n",
      "5. **建立基礎地圖圖層 (Point Layer)**\n",
      "   - 在左側的「Layers」面板，點擊「Add Layer」。\n",
      "   - 選擇「Point」圖層。\n",
      "   - 設定：\n",
      "     - **Longitude**: 選 `lng`\n",
      "     - **Latitude**: 選 `lat`\n",
      "     - **Color Encoding**: \n",
      "       - Data: `risk_score`\n",
      "       - Scale: 選「Color Range」→ 紅色漸變 (或自訂)\n",
      "     - **Size Encoding**:\n",
      "       - Data: `risk_score`\n",
      "       - Scale: “Range” 設為 [1, 10]（或看需求調整）\n",
      "   - 點擊「OK」後，即可看到所有點依照 `risk_score` 上色並顯示大小。\n",
      "\n",
      "6. **設定時間動畫 (Time Filter)**\n",
      "   - 左側點選「Filters」標籤，點「+ Add Filter」。\n",
      "   - 在彈出選單中選擇 `timestamp`，Kepler 會自動偵測時間格式並顯示「Time Filter」控制條。\n",
      "   - 開啟「Enable Time Playback」按鈕，設定：\n",
      "     - **Window Size**: 例如 1 天 (1d) 或 12 小時 (12h)，隨需求調整。\n",
      "     - **Playback Speed**: 隨需求調整 (例如 1x 或 2x)。\n",
      "   - 點擊播放按鍵即可看到地圖隨時間演進，點隨 `timestamp` 動態出現。\n",
      "\n",
      "7. **新增風險等級過濾器 (Category Filter)**\n",
      "   - 點「+ Add Filter」，選擇 `risk_level` 欄位。\n",
      "   - 這會自動建立「Category Filter」下拉選單，可以切換只顯示 High / Medium / Low 其中一種或多種組合。\n",
      "\n",
      "8. **新增其他篩選，如小時 & 星期 (Hour & DayOfWeek)**\n",
      "   - 同樣透過「+ Add Filter」分別選擇 `Hour` 或 `DayOfWeek` 欄位，\n",
      "     - 針對 `Hour`：可以選擇某些小時段（如只看早上 6~9 點、傍晚 16~19 點）。\n",
      "     - 針對 `DayOfWeek`：可以只顯示週末 (5,6) 或特定星期。\n",
      "   - 打開「Multiple Select」模式，可以多選不同值。\n",
      "\n",
      "9. **調整圖層樣式 (Layer Settings)**\n",
      "   - 點選剛剛建立的「Point Layer」，展開「Style」面板：\n",
      "     - **Opacity**: 依需求調整點的透明度（如 0.8 避免重疊過濃）。\n",
      "     - **Stroke Width**: 若想讓點看起來更突出，可調整邊框粗細 (e.g. 0.5)。\n",
      "     - **Outline Color**: 可選灰色或白色，讓點在底圖上更清晰可見。\n",
      "\n",
      "10. **建立其他圖層 (可選)**\n",
      "    - **Heatmap Layer**：\n",
      "      - 在「Add Layer」選「Heatmap」。\n",
      "      - Data: 選 `timestamp`、`lat`、`lng`、`weight`（如果想用 `risk_score`，就 Weight = `risk_score`）。\n",
      "      - 修改 `Radius` (e.g. 10 km) 來調整熱度分布範圍。\n",
      "    - **Hexagon Layer**：\n",
      "      - 在「Add Layer」選「Hexagon」。\n",
      "      - Data: 同樣選 `lat`、`lng`，Aggregation: `risk_score` → Sum 或平均。\n",
      "      - 修改 `Radius` (5~10 km) 及 `Elevation Scale` 來顯示立體柱狀。\n",
      "    - **Trip Layer (若要顯示連續軌跡)**：  \n",
      "      - 如果有「多段軌跡」資料可用，可考慮 Trip Layer，這裡暫不示範。\n",
      "\n",
      "11. **3D 模式**  \n",
      "    - 點擊地圖右上方的「3D」按鈕，可進入 3D 視角，調整垂直角度與縮放，更立體地看風險柱狀 (適用於 Hexagon Layer)。\n",
      "\n",
      "12. **匯出與分享**  \n",
      "    - 地圖設定完成後，點「Export Map」→ 選「Export as HTML」即可下載一個純 HTML 檔案，打開就能離線瀏覽互動地圖。\n",
      "    - 或者「Export Config」將當前的圖層與篩選配置打包成 JSON，下次匯入相同數據可直接復現設定。\n",
      "\n",
      "---\n",
      "\n",
      "### 欄位說明 (針對這個 CSV)：\n",
      "- **lat, lng**：地理座標 (緯度／經度)  \n",
      "- **timestamp**：預測時間戳 (Datetime)  \n",
      "- **risk_score**：模型預測出的風險分數 (0 ~ 1)  \n",
      "- **risk_level**：風險等級 (High / Medium / Low)  \n",
      "- **Hour**：小時 (0 ~ 23)  \n",
      "- **DayOfWeek**：星期幾 (0 = 週一 … 6 = 週日)  \n",
      "- **Month**：月份 (1 ~ 12)  \n",
      "- **其他訓練特徵**（如溫度、濕度、Weather_Category 等）如果需要，也可在 Kepler 裡作為 Color/Size/Filter 依據。\n",
      "\n",
      "---\n",
      "\n",
      "### 範例執行流程總結\n",
      "1. 拖入 `accident_severity_forecast_kepler_30days_realistic.csv`。  \n",
      "2. 建立一個 Point Layer，將 `lat`,`lng` 設為座標、`risk_score` 設為顏色與大小。  \n",
      "3. 建立 Time Filter → 播放未來 30 天 4 個時段 (0,6,12,18) 的風險地圖動態。  \n",
      "4. 建立 Category Filter (`risk_level`) 可快速顯示 High/Medium/Low。  \n",
      "5. 建立 Hour、DayOfWeek 篩選以聚焦特定時段或特定星期。  \n",
      "6. 可選擇 Heatmap 或 Hexagon Layer 展示整體風險熱度或聚合分布。  \n",
      "7. 進入 3D 模式、匯出 HTML 與設定檔，完成分享與分析。\n",
      "\n",
      "按以上步驟，你就能在 Kepler.gl 中完整展現這份「未來 30 天」的事故嚴重度風險預測，並且自由篩選、做時空動畫與不同圖層疊加。  \n",
      "\n",
      "\n",
      "數據預覽：\n",
      "     lat    lng           timestamp  risk_score risk_level  Hour  DayOfWeek  \\\n",
      "0  41.31 -74.13 2023-04-01 00:00:00    0.607269     Medium     0          5   \n",
      "1  41.31 -74.13 2023-04-01 06:00:00    0.712600       High     6          5   \n",
      "2  41.31 -74.13 2023-04-01 12:00:00    0.687416     Medium    12          5   \n",
      "3  41.31 -74.13 2023-04-01 18:00:00    0.680073     Medium    18          5   \n",
      "4  41.31 -74.13 2023-04-02 00:00:00    0.621528     Medium     0          6   \n",
      "5  41.31 -74.13 2023-04-02 06:00:00    0.729354       High     6          6   \n",
      "6  41.31 -74.13 2023-04-02 12:00:00    0.686590     Medium    12          6   \n",
      "7  41.31 -74.13 2023-04-02 18:00:00    0.684367     Medium    18          6   \n",
      "8  41.31 -74.13 2023-04-03 00:00:00    0.682931     Medium     0          0   \n",
      "9  41.31 -74.13 2023-04-03 06:00:00    0.678190     Medium     6          0   \n",
      "\n",
      "   Month  \n",
      "0      4  \n",
      "1      4  \n",
      "2      4  \n",
      "3      4  \n",
      "4      4  \n",
      "5      4  \n",
      "6      4  \n",
      "7      4  \n",
      "8      4  \n",
      "9      4  \n",
      "\n",
      "數據統計：\n",
      "- 總數據點: 999,960\n",
      "- 風險等級 High: 116,662\n",
      "- 風險等級 Medium: 883,298\n",
      "- 風險等級 Low: 0\n",
      "- 唯一 (lat,lng) 熱點數: 8,333\n",
      "\n",
      "GPU 記憶體使用: 0.00 GB\n",
      "GPU 記憶體快取: 0.00 GB\n",
      "\n",
      "教學完成！請打開 Kepler.gl 並依據步驟匯入 CSV。\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Cell 11: Kepler.gl 使用教學（針對 accident_severity_forecast_kepler_30days_realistic.csv）\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Kepler.gl 使用教學 (針對 accident_severity_forecast_kepler_30days_realistic.csv)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "instructions = \"\"\"\n",
    "### 如何使用 Kepler.gl 視覺化 「accident_severity_forecast_kepler_30days_realistic.csv」：\n",
    "\n",
    "1. **下載或確認 CSV 檔案路徑**\n",
    "   - 請確定「accident_severity_forecast_kepler_30days_realistic.csv」已經存在於本機硬碟上，並且你能夠在瀏覽器或檔案總管中找到它。\n",
    "\n",
    "2. **打開 Kepler.gl**\n",
    "   - 在瀏覽器中輸入: https://kepler.gl/\n",
    "   - 等待 Kepler.gl 網頁載入完成。\n",
    "\n",
    "3. **上傳資料**\n",
    "   - 點擊右上角的「Add Data」按鈕，或直接把 `accident_severity_forecast_kepler_30days_realistic.csv` 檔案拖放到網頁上。\n",
    "   - Kepler.gl 會自動解析 CSV 中的標題列，並將數據加載到地圖中。\n",
    "\n",
    "4. **檢查數據欄位**\n",
    "   - 確認以下欄位都已正確匯入：\n",
    "     - `lat`、`lng`：地理緯度與經度（必須）\n",
    "     - `timestamp`：時間戳\n",
    "     - `risk_score`：風險分數 (0~1)\n",
    "     - `risk_level`：風險等級（High / Medium / Low）\n",
    "     - `Hour`：小時\n",
    "     - `DayOfWeek`：星期幾 (0=週一, …, 6=週日)\n",
    "     - `Month`：月份 (1~12)\n",
    "     - **以及其他模型訓練時所需的數值特徵（如果需要在 Kepler 篩選，也可一併查看）**\n",
    "\n",
    "5. **建立基礎地圖圖層 (Point Layer)**\n",
    "   - 在左側的「Layers」面板，點擊「Add Layer」。\n",
    "   - 選擇「Point」圖層。\n",
    "   - 設定：\n",
    "     - **Longitude**: 選 `lng`\n",
    "     - **Latitude**: 選 `lat`\n",
    "     - **Color Encoding**: \n",
    "       - Data: `risk_score`\n",
    "       - Scale: 選「Color Range」→ 紅色漸變 (或自訂)\n",
    "     - **Size Encoding**:\n",
    "       - Data: `risk_score`\n",
    "       - Scale: “Range” 設為 [1, 10]（或看需求調整）\n",
    "   - 點擊「OK」後，即可看到所有點依照 `risk_score` 上色並顯示大小。\n",
    "\n",
    "6. **設定時間動畫 (Time Filter)**\n",
    "   - 左側點選「Filters」標籤，點「+ Add Filter」。\n",
    "   - 在彈出選單中選擇 `timestamp`，Kepler 會自動偵測時間格式並顯示「Time Filter」控制條。\n",
    "   - 開啟「Enable Time Playback」按鈕，設定：\n",
    "     - **Window Size**: 例如 1 天 (1d) 或 12 小時 (12h)，隨需求調整。\n",
    "     - **Playback Speed**: 隨需求調整 (例如 1x 或 2x)。\n",
    "   - 點擊播放按鍵即可看到地圖隨時間演進，點隨 `timestamp` 動態出現。\n",
    "\n",
    "7. **新增風險等級過濾器 (Category Filter)**\n",
    "   - 點「+ Add Filter」，選擇 `risk_level` 欄位。\n",
    "   - 這會自動建立「Category Filter」下拉選單，可以切換只顯示 High / Medium / Low 其中一種或多種組合。\n",
    "\n",
    "8. **新增其他篩選，如小時 & 星期 (Hour & DayOfWeek)**\n",
    "   - 同樣透過「+ Add Filter」分別選擇 `Hour` 或 `DayOfWeek` 欄位，\n",
    "     - 針對 `Hour`：可以選擇某些小時段（如只看早上 6~9 點、傍晚 16~19 點）。\n",
    "     - 針對 `DayOfWeek`：可以只顯示週末 (5,6) 或特定星期。\n",
    "   - 打開「Multiple Select」模式，可以多選不同值。\n",
    "\n",
    "9. **調整圖層樣式 (Layer Settings)**\n",
    "   - 點選剛剛建立的「Point Layer」，展開「Style」面板：\n",
    "     - **Opacity**: 依需求調整點的透明度（如 0.8 避免重疊過濃）。\n",
    "     - **Stroke Width**: 若想讓點看起來更突出，可調整邊框粗細 (e.g. 0.5)。\n",
    "     - **Outline Color**: 可選灰色或白色，讓點在底圖上更清晰可見。\n",
    "\n",
    "10. **建立其他圖層 (可選)**\n",
    "    - **Heatmap Layer**：\n",
    "      - 在「Add Layer」選「Heatmap」。\n",
    "      - Data: 選 `timestamp`、`lat`、`lng`、`weight`（如果想用 `risk_score`，就 Weight = `risk_score`）。\n",
    "      - 修改 `Radius` (e.g. 10 km) 來調整熱度分布範圍。\n",
    "    - **Hexagon Layer**：\n",
    "      - 在「Add Layer」選「Hexagon」。\n",
    "      - Data: 同樣選 `lat`、`lng`，Aggregation: `risk_score` → Sum 或平均。\n",
    "      - 修改 `Radius` (5~10 km) 及 `Elevation Scale` 來顯示立體柱狀。\n",
    "    - **Trip Layer (若要顯示連續軌跡)**：  \n",
    "      - 如果有「多段軌跡」資料可用，可考慮 Trip Layer，這裡暫不示範。\n",
    "\n",
    "11. **3D 模式**  \n",
    "    - 點擊地圖右上方的「3D」按鈕，可進入 3D 視角，調整垂直角度與縮放，更立體地看風險柱狀 (適用於 Hexagon Layer)。\n",
    "\n",
    "12. **匯出與分享**  \n",
    "    - 地圖設定完成後，點「Export Map」→ 選「Export as HTML」即可下載一個純 HTML 檔案，打開就能離線瀏覽互動地圖。\n",
    "    - 或者「Export Config」將當前的圖層與篩選配置打包成 JSON，下次匯入相同數據可直接復現設定。\n",
    "\n",
    "---\n",
    "\n",
    "### 欄位說明 (針對這個 CSV)：\n",
    "- **lat, lng**：地理座標 (緯度／經度)  \n",
    "- **timestamp**：預測時間戳 (Datetime)  \n",
    "- **risk_score**：模型預測出的風險分數 (0 ~ 1)  \n",
    "- **risk_level**：風險等級 (High / Medium / Low)  \n",
    "- **Hour**：小時 (0 ~ 23)  \n",
    "- **DayOfWeek**：星期幾 (0 = 週一 … 6 = 週日)  \n",
    "- **Month**：月份 (1 ~ 12)  \n",
    "- **其他訓練特徵**（如溫度、濕度、Weather_Category 等）如果需要，也可在 Kepler 裡作為 Color/Size/Filter 依據。\n",
    "\n",
    "---\n",
    "\n",
    "### 範例執行流程總結\n",
    "1. 拖入 `accident_severity_forecast_kepler_30days_realistic.csv`。  \n",
    "2. 建立一個 Point Layer，將 `lat`,`lng` 設為座標、`risk_score` 設為顏色與大小。  \n",
    "3. 建立 Time Filter → 播放未來 30 天 4 個時段 (0,6,12,18) 的風險地圖動態。  \n",
    "4. 建立 Category Filter (`risk_level`) 可快速顯示 High/Medium/Low。  \n",
    "5. 建立 Hour、DayOfWeek 篩選以聚焦特定時段或特定星期。  \n",
    "6. 可選擇 Heatmap 或 Hexagon Layer 展示整體風險熱度或聚合分布。  \n",
    "7. 進入 3D 模式、匯出 HTML 與設定檔，完成分享與分析。\n",
    "\n",
    "按以上步驟，你就能在 Kepler.gl 中完整展現這份「未來 30 天」的事故嚴重度風險預測，並且自由篩選、做時空動畫與不同圖層疊加。  \n",
    "\"\"\"\n",
    "\n",
    "print(instructions)\n",
    "\n",
    "# 顯示一些數據基本統計，確認匯入無誤\n",
    "print(\"\\n數據預覽：\")\n",
    "print(pred_df[['lat','lng','timestamp','risk_score','risk_level','Hour','DayOfWeek','Month']].head(10))\n",
    "\n",
    "print(f\"\\n數據統計：\")\n",
    "print(f\"- 總數據點: {len(pred_df):,}\")\n",
    "print(f\"- 風險等級 High: {len(pred_df[pred_df['risk_level']=='High']):,}\")\n",
    "print(f\"- 風險等級 Medium: {len(pred_df[pred_df['risk_level']=='Medium']):,}\")\n",
    "print(f\"- 風險等級 Low: {len(pred_df[pred_df['risk_level']=='Low']):,}\")\n",
    "print(f\"- 唯一 (lat,lng) 熱點數: {pred_df[['lat','lng']].drop_duplicates().shape[0]:,}\")\n",
    "\n",
    "# 顯示 GPU 使用情況 (如果需要)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU 記憶體使用: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU 記憶體快取: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"\\n教學完成！請打開 Kepler.gl 並依據步驟匯入 CSV。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
