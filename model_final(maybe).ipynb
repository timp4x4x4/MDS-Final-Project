{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b086c8e",
   "metadata": {},
   "source": [
    "### US Accidents 資料分析：模型比較與時空風險預測\n",
    "## 實驗設計\n",
    "1. 比較有無資料前處理的影響\n",
    "2. 比較有無混合採樣策略的影響\n",
    "3. 使用三個模型：LightGBM, XGBoost, CatBoost（GPU加速版）\n",
    "4. 包含交叉驗證和進度顯示\n",
    "5. 創建時空風險預測數據供 Kepler.gl 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f029f3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "環境檢查\n",
      "============================================================\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "GPU Memory: 23.56 GB\n",
      "XGBoost version: 3.0.2\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Cell 1: 導入套件和設定\n",
    "# ===========================\n",
    "import os, time, json, gc, warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# Imbalanced-learn（保留混合採樣）\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# 只保留 XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU 檢查\n",
    "print(\"=\"*60)\n",
    "print(\"環境檢查\")\n",
    "print(\"=\"*60)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8da381a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 2: 記憶體優化函數\n",
    "# ===========================\n",
    "\n",
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"通過改變數據類型來減少DataFrame的記憶體使用\"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f'記憶體使用減少了 {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "        print(f'{start_mem:.2f} MB --> {end_mem:.2f} MB')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_memory():\n",
    "    \"\"\"清理記憶體\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "739ddefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "載入資料: us-accidents/US_Accidents_March23.csv\n",
      "載入 10.0% 的資料...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "載入資料大小: (772839, 27)\n",
      "記憶體使用: 103.19 MB\n",
      "\n",
      "目標變數分布:\n",
      "Severity 1: 6,803 (0.88%)\n",
      "Severity 2: 615,250 (79.61%)\n",
      "Severity 3: 130,256 (16.85%)\n",
      "Severity 4: 20,530 (2.66%)\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Cell 3: 載入資料（優化版）\n",
    "# ===========================\n",
    "\n",
    "def load_data_optimized(file_path, sample_frac=0.1):  # 使用10%資料做實驗\n",
    "    \"\"\"優化的資料載入\"\"\"\n",
    "    print(f\"\\n載入資料: {file_path}\")\n",
    "    \n",
    "    # 定義需要的欄位\n",
    "    important_cols = [\n",
    "        'Severity', 'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng',\n",
    "        'Distance(mi)', 'Temperature(F)', 'Humidity(%)', 'Pressure(in)',\n",
    "        'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)',\n",
    "        'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way',\n",
    "        'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop',\n",
    "        'Traffic_Calming', 'Traffic_Signal', 'Sunrise_Sunset', 'State'\n",
    "    ]\n",
    "    \n",
    "    # 載入資料\n",
    "    print(f\"載入 {sample_frac*100}% 的資料...\")\n",
    "    df = pd.read_csv(file_path, usecols=lambda x: x in important_cols)\n",
    "    df = df.sample(frac=sample_frac, random_state=42)\n",
    "    \n",
    "    print(f\"載入資料大小: {df.shape}\")\n",
    "    print(f\"記憶體使用: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # 顯示目標變數分布\n",
    "    print(\"\\n目標變數分布:\")\n",
    "    severity_counts = df['Severity'].value_counts().sort_index()\n",
    "    for sev, count in severity_counts.items():\n",
    "        print(f\"Severity {sev}: {count:,} ({count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 執行載入\n",
    "file_path = 'us-accidents/US_Accidents_March23.csv'\n",
    "df = load_data_optimized(file_path, sample_frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43e1219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 4: 基礎特徵工程函數\n",
    "# ===========================\n",
    "\n",
    "def basic_preprocessing(df):\n",
    "    \"\"\"基礎前處理：只處理缺失值和基本轉換\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # 處理日期時間\n",
    "    df_copy['Start_Time'] = pd.to_datetime(df_copy['Start_Time'], errors='coerce')\n",
    "    df_copy['End_Time'] = pd.to_datetime(df_copy['End_Time'], errors='coerce')\n",
    "    \n",
    "    # 計算持續時間\n",
    "    df_copy['Duration_minutes'] = (df_copy['End_Time'] - df_copy['Start_Time']).dt.total_seconds() / 60\n",
    "    \n",
    "    # 過濾異常值\n",
    "    df_copy = df_copy[(df_copy['Duration_minutes'] > 0) & (df_copy['Duration_minutes'] < 1440*7)]\n",
    "    df_copy = df_copy.dropna(subset=['Start_Time'])\n",
    "    \n",
    "    # 提取基本時間特徵\n",
    "    df_copy['Hour'] = df_copy['Start_Time'].dt.hour\n",
    "    df_copy['DayOfWeek'] = df_copy['Start_Time'].dt.dayofweek\n",
    "    df_copy['Month'] = df_copy['Start_Time'].dt.month\n",
    "    \n",
    "    # 處理缺失值（簡單填充）\n",
    "    numeric_cols = df_copy.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col != 'Severity':\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
    "    \n",
    "    # 類別變數填充\n",
    "    categorical_cols = ['Weather_Condition', 'State', 'Sunrise_Sunset']\n",
    "    for col in categorical_cols:\n",
    "        if col in df_copy.columns:\n",
    "            df_copy[col] = df_copy[col].fillna('Unknown')\n",
    "    \n",
    "    # 布林型欄位轉換\n",
    "    bool_cols = df_copy.select_dtypes(include=['bool']).columns\n",
    "    for col in bool_cols:\n",
    "        df_copy[col] = df_copy[col].astype(int)\n",
    "    \n",
    "    # 刪除不需要的欄位\n",
    "    df_copy = df_copy.drop(['Start_Time', 'End_Time'], axis=1, errors='ignore')\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def advanced_preprocessing(df):\n",
    "    \"\"\"進階前處理：包含所有特徵工程\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # 先做基礎處理\n",
    "    df_copy = basic_preprocessing(df_copy)\n",
    "    \n",
    "    # 額外的特徵工程\n",
    "    # 1. 是否週末\n",
    "    df_copy['IsWeekend'] = (df_copy['DayOfWeek'] >= 5).astype(int)\n",
    "    \n",
    "    # 2. 是否尖峰時段\n",
    "    df_copy['IsRushHour'] = df_copy['Hour'].apply(\n",
    "        lambda x: 1 if (6 <= x <= 9) or (16 <= x <= 19) else 0\n",
    "    )\n",
    "    \n",
    "    # 3. 時段分類\n",
    "    df_copy['TimeOfDay'] = pd.cut(df_copy['Hour'], \n",
    "                                  bins=[-1, 6, 12, 18, 24], \n",
    "                                  labels=[0, 1, 2, 3]).astype(int)\n",
    "    \n",
    "    # 4. 季節\n",
    "    df_copy['Season'] = pd.cut(df_copy['Month'], \n",
    "                               bins=[0, 3, 6, 9, 12], \n",
    "                               labels=[0, 1, 2, 3]).astype(int)\n",
    "    \n",
    "    # 5. 天氣分類（如果有天氣條件）\n",
    "    if 'Weather_Condition' in df_copy.columns:\n",
    "        def categorize_weather(condition):\n",
    "            if pd.isna(condition):\n",
    "                return 0\n",
    "            condition = str(condition).lower()\n",
    "            if any(word in condition for word in ['clear', 'fair']):\n",
    "                return 1\n",
    "            elif any(word in condition for word in ['cloud', 'overcast']):\n",
    "                return 2\n",
    "            elif any(word in condition for word in ['rain', 'drizzle']):\n",
    "                return 3\n",
    "            elif any(word in condition for word in ['snow', 'sleet']):\n",
    "                return 4\n",
    "            elif any(word in condition for word in ['fog', 'mist']):\n",
    "                return 5\n",
    "            elif any(word in condition for word in ['storm', 'thunder']):\n",
    "                return 6\n",
    "            else:\n",
    "                return 7\n",
    "        \n",
    "        df_copy['Weather_Category'] = df_copy['Weather_Condition'].apply(categorize_weather)\n",
    "        df_copy = df_copy.drop('Weather_Condition', axis=1)\n",
    "    \n",
    "    # 6. 對類別變數進行標籤編碼\n",
    "    label_encoders = {}\n",
    "    categorical_cols = ['State', 'Sunrise_Sunset']\n",
    "    for col in categorical_cols:\n",
    "        if col in df_copy.columns:\n",
    "            le = LabelEncoder()\n",
    "            df_copy[col] = le.fit_transform(df_copy[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    return df_copy, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "217b93e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 5: 混合採樣策略\n",
    "# ===========================\n",
    "\n",
    "def apply_mixed_sampling(X_train, y_train):\n",
    "    \"\"\"應用混合採樣策略\"\"\"\n",
    "    print(\"\\n應用混合採樣策略...\")\n",
    "    \n",
    "    # 計算各類別數量\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    class_counts = dict(zip(unique, counts))\n",
    "    print(\"原始分布:\", class_counts)\n",
    "    \n",
    "    # 混合策略：對多數類欠採樣，對少數類過採樣\n",
    "    median_count = int(np.median(counts))\n",
    "    target_count = int(median_count * 1.5)\n",
    "    \n",
    "    # 第一步：欠採樣\n",
    "    undersample_strategy = {}\n",
    "    for cls, cnt in class_counts.items():\n",
    "        if cnt > target_count:\n",
    "            undersample_strategy[cls] = target_count\n",
    "        else:\n",
    "            undersample_strategy[cls] = cnt\n",
    "    \n",
    "    if len(undersample_strategy) > 0:\n",
    "        rus = RandomUnderSampler(sampling_strategy=undersample_strategy, random_state=42)\n",
    "        X_temp, y_temp = rus.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        X_temp, y_temp = X_train, y_train\n",
    "    \n",
    "    # 第二步：過採樣\n",
    "    temp_unique, temp_counts = np.unique(y_temp, return_counts=True)\n",
    "    temp_class_counts = dict(zip(temp_unique, temp_counts))\n",
    "    \n",
    "    oversample_strategy = {}\n",
    "    for cls, cnt in temp_class_counts.items():\n",
    "        if cnt < target_count:\n",
    "            oversample_strategy[cls] = target_count\n",
    "        else:\n",
    "            oversample_strategy[cls] = cnt\n",
    "    \n",
    "    if len(oversample_strategy) > 0:\n",
    "        ros = RandomOverSampler(sampling_strategy=oversample_strategy, random_state=42)\n",
    "        X_resampled, y_resampled = ros.fit_resample(X_temp, y_temp)\n",
    "    else:\n",
    "        X_resampled, y_resampled = X_temp, y_temp\n",
    "    \n",
    "    # 顯示新分布\n",
    "    unique_new, counts_new = np.unique(y_resampled, return_counts=True)\n",
    "    print(\"採樣後分布:\", dict(zip(unique_new, counts_new)))\n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaddd219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 6: XGBoost GPU 訓練器\n",
    "# ===========================\n",
    "def train_xgboost_gpu(\n",
    "        X_train, X_test, y_train, y_test,\n",
    "        X_val=None, y_val=None,\n",
    "        *,\n",
    "        objective='multi:softprob',\n",
    "        num_class=4\n",
    "    ):\n",
    "    \"\"\"\n",
    "    通用 XGBoost GPU 訓練器\n",
    "      - 多分類: objective='multi:softprob'，num_class=類別總數\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'objective': objective,\n",
    "        # ==== 3090 GPU 最佳化 ====\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'predictor':   'gpu_predictor',\n",
    "        'gpu_id': 0,\n",
    "        'max_bin': 256,\n",
    "        'sampling_method': 'gradient_based',\n",
    "        # ==== 常用超參 ====\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.08,\n",
    "        'n_estimators': 1500,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 1,\n",
    "        'gamma': 0.15,\n",
    "        'lambda': 1.0,\n",
    "        'alpha': 0.0,\n",
    "        'n_jobs': os.cpu_count()\n",
    "    }\n",
    "\n",
    "    # 只有在多分類時才加入 num_class & eval_metric\n",
    "    if objective.startswith('multi'):\n",
    "        params['num_class'] = num_class\n",
    "        params['eval_metric'] = ['mlogloss', 'merror']\n",
    "\n",
    "    Model = xgb.XGBClassifier\n",
    "    model = Model(**params)\n",
    "\n",
    "    eval_set = [(X_test, y_test)]\n",
    "    if X_val is not None:\n",
    "        eval_set.append((X_val, y_val))\n",
    "\n",
    "    start = time.time()\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=eval_set,\n",
    "        # early_stopping_rounds=80,\n",
    "        verbose=200\n",
    "    )\n",
    "    train_time = time.time() - start\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    return model, preds, train_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43aa7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 7: 交叉驗證函數\n",
    "# ===========================\n",
    "\n",
    "def cross_validate_model(model_func, X, y, cv_folds=5):\n",
    "    \"\"\"執行交叉驗證\"\"\"\n",
    "    print(f\"\\n執行 {cv_folds} 折交叉驗證...\")\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    cv_scores = {\n",
    "        'accuracy': [],\n",
    "        'f1_score': [],\n",
    "        'balanced_accuracy': []\n",
    "    }\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tqdm(skf.split(X, y), total=cv_folds, desc=\"CV Progress\")):\n",
    "        X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
    "        y_train_cv, y_val_cv = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # 訓練模型\n",
    "        model, y_pred, _ = model_func(X_train_cv, X_val_cv, y_train_cv, y_val_cv)\n",
    "        \n",
    "        # 計算指標\n",
    "        cv_scores['accuracy'].append(accuracy_score(y_val_cv, y_pred))\n",
    "        cv_scores['f1_score'].append(f1_score(y_val_cv, y_pred, average='weighted'))\n",
    "        cv_scores['balanced_accuracy'].append(balanced_accuracy_score(y_val_cv, y_pred))\n",
    "        \n",
    "        # 清理GPU記憶體\n",
    "        clean_memory()\n",
    "    \n",
    "    # 計算平均值和標準差\n",
    "    results = {}\n",
    "    for metric, scores in cv_scores.items():\n",
    "        results[f'{metric}_mean'] = np.mean(scores)\n",
    "        results[f'{metric}_std'] = np.std(scores)\n",
    "        print(f\"{metric}: {results[f'{metric}_mean']:.4f} (+/- {results[f'{metric}_std']:.4f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e5d8780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "開始執行嚴重度分類實驗...\n",
      "\n",
      "應用混合採樣策略...\n",
      "原始分布: {0: 4082, 1: 368806, 2: 78145, 3: 12253}\n",
      "採樣後分布: {0: 67798, 1: 67798, 2: 67798, 3: 67798}\n",
      "\n",
      "--- 訓練【嚴重度】XGBoost 分類 ---\n",
      "[0]\tvalidation_0-mlogloss:1.34663\tvalidation_0-merror:0.47987\tvalidation_1-mlogloss:1.34655\tvalidation_1-merror:0.47960\n",
      "[200]\tvalidation_0-mlogloss:0.72509\tvalidation_0-merror:0.34363\tvalidation_1-mlogloss:0.72328\tvalidation_1-merror:0.34198\n",
      "[400]\tvalidation_0-mlogloss:0.65181\tvalidation_0-merror:0.30183\tvalidation_1-mlogloss:0.65098\tvalidation_1-merror:0.29996\n",
      "[600]\tvalidation_0-mlogloss:0.60875\tvalidation_0-merror:0.27951\tvalidation_1-mlogloss:0.60818\tvalidation_1-merror:0.27734\n",
      "[800]\tvalidation_0-mlogloss:0.58700\tvalidation_0-merror:0.26829\tvalidation_1-mlogloss:0.58691\tvalidation_1-merror:0.26679\n",
      "[1000]\tvalidation_0-mlogloss:0.57890\tvalidation_0-merror:0.26459\tvalidation_1-mlogloss:0.57895\tvalidation_1-merror:0.26292\n",
      "[1200]\tvalidation_0-mlogloss:0.57259\tvalidation_0-merror:0.26140\tvalidation_1-mlogloss:0.57281\tvalidation_1-merror:0.25991\n",
      "[1400]\tvalidation_0-mlogloss:0.57132\tvalidation_0-merror:0.26076\tvalidation_1-mlogloss:0.57162\tvalidation_1-merror:0.25928\n",
      "[1499]\tvalidation_0-mlogloss:0.57106\tvalidation_0-merror:0.26059\tvalidation_1-mlogloss:0.57138\tvalidation_1-merror:0.25911\n",
      "Severity 分類結果 → acc: 0.7394, f1: 0.7724, bal_acc: 0.7407, time: 15.7s\n",
      "\n",
      "=== Experiment Summary ===\n",
      "嚴重度 (分類):   acc=0.7394, f1=0.7724, bal_acc=0.7407, time=15.7s\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Cell 8: run_experiment（僅嚴重度分類）\n",
    "# ===========================\n",
    "def run_experiment(df):\n",
    "    results = {}\n",
    "\n",
    "    # ---------- Ⅰ. 嚴重度（分類） ----------\n",
    "    df_cls, _ = advanced_preprocessing(df)\n",
    "    df_cls = df_cls[df_cls['Severity'].isin([1,2,3,4])].dropna()\n",
    "\n",
    "    obj_cols = df_cls.select_dtypes(include='object').columns\n",
    "    df_cls[obj_cols] = df_cls[obj_cols].astype('category').apply(lambda s: s.cat.codes)\n",
    "\n",
    "    X_cls = df_cls.drop('Severity', axis=1).values\n",
    "    y_cls = df_cls['Severity'].values - 1      # 0~3\n",
    "\n",
    "    X_tmp, X_test, y_tmp, y_test = train_test_split(\n",
    "        X_cls, y_cls, test_size=0.20, random_state=42, stratify=y_cls\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_tmp, y_tmp, test_size=0.25, random_state=42, stratify=y_tmp\n",
    "    )\n",
    "\n",
    "    X_train_s, y_train_s = apply_mixed_sampling(X_train, y_train)\n",
    "\n",
    "    print(\"\\n--- 訓練【嚴重度】XGBoost 分類 ---\")\n",
    "    sev_model, sev_pred, sev_time = train_xgboost_gpu(\n",
    "        X_train_s, X_test, y_train_s, y_test,\n",
    "        X_val=X_val, y_val=y_val,\n",
    "        objective='multi:softprob', num_class=4\n",
    "    )\n",
    "    sev_acc     = accuracy_score(y_test, sev_pred)\n",
    "    sev_f1      = f1_score(y_test, sev_pred, average='weighted')\n",
    "    sev_bal_acc = balanced_accuracy_score(y_test, sev_pred)\n",
    "    print(f\"Severity 分類結果 → acc: {sev_acc:.4f}, f1: {sev_f1:.4f}, bal_acc: {sev_bal_acc:.4f}, time: {sev_time:.1f}s\")\n",
    "\n",
    "    results['severity'] = {\n",
    "        'acc':        float(sev_acc),\n",
    "        'f1':         float(sev_f1),\n",
    "        'bal_acc':    float(sev_bal_acc),\n",
    "        'train_time': float(sev_time),\n",
    "        'model':      sev_model\n",
    "    }\n",
    "\n",
    "    # ---------- 打印最終結果 ----------\n",
    "    print(\"\\n=== Experiment Summary ===\")\n",
    "    print(f\"嚴重度 (分類):   acc={results['severity']['acc']:.4f}, \"\n",
    "          f\"f1={results['severity']['f1']:.4f}, \"\n",
    "          f\"bal_acc={results['severity']['bal_acc']:.4f}, \"\n",
    "          f\"time={results['severity']['train_time']:.1f}s\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# 執行實驗\n",
    "print(\"\\n開始執行嚴重度分類實驗...\")\n",
    "results = run_experiment(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "904804e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===========================\n",
    "# # Cell 9: 結果視覺化\n",
    "# # ===========================\n",
    "\n",
    "# def visualize_results(results):\n",
    "#     \"\"\"視覺化實驗結果\"\"\"\n",
    "#     # 準備數據\n",
    "#     metrics = ['accuracy', 'f1_score', 'balanced_accuracy']\n",
    "#     models = ['LightGBM', 'XGBoost', 'CatBoost']\n",
    "#     experiments = list(results.keys())\n",
    "    \n",
    "#     # 創建比較圖表\n",
    "#     fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "#     axes = axes.flatten()\n",
    "    \n",
    "#     for idx, metric in enumerate(metrics):\n",
    "#         ax = axes[idx]\n",
    "        \n",
    "#         # 準備數據\n",
    "#         data = []\n",
    "#         for exp in experiments:\n",
    "#             row = []\n",
    "#             for model in models:\n",
    "#                 row.append(results[exp][model][metric])\n",
    "#             data.append(row)\n",
    "        \n",
    "#         # 繪製熱力圖\n",
    "#         sns.heatmap(data, annot=True, fmt='.4f', \n",
    "#                    xticklabels=models, yticklabels=experiments,\n",
    "#                    cmap='YlOrRd', ax=ax, cbar_kws={'label': metric})\n",
    "#         ax.set_title(f'{metric.replace(\"_\", \" \").title()} 比較')\n",
    "    \n",
    "#     # 訓練時間比較\n",
    "#     ax = axes[3]\n",
    "#     time_data = []\n",
    "#     for exp in experiments:\n",
    "#         row = []\n",
    "#         for model in models:\n",
    "#             row.append(results[exp][model]['training_time'])\n",
    "#         time_data.append(row)\n",
    "    \n",
    "#     sns.heatmap(time_data, annot=True, fmt='.2f', \n",
    "#                xticklabels=models, yticklabels=experiments,\n",
    "#                cmap='Blues', ax=ax, cbar_kws={'label': '秒'})\n",
    "#     ax.set_title('訓練時間比較 (秒)')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('model_comparison_results.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "    \n",
    "#     # 打印總結表格\n",
    "#     print(\"\\n\" + \"=\"*100)\n",
    "#     print(\"實驗結果總結\")\n",
    "#     print(\"=\"*100)\n",
    "#     print(f\"{'實驗':<20} {'模型':<10} {'準確率':<10} {'F1分數':<10} {'平衡準確率':<12} {'訓練時間(秒)':<12}\")\n",
    "#     print(\"-\"*100)\n",
    "    \n",
    "#     for exp in experiments:\n",
    "#         for model in models:\n",
    "#             metrics = results[exp][model]\n",
    "#             print(f\"{exp:<20} {model:<10} {metrics['accuracy']:<10.4f} \"\n",
    "#                   f\"{metrics['f1_score']:<10.4f} {metrics['balanced_accuracy']:<12.4f} \"\n",
    "#                   f\"{metrics['training_time']:<12.2f}\")\n",
    "    \n",
    "#     # 找出最佳組合\n",
    "#     best_score = 0\n",
    "#     best_combo = None\n",
    "#     for exp in experiments:\n",
    "#         for model in models:\n",
    "#             score = results[exp][model]['balanced_accuracy']\n",
    "#             if score > best_score:\n",
    "#                 best_score = score\n",
    "#                 best_combo = (exp, model)\n",
    "    \n",
    "#     print(f\"\\n最佳組合: {best_combo[0]} - {best_combo[1]}\")\n",
    "#     print(f\"平衡準確率: {best_score:.4f}\")\n",
    "    \n",
    "#     return best_combo\n",
    "\n",
    "# best_combo = visualize_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cd4e2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Kepler] 建立未來 30 天風險預測 DataFrame …\n",
      "→ Batch 預測完成，共 60,000 筆，花費 0.6 秒\n",
      "Kepler 檔案已存：accident_severity_forecast_kepler_30days.csv\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Cell 10: Kepler.gl Data (向量化 Batch 推論，一個月預測)\n",
    "# ===========================\n",
    "def create_kepler_predictions_severity_batch(\n",
    "    df_geo,                # 包含 Start_Lat, Start_Lng, Start_Time\n",
    "    df_full_processed,     # advanced_preprocessing(df)→drop('Severity') 後的 DataFrame\n",
    "    sev_model,             # 已訓練好的 XGBClassifier\n",
    "    horizon_days=30,       # 只預測一個月（30 天）\n",
    "    daily_times=None       # 時段列表，若為 None 則預設 [0,6,12,18]\n",
    "):\n",
    "    if daily_times is None:\n",
    "        daily_times = [0, 6, 12, 18]\n",
    "\n",
    "    # 1. 取得訓練用過的所有特徵欄位（除 Severity）\n",
    "    feature_cols = df_full_processed.columns.tolist()\n",
    "    for c in ['Hour', 'DayOfWeek', 'Month']:\n",
    "        if c not in feature_cols:\n",
    "            raise ValueError(f\"訓練特徵裡缺少 '{c}'，請確認 advanced_preprocessing 已建立這三個欄位。\")\n",
    "\n",
    "    # 2. 計算每個特徵的平均值（作為模板）\n",
    "    template_series = df_full_processed[feature_cols].mean().round(4)\n",
    "\n",
    "    # 3. 建立熱點清單：取出經緯度四捨五入到小數點 2 位後，出現次數 ≥ 20 的前 500 個\n",
    "    df_geo['lat_bin'] = df_geo['Start_Lat'].round(2)\n",
    "    df_geo['lng_bin'] = df_geo['Start_Lng'].round(2)\n",
    "    hot = (\n",
    "        df_geo.groupby(['lat_bin','lng_bin'])\n",
    "              .size()\n",
    "              .reset_index(name='cnt')\n",
    "    )\n",
    "    hot = hot[hot['cnt'] >= 20].head(500)[['lat_bin','lng_bin']]\n",
    "\n",
    "    # 4. 構造所有組合 (location × day × 時段)\n",
    "    latest_day = df_geo['Start_Time'].max().normalize()\n",
    "    rows = []\n",
    "    for lat, lng in hot.values:\n",
    "        for d in range(1, horizon_days + 1):\n",
    "            ts_base = latest_day + pd.Timedelta(days=d)\n",
    "            dow = (ts_base.weekday())  # DayOfWeek for ts_base + hr 不變\n",
    "            for hr in daily_times:\n",
    "                ts = ts_base + pd.Timedelta(hours=hr)\n",
    "                rows.append((lat, lng, ts, hr, dow, ts.month))\n",
    "    big_df = pd.DataFrame(rows, columns=['lat','lng','timestamp','Hour','DayOfWeek','Month'])\n",
    "\n",
    "    # 5. 把模板的平均值覆蓋到 big_df，並保留時間特徵\n",
    "    for col in feature_cols:\n",
    "        if col in ['Hour','DayOfWeek','Month']:\n",
    "            continue\n",
    "        big_df[col] = float(template_series[col])\n",
    "\n",
    "    # 6. 整理出供模型推論的特徵矩陣\n",
    "    X_all = big_df[feature_cols].values  # shape = (hot_count × horizon_days × len(daily_times), len(feature_cols))\n",
    "\n",
    "    # 7. 一次性呼叫 predict_proba\n",
    "    all_probs = sev_model.predict_proba(X_all)    # shape=(n_rows, num_class)\n",
    "    risk_scores = all_probs.max(axis=1)           # 取每列最大值\n",
    "    # 依 risk_scores 給出 risk_level\n",
    "    risk_levels = np.where(risk_scores > 0.7, 'High',\n",
    "                  np.where(risk_scores > 0.4, 'Medium', 'Low'))\n",
    "\n",
    "    # 8. 把結果填回 big_df\n",
    "    big_df['risk_score'] = risk_scores\n",
    "    big_df['risk_level'] = risk_levels\n",
    "\n",
    "    return big_df\n",
    "\n",
    "# ------------------------------\n",
    "# 重新讀取地理資料，只需要 Start_Lat, Start_Lng, Start_Time\n",
    "df_geo = pd.read_csv(\n",
    "    file_path,\n",
    "    usecols=['Start_Lat','Start_Lng','Start_Time'],\n",
    "    parse_dates=['Start_Time'],\n",
    "    nrows=100_000\n",
    ")\n",
    "\n",
    "# 先用 advanced_preprocessing 取得全量訓練特徵 (含 Hour, DayOfWeek, Month)\n",
    "df_full_processed, _ = advanced_preprocessing(df.copy())\n",
    "df_full_processed = df_full_processed.drop('Severity', axis=1)\n",
    "\n",
    "print(\"\\n[Kepler] 建立未來 30 天風險預測 DataFrame …\")\n",
    "start_time = time.time()\n",
    "pred_df = create_kepler_predictions_severity_batch(\n",
    "    df_geo,\n",
    "    df_full_processed,\n",
    "    results['severity']['model'],\n",
    "    horizon_days=30,       # 只跑一個月\n",
    "    daily_times=[0,6,12,18] # 每天 4 個時段\n",
    ")\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"→ Batch 預測完成，共 {len(pred_df):,} 筆，花費 {elapsed:.1f} 秒\")\n",
    "\n",
    "# 直接輸出成 CSV 給 Kepler.gl\n",
    "csv_out = 'accident_severity_forecast_kepler_30days.csv'\n",
    "pred_df.to_csv(csv_out, index=False)\n",
    "print(f\"Kepler 檔案已存：{csv_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5df4c599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Kepler.gl 使用教學\n",
      "================================================================================\n",
      "\n",
      "### 如何使用 Kepler.gl 視覺化您的數據：\n",
      "\n",
      "1. **訪問 Kepler.gl**\n",
      "   - 打開瀏覽器，訪問: https://kepler.gl/\n",
      "   - 或使用 Kepler.gl 的 Jupyter 擴展（如果已安裝）\n",
      "\n",
      "2. **上傳數據**\n",
      "   - 點擊 \"Add Data\" 或直接拖拽 'us_accidents_kepler_data.csv' 文件到網頁上\n",
      "   - 等待數據載入完成\n",
      "\n",
      "3. **配置地圖圖層**\n",
      "   建議創建以下圖層來展示不同維度的數據：\n",
      "\n",
      "   a) **點圖層 (Point Layer) - 歷史事故**\n",
      "      - Filter: type = 'historical'\n",
      "      - Color: 根據 risk_score (使用紅色漸變)\n",
      "      - Size: 根據 value (事故數量)\n",
      "      - Radius: 5-10 pixels\n",
      "      \n",
      "   b) **熱力圖層 (Heatmap Layer) - 風險熱點**\n",
      "      - 使用所有數據點\n",
      "      - Radius: 20-30 km\n",
      "      - Weight: risk_score\n",
      "      - Intensity: 1-3\n",
      "      \n",
      "   c) **六邊形圖層 (Hexagon Layer) - 預測風險聚合**\n",
      "      - Filter: type = 'prediction'\n",
      "      - Height: 基於 risk_score 總和\n",
      "      - Color: 使用紅黃綠漸變\n",
      "      - Radius: 5-10 km\n",
      "\n",
      "   d) **弧線圖層 (Arc Layer) - 風險傳播路徑**（可選）\n",
      "      - 連接高風險區域\n",
      "      - 顏色表示風險等級\n",
      "\n",
      "4. **配置時間動畫**\n",
      "   - 點擊左側面板的 \"Filters\"\n",
      "   - 添加 timestamp 過濾器\n",
      "   - 啟用時間動畫播放（點擊播放按鈕）\n",
      "   - 調整播放速度和時間窗口\n",
      "\n",
      "5. **互動功能**\n",
      "   - 懸停查看詳細信息\n",
      "   - 點擊數據點查看完整屬性\n",
      "   - 使用圖層可見性開關比較不同視圖\n",
      "   - 調整透明度查看重疊區域\n",
      "\n",
      "6. **3D 視覺化**\n",
      "   - 點擊右上角的 3D 按鈕\n",
      "   - 使用滑鼠拖動旋轉視角\n",
      "   - 滾輪縮放\n",
      "\n",
      "7. **導出和分享**\n",
      "   - 配置完成後，點擊 \"Export Map\"\n",
      "   - 選擇 \"Export as HTML\" 生成獨立網頁\n",
      "   - 或選擇 \"Export Config\" 保存配置文件\n",
      "\n",
      "### 數據欄位說明：\n",
      "- **lat, lng**: 地理座標\n",
      "- **timestamp**: 時間戳（用於時間動畫）\n",
      "- **type**: 數據類型（historical=歷史數據, prediction=預測數據）\n",
      "- **value**: 歷史事故數量\n",
      "- **risk_score**: 風險分數（0-1）\n",
      "- **category**: 風險等級（High/Medium/Low）\n",
      "\n",
      "### 進階技巧：\n",
      "1. **多層疊加**：同時顯示歷史和預測數據，使用不同顏色區分\n",
      "2. **時間比較**：使用分屏功能比較不同時期的風險分布\n",
      "3. **自定義配色**：根據風險等級使用自定義顏色方案\n",
      "4. **數據過濾**：結合多個過濾器查看特定條件下的風險模式\n",
      "5. **地理圍欄**：繪製多邊形關注特定區域\n",
      "\n",
      "### GPU 加速提示：\n",
      "- Kepler.gl 會自動使用 WebGL 進行 GPU 加速渲染\n",
      "- 如果數據量大，可以調整 \"Resolution\" 降低渲染精度提升性能\n",
      "\n",
      "\n",
      "數據預覽：\n",
      "     lat     lng           timestamp  Hour  DayOfWeek  Month  Start_Lat  \\\n",
      "0  32.63 -117.04 2017-01-27 00:00:00     0          4      1    36.2126   \n",
      "1  32.63 -117.04 2017-01-27 06:00:00     6          4      1    36.2126   \n",
      "2  32.63 -117.04 2017-01-27 12:00:00    12          4      1    36.2126   \n",
      "3  32.63 -117.04 2017-01-27 18:00:00    18          4      1    36.2126   \n",
      "4  32.63 -117.04 2017-01-28 00:00:00     0          5      1    36.2126   \n",
      "5  32.63 -117.04 2017-01-28 06:00:00     6          5      1    36.2126   \n",
      "6  32.63 -117.04 2017-01-28 12:00:00    12          5      1    36.2126   \n",
      "7  32.63 -117.04 2017-01-28 18:00:00    18          5      1    36.2126   \n",
      "8  32.63 -117.04 2017-01-29 00:00:00     0          6      1    36.2126   \n",
      "9  32.63 -117.04 2017-01-29 06:00:00     6          6      1    36.2126   \n",
      "\n",
      "   Start_Lng  Distance(mi)    State  ...  Traffic_Signal  Sunrise_Sunset  \\\n",
      "0   -94.6893        0.5624  20.0787  ...           0.148          0.3126   \n",
      "1   -94.6893        0.5624  20.0787  ...           0.148          0.3126   \n",
      "2   -94.6893        0.5624  20.0787  ...           0.148          0.3126   \n",
      "3   -94.6893        0.5624  20.0787  ...           0.148          0.3126   \n",
      "4   -94.6893        0.5624  20.0787  ...           0.148          0.3126   \n",
      "5   -94.6893        0.5624  20.0787  ...           0.148          0.3126   \n",
      "6   -94.6893        0.5624  20.0787  ...           0.148          0.3126   \n",
      "7   -94.6893        0.5624  20.0787  ...           0.148          0.3126   \n",
      "8   -94.6893        0.5624  20.0787  ...           0.148          0.3126   \n",
      "9   -94.6893        0.5624  20.0787  ...           0.148          0.3126   \n",
      "\n",
      "   Duration_minutes  IsWeekend  IsRushHour  TimeOfDay  Season  \\\n",
      "0          115.4525     0.1596      0.4951       1.49  1.5657   \n",
      "1          115.4525     0.1596      0.4951       1.49  1.5657   \n",
      "2          115.4525     0.1596      0.4951       1.49  1.5657   \n",
      "3          115.4525     0.1596      0.4951       1.49  1.5657   \n",
      "4          115.4525     0.1596      0.4951       1.49  1.5657   \n",
      "5          115.4525     0.1596      0.4951       1.49  1.5657   \n",
      "6          115.4525     0.1596      0.4951       1.49  1.5657   \n",
      "7          115.4525     0.1596      0.4951       1.49  1.5657   \n",
      "8          115.4525     0.1596      0.4951       1.49  1.5657   \n",
      "9          115.4525     0.1596      0.4951       1.49  1.5657   \n",
      "\n",
      "   Weather_Category  risk_score  risk_level  \n",
      "0            1.9317    0.634713      Medium  \n",
      "1            1.9317    0.474547      Medium  \n",
      "2            1.9317    0.519219      Medium  \n",
      "3            1.9317    0.484661      Medium  \n",
      "4            1.9317    0.633580      Medium  \n",
      "5            1.9317    0.491982      Medium  \n",
      "6            1.9317    0.525529      Medium  \n",
      "7            1.9317    0.486168      Medium  \n",
      "8            1.9317    0.678206      Medium  \n",
      "9            1.9317    0.497255      Medium  \n",
      "\n",
      "[10 rows x 37 columns]\n",
      "\n",
      "數據統計：\n",
      "- 總數據點: 60,000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/b11705051/MDS/MDS-Final-Project/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/b11705051/MDS/MDS-Final-Project/.venv/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/b11705051/MDS/MDS-Final-Project/.venv/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'type'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m數據統計：\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- 總數據點: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(kepler_data)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- 歷史數據: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(kepler_data[\u001b[43mkepler_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistorical\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- 預測數據: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(kepler_data[kepler_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- 唯一位置數: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkepler_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlng\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/b11705051/MDS/MDS-Final-Project/.venv/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/b11705051/MDS/MDS-Final-Project/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'type'"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Cell 11: Kepler.gl 使用教學\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Kepler.gl 使用教學\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "instructions = \"\"\"\n",
    "### 如何使用 Kepler.gl 視覺化您的數據：\n",
    "\n",
    "1. **訪問 Kepler.gl**\n",
    "   - 打開瀏覽器，訪問: https://kepler.gl/\n",
    "   - 或使用 Kepler.gl 的 Jupyter 擴展（如果已安裝）\n",
    "\n",
    "2. **上傳數據**\n",
    "   - 點擊 \"Add Data\" 或直接拖拽 'us_accidents_kepler_data.csv' 文件到網頁上\n",
    "   - 等待數據載入完成\n",
    "\n",
    "3. **配置地圖圖層**\n",
    "   建議創建以下圖層來展示不同維度的數據：\n",
    "\n",
    "   a) **點圖層 (Point Layer) - 歷史事故**\n",
    "      - Filter: type = 'historical'\n",
    "      - Color: 根據 risk_score (使用紅色漸變)\n",
    "      - Size: 根據 value (事故數量)\n",
    "      - Radius: 5-10 pixels\n",
    "      \n",
    "   b) **熱力圖層 (Heatmap Layer) - 風險熱點**\n",
    "      - 使用所有數據點\n",
    "      - Radius: 20-30 km\n",
    "      - Weight: risk_score\n",
    "      - Intensity: 1-3\n",
    "      \n",
    "   c) **六邊形圖層 (Hexagon Layer) - 預測風險聚合**\n",
    "      - Filter: type = 'prediction'\n",
    "      - Height: 基於 risk_score 總和\n",
    "      - Color: 使用紅黃綠漸變\n",
    "      - Radius: 5-10 km\n",
    "\n",
    "   d) **弧線圖層 (Arc Layer) - 風險傳播路徑**（可選）\n",
    "      - 連接高風險區域\n",
    "      - 顏色表示風險等級\n",
    "\n",
    "4. **配置時間動畫**\n",
    "   - 點擊左側面板的 \"Filters\"\n",
    "   - 添加 timestamp 過濾器\n",
    "   - 啟用時間動畫播放（點擊播放按鈕）\n",
    "   - 調整播放速度和時間窗口\n",
    "\n",
    "5. **互動功能**\n",
    "   - 懸停查看詳細信息\n",
    "   - 點擊數據點查看完整屬性\n",
    "   - 使用圖層可見性開關比較不同視圖\n",
    "   - 調整透明度查看重疊區域\n",
    "\n",
    "6. **3D 視覺化**\n",
    "   - 點擊右上角的 3D 按鈕\n",
    "   - 使用滑鼠拖動旋轉視角\n",
    "   - 滾輪縮放\n",
    "\n",
    "7. **導出和分享**\n",
    "   - 配置完成後，點擊 \"Export Map\"\n",
    "   - 選擇 \"Export as HTML\" 生成獨立網頁\n",
    "   - 或選擇 \"Export Config\" 保存配置文件\n",
    "\n",
    "### 數據欄位說明：\n",
    "- **lat, lng**: 地理座標\n",
    "- **timestamp**: 時間戳（用於時間動畫）\n",
    "- **type**: 數據類型（historical=歷史數據, prediction=預測數據）\n",
    "- **value**: 歷史事故數量\n",
    "- **risk_score**: 風險分數（0-1）\n",
    "- **category**: 風險等級（High/Medium/Low）\n",
    "\n",
    "### 進階技巧：\n",
    "1. **多層疊加**：同時顯示歷史和預測數據，使用不同顏色區分\n",
    "2. **時間比較**：使用分屏功能比較不同時期的風險分布\n",
    "3. **自定義配色**：根據風險等級使用自定義顏色方案\n",
    "4. **數據過濾**：結合多個過濾器查看特定條件下的風險模式\n",
    "5. **地理圍欄**：繪製多邊形關注特定區域\n",
    "\n",
    "### GPU 加速提示：\n",
    "- Kepler.gl 會自動使用 WebGL 進行 GPU 加速渲染\n",
    "- 如果數據量大，可以調整 \"Resolution\" 降低渲染精度提升性能\n",
    "\"\"\"\n",
    "\n",
    "print(instructions)\n",
    "\n",
    "# 顯示數據預覽和統計\n",
    "kepler_data = pred_df\n",
    "print(\"\\n數據預覽：\")\n",
    "print(kepler_data.head(10))\n",
    "\n",
    "print(f\"\\n數據統計：\")\n",
    "print(f\"- 總數據點: {len(kepler_data):,}\")\n",
    "print(f\"- 歷史數據: {len(kepler_data[kepler_data['type'] == 'historical']):,}\")\n",
    "print(f\"- 預測數據: {len(kepler_data[kepler_data['type'] == 'prediction']):,}\")\n",
    "print(f\"- 唯一位置數: {kepler_data[['lat', 'lng']].drop_duplicates().shape[0]:,}\")\n",
    "print(f\"- 高風險預測點: {len(kepler_data[(kepler_data['type'] == 'prediction') & (kepler_data['category'] == 'High')]):,}\")\n",
    "\n",
    "# 顯示GPU使用情況\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU 記憶體使用: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU 記憶體快取: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"\\n實驗完成！請查看生成的文件和圖表。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
