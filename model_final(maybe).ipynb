{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b086c8e",
   "metadata": {},
   "source": [
    "### US Accidents 資料分析：模型比較與時空風險預測\n",
    "## 實驗設計\n",
    "1. 比較有無資料前處理的影響\n",
    "2. 比較有無混合採樣策略的影響\n",
    "3. 使用三個模型：LightGBM, XGBoost, CatBoost（GPU加速版）\n",
    "4. 包含交叉驗證和進度顯示\n",
    "5. 創建時空風險預測數據供 Kepler.gl 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 1: 導入套件和設定\n",
    "# ===========================\n",
    "import os, time, json, gc, warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# Imbalanced-learn（保留混合採樣）\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# 只保留 XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU 檢查\n",
    "print(\"=\"*60)\n",
    "print(\"環境檢查\")\n",
    "print(\"=\"*60)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da381a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 2: 記憶體優化函數\n",
    "# ===========================\n",
    "\n",
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"通過改變數據類型來減少DataFrame的記憶體使用\"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f'記憶體使用減少了 {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "        print(f'{start_mem:.2f} MB --> {end_mem:.2f} MB')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_memory():\n",
    "    \"\"\"清理記憶體\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ddefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 3: 載入資料（優化版）\n",
    "# ===========================\n",
    "\n",
    "def load_data_optimized(file_path, sample_frac=1):  # 使用10%資料做實驗\n",
    "    \"\"\"優化的資料載入\"\"\"\n",
    "    print(f\"\\n載入資料: {file_path}\")\n",
    "    \n",
    "    # 定義需要的欄位\n",
    "    important_cols = [\n",
    "        'Severity', 'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng',\n",
    "        'Distance(mi)', 'Temperature(F)', 'Humidity(%)', 'Pressure(in)',\n",
    "        'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)',\n",
    "        'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way',\n",
    "        'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop',\n",
    "        'Traffic_Calming', 'Traffic_Signal', 'Sunrise_Sunset', 'State'\n",
    "    ]\n",
    "    \n",
    "    # 載入資料\n",
    "    print(f\"載入 {sample_frac*100}% 的資料...\")\n",
    "    df = pd.read_csv(file_path, usecols=lambda x: x in important_cols)\n",
    "    df = df.sample(frac=sample_frac, random_state=42)\n",
    "    \n",
    "    print(f\"載入資料大小: {df.shape}\")\n",
    "    print(f\"記憶體使用: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # 顯示目標變數分布\n",
    "    print(\"\\n目標變數分布:\")\n",
    "    severity_counts = df['Severity'].value_counts().sort_index()\n",
    "    for sev, count in severity_counts.items():\n",
    "        print(f\"Severity {sev}: {count:,} ({count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 執行載入\n",
    "file_path = 'us-accidents/US_Accidents_March23.csv'\n",
    "df = load_data_optimized(file_path, sample_frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e1219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 4: 基礎特徵工程函數\n",
    "# ===========================\n",
    "\n",
    "def basic_preprocessing(df):\n",
    "    \"\"\"基礎前處理：只處理缺失值和基本轉換\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # 處理日期時間\n",
    "    df_copy['Start_Time'] = pd.to_datetime(df_copy['Start_Time'], errors='coerce')\n",
    "    df_copy['End_Time'] = pd.to_datetime(df_copy['End_Time'], errors='coerce')\n",
    "    \n",
    "    # 計算持續時間\n",
    "    df_copy['Duration_minutes'] = (df_copy['End_Time'] - df_copy['Start_Time']).dt.total_seconds() / 60\n",
    "    \n",
    "    # 過濾異常值\n",
    "    df_copy = df_copy[(df_copy['Duration_minutes'] > 0) & (df_copy['Duration_minutes'] < 1440*7)]\n",
    "    df_copy = df_copy.dropna(subset=['Start_Time'])\n",
    "    \n",
    "    # 提取基本時間特徵\n",
    "    df_copy['Hour'] = df_copy['Start_Time'].dt.hour\n",
    "    df_copy['DayOfWeek'] = df_copy['Start_Time'].dt.dayofweek\n",
    "    df_copy['Month'] = df_copy['Start_Time'].dt.month\n",
    "    \n",
    "    # 處理缺失值（簡單填充）\n",
    "    numeric_cols = df_copy.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col != 'Severity':\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
    "    \n",
    "    # 類別變數填充\n",
    "    categorical_cols = ['Weather_Condition', 'State', 'Sunrise_Sunset']\n",
    "    for col in categorical_cols:\n",
    "        if col in df_copy.columns:\n",
    "            df_copy[col] = df_copy[col].fillna('Unknown')\n",
    "    \n",
    "    # 布林型欄位轉換\n",
    "    bool_cols = df_copy.select_dtypes(include=['bool']).columns\n",
    "    for col in bool_cols:\n",
    "        df_copy[col] = df_copy[col].astype(int)\n",
    "    \n",
    "    # 刪除不需要的欄位\n",
    "    df_copy = df_copy.drop(['Start_Time', 'End_Time'], axis=1, errors='ignore')\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def advanced_preprocessing(df):\n",
    "    \"\"\"進階前處理：包含所有特徵工程\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # 先做基礎處理\n",
    "    df_copy = basic_preprocessing(df_copy)\n",
    "    \n",
    "    # 額外的特徵工程\n",
    "    # 1. 是否週末\n",
    "    df_copy['IsWeekend'] = (df_copy['DayOfWeek'] >= 5).astype(int)\n",
    "    \n",
    "    # 2. 是否尖峰時段\n",
    "    df_copy['IsRushHour'] = df_copy['Hour'].apply(\n",
    "        lambda x: 1 if (6 <= x <= 9) or (16 <= x <= 19) else 0\n",
    "    )\n",
    "    \n",
    "    # 3. 時段分類\n",
    "    df_copy['TimeOfDay'] = pd.cut(df_copy['Hour'], \n",
    "                                  bins=[-1, 6, 12, 18, 24], \n",
    "                                  labels=[0, 1, 2, 3]).astype(int)\n",
    "    \n",
    "    # 4. 季節\n",
    "    df_copy['Season'] = pd.cut(df_copy['Month'], \n",
    "                               bins=[0, 3, 6, 9, 12], \n",
    "                               labels=[0, 1, 2, 3]).astype(int)\n",
    "    \n",
    "    # 5. 天氣分類（如果有天氣條件）\n",
    "    if 'Weather_Condition' in df_copy.columns:\n",
    "        def categorize_weather(condition):\n",
    "            if pd.isna(condition):\n",
    "                return 0\n",
    "            condition = str(condition).lower()\n",
    "            if any(word in condition for word in ['clear', 'fair']):\n",
    "                return 1\n",
    "            elif any(word in condition for word in ['cloud', 'overcast']):\n",
    "                return 2\n",
    "            elif any(word in condition for word in ['rain', 'drizzle']):\n",
    "                return 3\n",
    "            elif any(word in condition for word in ['snow', 'sleet']):\n",
    "                return 4\n",
    "            elif any(word in condition for word in ['fog', 'mist']):\n",
    "                return 5\n",
    "            elif any(word in condition for word in ['storm', 'thunder']):\n",
    "                return 6\n",
    "            else:\n",
    "                return 7\n",
    "        \n",
    "        df_copy['Weather_Category'] = df_copy['Weather_Condition'].apply(categorize_weather)\n",
    "        df_copy = df_copy.drop('Weather_Condition', axis=1)\n",
    "    \n",
    "    # 6. 對類別變數進行標籤編碼\n",
    "    label_encoders = {}\n",
    "    categorical_cols = ['State', 'Sunrise_Sunset']\n",
    "    for col in categorical_cols:\n",
    "        if col in df_copy.columns:\n",
    "            le = LabelEncoder()\n",
    "            df_copy[col] = le.fit_transform(df_copy[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    return df_copy, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b93e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 5: 混合採樣策略\n",
    "# ===========================\n",
    "\n",
    "def apply_mixed_sampling(X_train, y_train):\n",
    "    \"\"\"應用混合採樣策略\"\"\"\n",
    "    print(\"\\n應用混合採樣策略...\")\n",
    "    \n",
    "    # 計算各類別數量\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    class_counts = dict(zip(unique, counts))\n",
    "    print(\"原始分布:\", class_counts)\n",
    "    \n",
    "    # 混合策略：對多數類欠採樣，對少數類過採樣\n",
    "    median_count = int(np.median(counts))\n",
    "    target_count = int(median_count * 1.5)\n",
    "    \n",
    "    # 第一步：欠採樣\n",
    "    undersample_strategy = {}\n",
    "    for cls, cnt in class_counts.items():\n",
    "        if cnt > target_count:\n",
    "            undersample_strategy[cls] = target_count\n",
    "        else:\n",
    "            undersample_strategy[cls] = cnt\n",
    "    \n",
    "    if len(undersample_strategy) > 0:\n",
    "        rus = RandomUnderSampler(sampling_strategy=undersample_strategy, random_state=42)\n",
    "        X_temp, y_temp = rus.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        X_temp, y_temp = X_train, y_train\n",
    "    \n",
    "    # 第二步：過採樣\n",
    "    temp_unique, temp_counts = np.unique(y_temp, return_counts=True)\n",
    "    temp_class_counts = dict(zip(temp_unique, temp_counts))\n",
    "    \n",
    "    oversample_strategy = {}\n",
    "    for cls, cnt in temp_class_counts.items():\n",
    "        if cnt < target_count:\n",
    "            oversample_strategy[cls] = target_count\n",
    "        else:\n",
    "            oversample_strategy[cls] = cnt\n",
    "    \n",
    "    if len(oversample_strategy) > 0:\n",
    "        ros = RandomOverSampler(sampling_strategy=oversample_strategy, random_state=42)\n",
    "        X_resampled, y_resampled = ros.fit_resample(X_temp, y_temp)\n",
    "    else:\n",
    "        X_resampled, y_resampled = X_temp, y_temp\n",
    "    \n",
    "    # 顯示新分布\n",
    "    unique_new, counts_new = np.unique(y_resampled, return_counts=True)\n",
    "    print(\"採樣後分布:\", dict(zip(unique_new, counts_new)))\n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaddd219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 6: XGBoost GPU 訓練器\n",
    "# ===========================\n",
    "def train_xgboost_gpu(\n",
    "        X_train, X_test, y_train, y_test,\n",
    "        X_val=None, y_val=None,\n",
    "        *,\n",
    "        objective='multi:softprob',\n",
    "        num_class=4\n",
    "    ):\n",
    "    \"\"\"\n",
    "    通用 XGBoost GPU 訓練器\n",
    "      - 多分類: objective='multi:softprob'，num_class=類別總數\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'objective': objective,\n",
    "        # ==== 3090 GPU 最佳化 ====\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'predictor':   'gpu_predictor',\n",
    "        'gpu_id': 0,\n",
    "        'max_bin': 256,\n",
    "        'sampling_method': 'gradient_based',\n",
    "        # ==== 常用超參 ====\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.08,\n",
    "        'n_estimators': 2000,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 1,\n",
    "        'gamma': 0.15,\n",
    "        'lambda': 1.0,\n",
    "        'alpha': 0.0,\n",
    "        'n_jobs': os.cpu_count()\n",
    "    }\n",
    "\n",
    "    # 只有在多分類時才加入 num_class & eval_metric\n",
    "    if objective.startswith('multi'):\n",
    "        params['num_class'] = num_class\n",
    "        params['eval_metric'] = ['mlogloss', 'merror']\n",
    "\n",
    "    Model = xgb.XGBClassifier\n",
    "    model = Model(**params)\n",
    "\n",
    "    eval_set = [(X_test, y_test)]\n",
    "    if X_val is not None:\n",
    "        eval_set.append((X_val, y_val))\n",
    "\n",
    "    start = time.time()\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=eval_set,\n",
    "        # early_stopping_rounds=80,\n",
    "        verbose=200\n",
    "    )\n",
    "    train_time = time.time() - start\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    return model, preds, train_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 7: 交叉驗證函數\n",
    "# ===========================\n",
    "\n",
    "def cross_validate_model(model_func, X, y, cv_folds=5):\n",
    "    \"\"\"執行交叉驗證\"\"\"\n",
    "    print(f\"\\n執行 {cv_folds} 折交叉驗證...\")\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    cv_scores = {\n",
    "        'accuracy': [],\n",
    "        'f1_score': [],\n",
    "        'balanced_accuracy': []\n",
    "    }\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tqdm(skf.split(X, y), total=cv_folds, desc=\"CV Progress\")):\n",
    "        X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
    "        y_train_cv, y_val_cv = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # 訓練模型\n",
    "        model, y_pred, _ = model_func(X_train_cv, X_val_cv, y_train_cv, y_val_cv)\n",
    "        \n",
    "        # 計算指標\n",
    "        cv_scores['accuracy'].append(accuracy_score(y_val_cv, y_pred))\n",
    "        cv_scores['f1_score'].append(f1_score(y_val_cv, y_pred, average='weighted'))\n",
    "        cv_scores['balanced_accuracy'].append(balanced_accuracy_score(y_val_cv, y_pred))\n",
    "        \n",
    "        # 清理GPU記憶體\n",
    "        clean_memory()\n",
    "    \n",
    "    # 計算平均值和標準差\n",
    "    results = {}\n",
    "    for metric, scores in cv_scores.items():\n",
    "        results[f'{metric}_mean'] = np.mean(scores)\n",
    "        results[f'{metric}_std'] = np.std(scores)\n",
    "        print(f\"{metric}: {results[f'{metric}_mean']:.4f} (+/- {results[f'{metric}_std']:.4f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d8780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 8: run_experiment（僅嚴重度分類）\n",
    "# ===========================\n",
    "def run_experiment(df):\n",
    "    results = {}\n",
    "\n",
    "    # ---------- Ⅰ. 嚴重度（分類） ----------\n",
    "    df_cls, _ = advanced_preprocessing(df)\n",
    "    df_cls = df_cls[df_cls['Severity'].isin([1,2,3,4])].dropna()\n",
    "\n",
    "    obj_cols = df_cls.select_dtypes(include='object').columns\n",
    "    df_cls[obj_cols] = df_cls[obj_cols].astype('category').apply(lambda s: s.cat.codes)\n",
    "\n",
    "    X_cls = df_cls.drop('Severity', axis=1).values\n",
    "    y_cls = df_cls['Severity'].values - 1      # 0~3\n",
    "\n",
    "    X_tmp, X_test, y_tmp, y_test = train_test_split(\n",
    "        X_cls, y_cls, test_size=0.20, random_state=42, stratify=y_cls\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_tmp, y_tmp, test_size=0.25, random_state=42, stratify=y_tmp\n",
    "    )\n",
    "\n",
    "    X_train_s, y_train_s = apply_mixed_sampling(X_train, y_train)\n",
    "\n",
    "    print(\"\\n--- 訓練【嚴重度】XGBoost 分類 ---\")\n",
    "    sev_model, sev_pred, sev_time = train_xgboost_gpu(\n",
    "        X_train_s, X_test, y_train_s, y_test,\n",
    "        X_val=X_val, y_val=y_val,\n",
    "        objective='multi:softprob', num_class=4\n",
    "    )\n",
    "    sev_acc     = accuracy_score(y_test, sev_pred)\n",
    "    sev_f1      = f1_score(y_test, sev_pred, average='weighted')\n",
    "    sev_bal_acc = balanced_accuracy_score(y_test, sev_pred)\n",
    "    print(f\"Severity 分類結果 → acc: {sev_acc:.4f}, f1: {sev_f1:.4f}, bal_acc: {sev_bal_acc:.4f}, time: {sev_time:.1f}s\")\n",
    "\n",
    "    results['severity'] = {\n",
    "        'acc':        float(sev_acc),\n",
    "        'f1':         float(sev_f1),\n",
    "        'bal_acc':    float(sev_bal_acc),\n",
    "        'train_time': float(sev_time),\n",
    "        'model':      sev_model\n",
    "    }\n",
    "\n",
    "    # ---------- 打印最終結果 ----------\n",
    "    print(\"\\n=== Experiment Summary ===\")\n",
    "    print(f\"嚴重度 (分類):   acc={results['severity']['acc']:.4f}, \"\n",
    "          f\"f1={results['severity']['f1']:.4f}, \"\n",
    "          f\"bal_acc={results['severity']['bal_acc']:.4f}, \"\n",
    "          f\"time={results['severity']['train_time']:.1f}s\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# 執行實驗\n",
    "print(\"\\n開始執行嚴重度分類實驗...\")\n",
    "results = run_experiment(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 10: Kepler.gl Data (真實分布抽樣 + Batch 推論，30 天預測、預估 60 萬筆)\n",
    "# ===========================\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_kepler_predictions_realistic(\n",
    "    df_geo,                # 原始全部資料，需包含 Start_Lat, Start_Lng, Start_Time, State\n",
    "    df_full_processed,     # advanced_preprocessing(df) → drop('Severity') 後的 DataFrame\n",
    "    sev_model,             # 已訓練好的 XGBClassifier\n",
    "    horizon_days=30,       # 預測 30 天\n",
    "    daily_times=[0, 6, 12, 18],  # 每天四個時段\n",
    "    total_predictions=600000     # 目標輸出筆數 (約 0.6M)\n",
    "):\n",
    "    \"\"\"\n",
    "    步驟概覽：\n",
    "    1. 以經緯度四捨五入至 2 位 (lat_bin, lng_bin)，計算每個格點的事故計數。\n",
    "    2. 計算每個格點的抽樣權重 = cnt / sum(cnt)，\n",
    "       依此從所有格點中抽取 n_locs 個格點，n_locs = total_predictions / (horizon_days * len(daily_times))。\n",
    "    3. 生成 (lat, lng, timestamp, Hour, DayOfWeek, Month) 的 Cartesian product → big_df。\n",
    "    4. 其餘所有訓練時用過的特徵用「整體平均值」填入 big_df。\n",
    "    5. 一次性呼叫 sev_model.predict_proba(batch_features)，塞回 risk_score, risk_level。\n",
    "    6. 回傳 big_df。\n",
    "    \"\"\"\n",
    "    # 1. 建立格點 (lat_bin, lng_bin) 並計算計數與權重\n",
    "    df_geo['lat_bin'] = df_geo['Start_Lat'].round(2)\n",
    "    df_geo['lng_bin'] = df_geo['Start_Lng'].round(2)\n",
    "    grid_counts = (\n",
    "        df_geo.groupby(['lat_bin', 'lng_bin'])\n",
    "              .size()\n",
    "              .reset_index(name='cnt')\n",
    "    )\n",
    "    grid_counts['weight'] = grid_counts['cnt'] / grid_counts['cnt'].sum()\n",
    "\n",
    "    # 2. 計算要抽取的格點數量\n",
    "    time_points = horizon_days * len(daily_times)  # 30 * 4 = 120\n",
    "    n_locs = int(total_predictions / time_points)\n",
    "    n_locs = min(n_locs, len(grid_counts))\n",
    "\n",
    "    # 3. 依照權重隨機抽 n_locs 個格點 (無放回)\n",
    "    sampled_idxs = np.random.choice(\n",
    "        grid_counts.index,\n",
    "        size=n_locs,\n",
    "        replace=False,\n",
    "        p=grid_counts['weight'].values\n",
    "    )\n",
    "    sample_locs = grid_counts.loc[sampled_idxs, ['lat_bin', 'lng_bin']].reset_index(drop=True)\n",
    "    sample_locs = sample_locs.rename(columns={'lat_bin':'lat','lng_bin':'lng'})\n",
    "\n",
    "    # 4. 構造所有 (lat, lng, timestamp, Hour, DayOfWeek, Month) 組合\n",
    "    latest_day = df_geo['Start_Time'].max().normalize()\n",
    "    rows = []\n",
    "    for lat, lng in sample_locs.values:\n",
    "        for d in range(1, horizon_days + 1):\n",
    "            ts_base = latest_day + pd.Timedelta(days=d)\n",
    "            dow = ts_base.weekday()\n",
    "            for hr in daily_times:\n",
    "                ts = ts_base + pd.Timedelta(hours=hr)\n",
    "                rows.append((lat, lng, ts, hr, dow, ts.month))\n",
    "    big_df = pd.DataFrame(rows, columns=['lat','lng','timestamp','Hour','DayOfWeek','Month'])\n",
    "\n",
    "    # 5. 其餘訓練特徵以平均值填充\n",
    "    feature_cols = df_full_processed.columns.tolist()\n",
    "    # 檢查常用時間特徵一定存在\n",
    "    for c in ['Hour','DayOfWeek','Month']:\n",
    "        if c not in feature_cols:\n",
    "            raise ValueError(f\"訓練特徵缺少 '{c}'，請先確認 advanced_preprocessing 有生成這三個欄。\")\n",
    "    template = df_full_processed[feature_cols].mean().round(4)\n",
    "    for col in feature_cols:\n",
    "        if col in ['Hour','DayOfWeek','Month']:\n",
    "            continue\n",
    "        big_df[col] = float(template[col])\n",
    "\n",
    "    # 6. Batch 推論：一次性呼叫 predict_proba\n",
    "    X_all = big_df[feature_cols].values  # shape=(n_locs*120, len(feature_cols))\n",
    "    all_probs = sev_model.predict_proba(X_all)  # shape=(n_rows, num_class)\n",
    "    risk_scores = all_probs.max(axis=1)  # 取最大機率作為風險分數\n",
    "    risk_levels = np.where(risk_scores > 0.7, 'High',\n",
    "                  np.where(risk_scores > 0.4, 'Medium', 'Low'))\n",
    "    big_df['risk_score'] = risk_scores\n",
    "    big_df['risk_level'] = risk_levels\n",
    "\n",
    "    return big_df\n",
    "\n",
    "\n",
    "# ——— 重新讀取完整地理資料，這次要帶 State，用 grid_counts 時其實不需要 State，但不影響抽樣分布 —— \n",
    "df_geo_full = pd.read_csv(\n",
    "    file_path,\n",
    "    usecols=['Start_Lat','Start_Lng','Start_Time','State'],\n",
    "    parse_dates=['Start_Time']\n",
    ")\n",
    "df_geo = df_geo_full.copy()\n",
    "\n",
    "# ——— 取得與訓練時相同的完整特徵 DataFrame —— \n",
    "df_full_processed, _ = advanced_preprocessing(df.copy())\n",
    "df_full_processed = df_full_processed.drop('Severity', axis=1)\n",
    "\n",
    "# ——— 執行 Batch 預測，預測 30 天，共約 600k 筆資料 —— \n",
    "print(\"\\n[Kepler] 以真實格點分布抽樣，開始 Batch 推論 30 天 …\")\n",
    "start_time = time.time()\n",
    "pred_df = create_kepler_predictions_realistic(\n",
    "    df_geo,\n",
    "    df_full_processed,\n",
    "    results['severity']['model'],\n",
    "    horizon_days=30,         # 預測未來 30 天\n",
    "    daily_times=[0,6,12,18],  # 每天 4 個時段\n",
    "    total_predictions=1000000  # 共約 600k 筆輸出\n",
    ")\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"→ 完成 {len(pred_df):,} 筆預測，花費 {elapsed:.1f} 秒\")\n",
    "\n",
    "# ——— 將結果存為 CSV，方便 Kepler.gl 使用 —— \n",
    "csv_out = 'accident_severity_forecast_kepler_30days_realistic.csv'\n",
    "pred_df.to_csv(csv_out, index=False)\n",
    "print(f\"Kepler CSV 已存：{csv_out}  （檔案大小約 { (pred_df.memory_usage(deep=True).sum()/1024**2):.1f } MB）\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
