# ===========================
# Cell 1: å°å…¥å¥—ä»¶å’Œè¨­å®š
# ===========================
import pandas as pd
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import gc  # åƒåœ¾å›æ”¶
warnings.filterwarnings('ignore')

# åŸºæœ¬å¥—ä»¶
import os
import time
import joblib
import json
from collections import Counter

# Scikit-learn
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, f1_score, classification_report, 
    confusion_matrix, balanced_accuracy_score, 
    cohen_kappa_score, make_scorer
)
from sklearn.utils.class_weight import compute_class_weight

# è™•ç†ä¸å¹³è¡¡è³‡æ–™
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.ensemble import BalancedRandomForestClassifier

# Boostingæ¨¡å‹
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier

# PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler
import torch.nn.functional as F 

print("ç’°å¢ƒæª¢æŸ¥:")
print(f"PyTorch: {torch.__version__}")
print(f"XGBoost: {xgb.__version__}")
print(f"LightGBM: {lgb.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# ===========================
# Cell 2: è¨˜æ†¶é«”å„ªåŒ–å‡½æ•¸
# ===========================

def reduce_memory_usage(df, verbose=True):
    """
    é€šéæ”¹è®Šæ•¸æ“šé¡å‹ä¾†æ¸›å°‘DataFrameçš„è¨˜æ†¶é«”ä½¿ç”¨
    åƒè€ƒè‡ªKaggleçš„è¨˜æ†¶é«”å„ªåŒ–æŠ€è¡“
    """
    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
    start_mem = df.memory_usage().sum() / 1024**2
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type in numerics:
            c_min = df[col].min()
            c_max = df[col].max()
            
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
    
    end_mem = df.memory_usage().sum() / 1024**2
    if verbose:
        print(f'è¨˜æ†¶é«”ä½¿ç”¨æ¸›å°‘äº† {100 * (start_mem - end_mem) / start_mem:.1f}%')
        print(f'{start_mem:.2f} MB --> {end_mem:.2f} MB')
    
    return df

def clean_memory():
    """æ¸…ç†è¨˜æ†¶é«”"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# ===========================
# Cell 3: è¼‰å…¥è³‡æ–™ï¼ˆå„ªåŒ–ç‰ˆï¼‰
# ===========================

def load_data_optimized(file_path, sample_frac=None, chunksize=None):
    """
    å„ªåŒ–çš„è³‡æ–™è¼‰å…¥ï¼Œæ”¯æ´æ¡æ¨£å’Œåˆ†å¡Šè®€å–
    """
    print(f"è¼‰å…¥è³‡æ–™: {file_path}")
    
    # å…ˆè®€å–ä¸€å°éƒ¨åˆ†ä¾†äº†è§£è³‡æ–™
    sample_df = pd.read_csv(file_path, nrows=5)
    print("è³‡æ–™æ¬„ä½é è¦½:")
    print(sample_df.columns.tolist())
    
    # å®šç¾©éœ€è¦çš„æ¬„ä½ï¼ˆæ’é™¤ä¸éœ€è¦çš„æ–‡å­—æ¬„ä½ä»¥ç¯€çœè¨˜æ†¶é«”ï¼‰
    # æ ¹æ“šå…¶ä»–Kaggle notebookçš„ç¶“é©—ï¼Œé€™äº›æ˜¯æœ€é‡è¦çš„æ¬„ä½
    important_cols = [
        'Severity', 'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng',
        'Distance(mi)', 'Temperature(F)', 'Humidity(%)', 'Pressure(in)',
        'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)',
        'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way',
        'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop',
        'Traffic_Calming', 'Traffic_Signal', 'Sunrise_Sunset', 'State',
        'Side', 'Weather_Timestamp'
    ]
    
    # éæ¿¾å­˜åœ¨çš„æ¬„ä½
    existing_cols = [col for col in important_cols if col in sample_df.columns]
    
    # å®šç¾©æ•¸æ“šé¡å‹ä»¥æ¸›å°‘è¨˜æ†¶é«”
    dtype_dict = {
        'Severity': 'int8',
        'Distance(mi)': 'float32',
        'Temperature(F)': 'float32',
        'Humidity(%)': 'float32',
        'Pressure(in)': 'float32',
        'Visibility(mi)': 'float32',
        'Wind_Speed(mph)': 'float32',
        'Precipitation(in)': 'float32',
        'Amenity': 'bool',
        'Bump': 'bool',
        'Crossing': 'bool',
        'Give_Way': 'bool',
        'Junction': 'bool',
        'No_Exit': 'bool',
        'Railway': 'bool',
        'Roundabout': 'bool',
        'Station': 'bool',
        'Stop': 'bool',
        'Traffic_Calming': 'bool',
        'Traffic_Signal': 'bool'
    }
    
    # è¼‰å…¥è³‡æ–™
    if sample_frac:
        # éš¨æ©Ÿæ¡æ¨£
        print(f"è¼‰å…¥ {sample_frac*100}% çš„è³‡æ–™...")
        df = pd.read_csv(file_path, usecols=existing_cols, dtype=dtype_dict)
        df = df.sample(frac=sample_frac, random_state=42)
    elif chunksize:
        # åˆ†å¡Šè¼‰å…¥
        print(f"åˆ†å¡Šè¼‰å…¥ï¼Œæ¯å¡Š {chunksize} è¡Œ...")
        chunks = []
        for chunk in pd.read_csv(file_path, usecols=existing_cols, 
                                dtype=dtype_dict, chunksize=chunksize):
            chunks.append(chunk)
            if len(chunks) * chunksize >= 1000000:  # é™åˆ¶åœ¨100è¬è¡Œ
                break
        df = pd.concat(chunks, ignore_index=True)
    else:
        # å®Œæ•´è¼‰å…¥
        df = pd.read_csv(file_path, usecols=existing_cols, dtype=dtype_dict)
    
    print(f"è¼‰å…¥è³‡æ–™å¤§å°: {df.shape}")
    print(f"è¨˜æ†¶é«”ä½¿ç”¨: {df.memory_usage().sum() / 1024**2:.2f} MB")
    
    # é¡¯ç¤ºç›®æ¨™è®Šæ•¸åˆ†å¸ƒ
    print("\nç›®æ¨™è®Šæ•¸åˆ†å¸ƒ:")
    severity_counts = df['Severity'].value_counts().sort_index()
    for sev, count in severity_counts.items():
        print(f"Severity {sev}: {count:,} ({count/len(df)*100:.2f}%)")
    
    return df

# åŸ·è¡Œè¼‰å…¥ï¼ˆå»ºè­°å…ˆç”¨å°æ¨£æœ¬æ¸¬è©¦ï¼‰
file_path = 'us-accidents/US_Accidents_March23.csv'

# é¸é …1: ä½¿ç”¨éƒ¨åˆ†è³‡æ–™ï¼ˆæ¨è–¦ç”¨æ–¼æ¸¬è©¦ï¼‰
# df = load_data_optimized(file_path, sample_frac=0.1)  # 10%è³‡æ–™

# é¸é …2: åˆ†å¡Šè¼‰å…¥
# df = load_data_optimized(file_path, chunksize=500000)  # æ¯æ¬¡50è¬è¡Œ

# é¸é …3: å®Œæ•´è¼‰å…¥ï¼ˆéœ€è¦å¤§é‡è¨˜æ†¶é«”ï¼‰
df = load_data_optimized(file_path)

# ===========================
# Cell 4: æ—¥æœŸæ™‚é–“è™•ç†ï¼ˆå„ªåŒ–ç‰ˆï¼‰
# ===========================

def process_datetime_features(df):
    """è™•ç†æ—¥æœŸæ™‚é–“ç‰¹å¾µ"""
    print("\nè™•ç†æ—¥æœŸæ™‚é–“ç‰¹å¾µ...")
    
    # è½‰æ›æ—¥æœŸæ™‚é–“
    df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')
    df['End_Time'] = pd.to_datetime(df['End_Time'], errors='coerce')
    
    # è¨ˆç®—æŒçºŒæ™‚é–“
    df['Duration_minutes'] = (df['End_Time'] - df['Start_Time']).dt.total_seconds() / 60
    
    # éæ¿¾ç•°å¸¸å€¼ï¼ˆä½¿ç”¨æ›´å¯¬é¬†çš„ç¯„åœï¼‰
    df = df[(df['Duration_minutes'] > 0) & (df['Duration_minutes'] < 1440*7)]  # å°æ–¼7å¤©
    
    # ç§»é™¤æ—¥æœŸæ™‚é–“ç‚ºç©ºçš„è¨˜éŒ„
    df = df.dropna(subset=['Start_Time'])
    
    # æå–æ™‚é–“ç‰¹å¾µ
    df['Hour'] = df['Start_Time'].dt.hour.astype('int8')
    df['DayOfWeek'] = df['Start_Time'].dt.dayofweek.astype('int8')
    df['Month'] = df['Start_Time'].dt.month.astype('int8')
    df['Year'] = df['Start_Time'].dt.year.astype('int16')
    
    # è¡ç”Ÿç‰¹å¾µ
    df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype('int8')
    df['IsRushHour'] = df['Hour'].apply(
        lambda x: 1 if (6 <= x <= 9) or (16 <= x <= 19) else 0
    ).astype('int8')
    
    # æ™‚æ®µåˆ†é¡
    df['TimeOfDay'] = pd.cut(df['Hour'], 
                            bins=[-1, 6, 12, 18, 24], 
                            labels=[0, 1, 2, 3]).astype('int8')  # è½‰æ›ç‚ºæ•¸å€¼
    
    # å­£ç¯€
    df['Season'] = pd.cut(df['Month'], 
                         bins=[0, 3, 6, 9, 12], 
                         labels=[0, 1, 2, 3]).astype('int8')  # è½‰æ›ç‚ºæ•¸å€¼
    
    # åˆªé™¤åŸå§‹æ™‚é–“æ¬„ä½ä»¥ç¯€çœè¨˜æ†¶é«”
    df = df.drop(['Start_Time', 'End_Time', 'Weather_Timestamp'], axis=1, errors='ignore')
    
    print(f"è™•ç†å¾Œå¤§å°: {df.shape}")
    clean_memory()
    
    return df

df = process_datetime_features(df)

# ===========================
# Cell 5: å¤©æ°£ç‰¹å¾µè™•ç†
# ===========================

def process_weather_features(df):
    """è™•ç†å¤©æ°£ç›¸é—œç‰¹å¾µ"""
    print("\nè™•ç†å¤©æ°£ç‰¹å¾µ...")
    
    if 'Weather_Condition' in df.columns:
        # ç°¡åŒ–å¤©æ°£åˆ†é¡
        def categorize_weather(condition):
            if pd.isna(condition):
                return 0  # Unknown
            condition = str(condition).lower()
            if any(word in condition for word in ['clear', 'fair']):
                return 1  # Clear
            elif any(word in condition for word in ['cloud', 'overcast']):
                return 2  # Cloudy
            elif any(word in condition for word in ['rain', 'drizzle']):
                return 3  # Rain
            elif any(word in condition for word in ['snow', 'sleet']):
                return 4  # Snow
            elif any(word in condition for word in ['fog', 'mist']):
                return 5  # Fog
            elif any(word in condition for word in ['storm', 'thunder']):
                return 6  # Storm
            else:
                return 7  # Other
        
        df['Weather_Category'] = df['Weather_Condition'].apply(categorize_weather).astype('int8')
        df = df.drop('Weather_Condition', axis=1)
    
    # è™•ç†å…¶ä»–å¤©æ°£æ•¸å€¼ç‰¹å¾µçš„ç¼ºå¤±å€¼
    weather_numeric_cols = ['Temperature(F)', 'Humidity(%)', 'Pressure(in)', 
                           'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']
    
    for col in weather_numeric_cols:
        if col in df.columns:
            # ä½¿ç”¨ä¸­ä½æ•¸å¡«å……
            median_val = df[col].median()
            df[col] = df[col].fillna(median_val)
    
    clean_memory()
    return df

df = process_weather_features(df)

# ===========================
# Cell 6: è™•ç†ç¼ºå¤±å€¼å’Œç·¨ç¢¼é¡åˆ¥è®Šæ•¸
# ===========================

def handle_missing_and_encode(df):
    """è™•ç†ç¼ºå¤±å€¼ä¸¦ç·¨ç¢¼é¡åˆ¥è®Šæ•¸"""
    print("\nè™•ç†ç¼ºå¤±å€¼å’Œç·¨ç¢¼...")
    
    # åˆªé™¤ç¼ºå¤±å€¼éå¤šçš„æ¬„ä½
    missing_pct = df.isnull().sum() / len(df)
    high_missing_cols = missing_pct[missing_pct > 0.5].index.tolist()
    
    # ä¿ç•™Severity
    if 'Severity' in high_missing_cols:
        high_missing_cols.remove('Severity')
    
    df = df.drop(columns=high_missing_cols, errors='ignore')
    print(f"åˆªé™¤é«˜ç¼ºå¤±ç‡æ¬„ä½: {len(high_missing_cols)}")
    
    # å°é¡åˆ¥è®Šæ•¸é€²è¡Œæ¨™ç±¤ç·¨ç¢¼
    categorical_cols = ['State', 'Side', 'Sunrise_Sunset']
    label_encoders = {}
    
    for col in categorical_cols:
        if col in df.columns:
            le = LabelEncoder()
            # å¡«å……ç¼ºå¤±å€¼
            df[col] = df[col].fillna('Unknown')
            # ç·¨ç¢¼
            df[col] = le.fit_transform(df[col].astype(str))
            label_encoders[col] = le
    
    # å¡«å……æ•¸å€¼å‹ç¼ºå¤±å€¼
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col != 'Severity':
            df[col] = df[col].fillna(df[col].median())
    
    # ç¢ºä¿å¸ƒæ—å‹æ¬„ä½æ˜¯æ•´æ•¸
    bool_cols = df.select_dtypes(include=['bool']).columns
    for col in bool_cols:
        df[col] = df[col].astype('int8')
    
    print(f"è™•ç†å¾Œè³‡æ–™å¤§å°: {df.shape}")
    print(f"å‰©é¤˜ç¼ºå¤±å€¼: {df.isnull().sum().sum()}")
    
    clean_memory()
    return df, label_encoders

df, label_encoders = handle_missing_and_encode(df)

# ===========================
# Cell 7: ç‰¹å¾µé¸æ“‡å’Œæº–å‚™æœ€çµ‚æ•¸æ“š
# ===========================

def prepare_final_data(df):
    """æº–å‚™æœ€çµ‚çš„è¨“ç·´æ•¸æ“š"""
    print("\næº–å‚™æœ€çµ‚æ•¸æ“š...")
    
    # åˆªé™¤ä»»ä½•ä»æœ‰ç¼ºå¤±å€¼çš„è¡Œ
    df = df.dropna()
    
    # ç¢ºä¿Severityæ˜¯æ­£ç¢ºçš„å€¼
    df = df[df['Severity'].isin([1, 2, 3, 4])]
    
    # æ ¹æ“šKaggleä¸Šçš„å»ºè­°ï¼Œè€ƒæ…®åˆä½µSeverity 1å’Œ2
    # å› ç‚ºSeverity 1çš„æ¨£æœ¬å¤ªå°‘
    print("\nåŸå§‹é¡åˆ¥åˆ†å¸ƒ:")
    print(df['Severity'].value_counts().sort_index())
    
    # é¸é …ï¼šåˆä½µé¡åˆ¥ï¼ˆå¯é¸ï¼‰
    # df['Severity'] = df['Severity'].replace({1: 2})
    
    # åˆ†é›¢ç‰¹å¾µå’Œç›®æ¨™
    feature_cols = [col for col in df.columns if col != 'Severity']
    X = df[feature_cols].values
    y = df['Severity'].values - 1  # è½‰æ›ç‚º0-3
    
    print(f"\næœ€çµ‚æ•¸æ“šå¤§å°: X={X.shape}, y={y.shape}")
    print("æœ€çµ‚é¡åˆ¥åˆ†å¸ƒ:")
    unique, counts = np.unique(y, return_counts=True)
    for cls, cnt in zip(unique, counts):
        print(f"  é¡åˆ¥ {cls} (Severity {cls+1}): {cnt:,} ({cnt/len(y)*100:.2f}%)")
    
    return X, y, feature_cols

X, y, feature_names = prepare_final_data(df)

# ===========================
# Cell 8: è³‡æ–™åˆ†å‰²
# ===========================

# åˆ†å±¤åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"è¨“ç·´é›†: {X_train.shape}")
print(f"æ¸¬è©¦é›†: {X_test.shape}")

# è¨ˆç®—é¡åˆ¥æ¬Šé‡
class_weights = compute_class_weight('balanced', 
                                   classes=np.unique(y_train), 
                                   y=y_train)
class_weight_dict = dict(enumerate(class_weights))

print("\né¡åˆ¥æ¬Šé‡:")
for cls, weight in class_weight_dict.items():
    print(f"  é¡åˆ¥ {cls}: {weight:.4f}")

# ===========================
# Cell 9: è™•ç†ä¸å¹³è¡¡ - æ··åˆæ¡æ¨£ç­–ç•¥
# ===========================

def apply_mixed_sampling(X_train, y_train, strategy='mixed'):
    """
    æ‡‰ç”¨æ··åˆæ¡æ¨£ç­–ç•¥
    åƒè€ƒKaggleæœ€ä½³å¯¦è¸ï¼šçµåˆéæ¡æ¨£å’Œæ¬ æ¡æ¨£
    """
    print(f"\næ‡‰ç”¨æ¡æ¨£ç­–ç•¥: {strategy}")
    
    if strategy == 'none':
        return X_train, y_train
    
    # è¨ˆç®—å„é¡åˆ¥æ•¸é‡
    unique, counts = np.unique(y_train, return_counts=True)
    class_counts = dict(zip(unique, counts))
    print("åŸå§‹åˆ†å¸ƒ:", class_counts)
    
    if strategy == 'mixed':
        # æ··åˆç­–ç•¥ï¼šå°å¤šæ•¸é¡æ¬ æ¡æ¨£ï¼Œå°å°‘æ•¸é¡éæ¡æ¨£
        # ç›®æ¨™ï¼šè®“æ‰€æœ‰é¡åˆ¥æ¥è¿‘ä¸­ä½æ•¸
        median_count = int(np.median(counts))
        target_count = int(median_count * 1.5)  # ç›®æ¨™æ•¸é‡è¨­ç‚ºä¸­ä½æ•¸çš„1.5å€
        
        # ç¬¬ä¸€æ­¥ï¼šæ¬ æ¡æ¨£ - åªå°è¶…éç›®æ¨™æ•¸é‡çš„é¡åˆ¥é€²è¡Œæ¬ æ¡æ¨£
        undersample_strategy = {}
        for cls, cnt in class_counts.items():
            if cnt > target_count:
                undersample_strategy[cls] = target_count
            else:
                undersample_strategy[cls] = cnt  # ä¿æŒåŸæ¨£
        
        if len(undersample_strategy) > 0 and any(cnt < class_counts[cls] for cls, cnt in undersample_strategy.items()):
            rus = RandomUnderSampler(sampling_strategy=undersample_strategy, random_state=42)
            X_temp, y_temp = rus.fit_resample(X_train, y_train)
        else:
            X_temp, y_temp = X_train, y_train
        
        # ç¬¬äºŒæ­¥ï¼šéæ¡æ¨£ - åªå°å°‘æ–¼ç›®æ¨™æ•¸é‡çš„é¡åˆ¥é€²è¡Œéæ¡æ¨£
        temp_unique, temp_counts = np.unique(y_temp, return_counts=True)
        temp_class_counts = dict(zip(temp_unique, temp_counts))
        
        oversample_strategy = {}
        for cls, cnt in temp_class_counts.items():
            if cnt < target_count:
                oversample_strategy[cls] = target_count
            else:
                oversample_strategy[cls] = cnt  # ä¿æŒåŸæ¨£
        
        if len(oversample_strategy) > 0 and any(cnt > temp_class_counts[cls] for cls, cnt in oversample_strategy.items()):
            ros = RandomOverSampler(sampling_strategy=oversample_strategy, random_state=42)
            X_resampled, y_resampled = ros.fit_resample(X_temp, y_temp)
        else:
            X_resampled, y_resampled = X_temp, y_temp
            
    elif strategy == 'smote':
        # SMOTEç­–ç•¥ï¼šåªéæ¡æ¨£åˆ°æœ€å¤šé¡åˆ¥çš„50%
        max_count = max(counts)
        target_count = int(max_count * 0.5)
        
        # ç¢ºä¿ç›®æ¨™æ•¸é‡ä¸å°æ–¼ç•¶å‰æ•¸é‡
        sampling_strategy = {}
        for cls, cnt in class_counts.items():
            if cnt < target_count:
                sampling_strategy[cls] = target_count
            else:
                sampling_strategy[cls] = cnt
        
        smote = SMOTE(sampling_strategy=sampling_strategy, k_neighbors=5, random_state=42)
        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
    
    elif strategy == 'undersample_only':
        # åªæ¬ æ¡æ¨£åˆ°æœ€å°‘é¡åˆ¥çš„2å€
        min_count = min(counts)
        target_count = min_count * 2
        
        sampling_strategy = {}
        for cls, cnt in class_counts.items():
            sampling_strategy[cls] = min(cnt, target_count)
        
        rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)
        X_resampled, y_resampled = rus.fit_resample(X_train, y_train)
    
    # é¡¯ç¤ºæ–°åˆ†å¸ƒ
    unique_new, counts_new = np.unique(y_resampled, return_counts=True)
    new_distribution = dict(zip(unique_new, counts_new))
    print("æ¡æ¨£å¾Œåˆ†å¸ƒ:", new_distribution)
    
    # é¡¯ç¤ºè®ŠåŒ–
    print("\næ¡æ¨£è®ŠåŒ–:")
    for cls in range(4):
        original = class_counts.get(cls, 0)
        new = new_distribution.get(cls, 0)
        change = ((new - original) / original * 100) if original > 0 else 0
        print(f"  é¡åˆ¥ {cls}: {original:,} â†’ {new:,} ({change:+.1f}%)")
    
    return X_resampled, y_resampled

# æ‡‰ç”¨æ··åˆæ¡æ¨£
# å¯ä»¥å˜—è©¦ä¸åŒç­–ç•¥
X_train_balanced, y_train_balanced = apply_mixed_sampling(X_train, y_train, 'mixed')

# å¦‚æœæ··åˆç­–ç•¥é‚„æ˜¯æœ‰å•é¡Œï¼Œå¯ä»¥å˜—è©¦å…¶ä»–ç­–ç•¥ï¼š
# X_train_balanced, y_train_balanced = apply_mixed_sampling(X_train, y_train, 'undersample_only')
# æˆ–è€…ä¸é€²è¡Œæ¡æ¨£ï¼š
# X_train_balanced, y_train_balanced = X_train, y_train

# ===========================
# Cell 10: LightGBMæ¨¡å‹ï¼ˆå„ªåŒ–ç‰ˆï¼‰
# ===========================

def train_lightgbm_optimized(X_train, X_test, y_train, y_test, class_weights):
    """è¨“ç·´å„ªåŒ–çš„LightGBMæ¨¡å‹"""
    print("\nè¨“ç·´ LightGBM (å„ªåŒ–ç‰ˆ)...")
    
    # å‰µå»ºæ•¸æ“šé›†
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
    
    # åƒæ•¸è¨­ç½®ï¼ˆåŸºæ–¼Kaggleæœ€ä½³å¯¦è¸ï¼‰
    params = {
        'objective': 'multiclass',
        'num_class': 4,
        'metric': 'multi_logloss',
        'boosting_type': 'gbdt',
        'num_leaves': 127,
        'max_depth': -1,
        'learning_rate': 0.05,
        'n_estimators': 1000,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1,
        'min_child_samples': 20,
        'min_split_gain': 0.02,
        'class_weight': 'balanced',
        'device': 'gpu' if torch.cuda.is_available() else 'cpu',
        'gpu_platform_id': 0,
        'gpu_device_id': 0,
        'verbose': -1,
        'random_state': 42,
        'n_jobs': -1
    }
    
    # è¨“ç·´
    start_time = time.time()
    
    model = lgb.train(
        params,
        train_data,
        valid_sets=[valid_data],
        num_boost_round=1000,
        callbacks=[
            lgb.early_stopping(stopping_rounds=50),
            lgb.log_evaluation(period=100)
        ]
    )
    
    train_time = time.time() - start_time
    
    # é æ¸¬
    y_pred = model.predict(X_test, num_iteration=model.best_iteration)
    y_pred = np.argmax(y_pred, axis=1)
    
    # è©•ä¼°
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    balanced_acc = balanced_accuracy_score(y_test, y_pred)
    
    print(f"\nè¨“ç·´æ™‚é–“: {train_time:.2f} ç§’")
    print(f"æœ€ä½³è¿­ä»£æ¬¡æ•¸: {model.best_iteration}")
    print(f"æº–ç¢ºç‡: {accuracy:.4f}")
    print(f"F1åˆ†æ•¸: {f1:.4f}")
    print(f"å¹³è¡¡æº–ç¢ºç‡: {balanced_acc:.4f}")
    
    # è©³ç´°å ±å‘Š
    print("\nåˆ†é¡å ±å‘Š:")
    print(classification_report(y_test, y_pred, 
                              target_names=[f'Severity {i+1}' for i in range(4)]))
    
    return model, accuracy, f1, balanced_acc

# è¨“ç·´æ¨¡å‹
lgb_model, lgb_acc, lgb_f1, lgb_balanced_acc = train_lightgbm_optimized(
    X_train_balanced, X_test, y_train_balanced, y_test, class_weight_dict
)

# ===========================
# Cell 11: XGBoostæ¨¡å‹ï¼ˆç©©å®šç‰ˆï¼‰
# ===========================

def train_xgboost_stable(X_train, X_test, y_train, y_test, use_sample_weight=True):
    """ç©©å®šç‰ˆXGBoost"""
    print("\nè¨“ç·´ XGBoost (ç©©å®šç‰ˆ)...")
    
    # ä½¿ç”¨åŸå§‹çš„é¡åˆ¥æ¬Šé‡ï¼Œä½†ä¸è¦å¤ªæ¥µç«¯
    if use_sample_weight:
        # æº«å’Œçš„é¡åˆ¥æ¬Šé‡
        unique, counts = np.unique(y_train, return_counts=True)
        weight_dict = {}
        max_count = max(counts)
        for cls, count in zip(unique, counts):
            # æ¬Šé‡ä¸è¶…é10å€
            weight_dict[cls] = min(max_count / count, 10.0)
        
        sample_weights = np.array([weight_dict[y] for y in y_train])
    else:
        sample_weights = None
    
    # XGBooståƒæ•¸
    params = {
        'objective': 'multi:softprob',
        'num_class': 4,
        'max_depth': 6,
        'learning_rate': 0.1,
        'n_estimators': 300,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'min_child_weight': 5,  # å¢åŠ ä»¥é˜²æ­¢éæ“¬åˆ
        'gamma': 0.1,
        'reg_alpha': 0.1,
        'reg_lambda': 1,
        'tree_method': 'gpu_hist' if torch.cuda.is_available() else 'hist',
        'random_state': 42,
        'use_label_encoder': False,
        'eval_metric': 'mlogloss'
    }
    
    # è¨“ç·´
    model = xgb.XGBClassifier(**params)
    
    start_time = time.time()
    model.fit(
        X_train, y_train,
        sample_weight=sample_weights,
        eval_set=[(X_test, y_test)],
        # early_stopping_rounds=50,
        verbose=100
    )
    train_time = time.time() - start_time
    
    # é æ¸¬
    y_pred = model.predict(X_test)
    
    # è©•ä¼°
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    balanced_acc = balanced_accuracy_score(y_test, y_pred)
    
    print(f"\nè¨“ç·´æ™‚é–“: {train_time:.2f} ç§’")
    print(f"æº–ç¢ºç‡: {accuracy:.4f}")
    print(f"F1åˆ†æ•¸: {f1:.4f}")
    print(f"å¹³è¡¡æº–ç¢ºç‡: {balanced_acc:.4f}")
    
    print("\nåˆ†é¡å ±å‘Š:")
    print(classification_report(y_test, y_pred, 
                              target_names=[f'Severity {i+1}' for i in range(4)]))
    
    return model, accuracy, f1, balanced_acc

# åŸ·è¡Œè¨“ç·´
xgb_model, xgb_acc, xgb_f1, xgb_balanced_acc = train_xgboost_stable(
    X_train_balanced, X_test, y_train_balanced, y_test
)

# ===========================
# Cell 12: CatBoostæ¨¡å‹
# ===========================

def train_catboost_optimized(X_train, X_test, y_train, y_test):
    """è¨“ç·´å„ªåŒ–çš„CatBoostæ¨¡å‹"""
    print("\nè¨“ç·´ CatBoost (å„ªåŒ–ç‰ˆ)...")
    
    # CatBooståƒæ•¸
    model = CatBoostClassifier(
        iterations=1000,
        depth=8,
        learning_rate=0.05,
        loss_function='MultiClass',
        eval_metric='TotalF1',
        auto_class_weights='Balanced',
        l2_leaf_reg=3,
        random_strength=1,
        bagging_temperature=1,
        od_type='Iter',
        od_wait=50,
        task_type='GPU' if torch.cuda.is_available() else 'CPU',
        devices='0',
        random_state=42,
        verbose=100
    )
    
    # è¨“ç·´
    start_time = time.time()
    
    model.fit(
        X_train, y_train,
        eval_set=(X_test, y_test),
        early_stopping_rounds=50,
        plot=False
    )
    
    train_time = time.time() - start_time
    
    # é æ¸¬
    y_pred = model.predict(X_test)
    
    # è©•ä¼°
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    balanced_acc = balanced_accuracy_score(y_test, y_pred)
    
    print(f"\nè¨“ç·´æ™‚é–“: {train_time:.2f} ç§’")
    print(f"æº–ç¢ºç‡: {accuracy:.4f}")
    print(f"F1åˆ†æ•¸: {f1:.4f}")
    print(f"å¹³è¡¡æº–ç¢ºç‡: {balanced_acc:.4f}")
    
    return model, accuracy, f1, balanced_acc

# è¨“ç·´CatBoost
cat_model, cat_acc, cat_f1, cat_balanced_acc = train_catboost_optimized(
    X_train_balanced, X_test, y_train_balanced, y_test
)

# ===========================
# Cell 13: Balanced Random Forest
# ===========================

def train_balanced_rf(X_train, X_test, y_train, y_test):
    """è¨“ç·´Balanced Random Forest"""
    print("\nè¨“ç·´ Balanced Random Forest...")
    
    model = BalancedRandomForestClassifier(
        n_estimators=300,
        max_depth=20,
        min_samples_split=10,
        min_samples_leaf=5,
        max_features='sqrt',
        bootstrap=True,
        oob_score=True,
        class_weight='balanced_subsample',
        n_jobs=-1,
        random_state=42,
        verbose=1
    )
    
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time
    
    # é æ¸¬
    y_pred = model.predict(X_test)
    
    # è©•ä¼°
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    balanced_acc = balanced_accuracy_score(y_test, y_pred)
    
    print(f"\nè¨“ç·´æ™‚é–“: {train_time:.2f} ç§’")
    print(f"OOBåˆ†æ•¸: {model.oob_score_:.4f}")
    print(f"æº–ç¢ºç‡: {accuracy:.4f}")
    print(f"F1åˆ†æ•¸: {f1:.4f}")
    print(f"å¹³è¡¡æº–ç¢ºç‡: {balanced_acc:.4f}")
    
    # è©³ç´°å ±å‘Š
    print("\nåˆ†é¡å ±å‘Š:")
    print(classification_report(y_test, y_pred, 
                              target_names=[f'Severity {i+1}' for i in range(4)]))
    
    return model, accuracy, f1, balanced_acc

# è¨“ç·´Balanced RF
brf_model, brf_acc, brf_f1, brf_balanced_acc = train_balanced_rf(
    X_train, X_test, y_train, y_test  # ä½¿ç”¨åŸå§‹æ•¸æ“šï¼Œå› ç‚ºæ¨¡å‹å…§éƒ¨æœƒå¹³è¡¡
)

# ===========================
# æ”¹é€²çš„æ·±åº¦å­¸ç¿’æ¨¡å‹ï¼ˆæ›¿æ› Cell 14ï¼‰
# ===========================

class ImprovedNN(nn.Module):
    """æ”¹é€²çš„ç¥ç¶“ç¶²è·¯ - åŠ å…¥æ›´å¤šæŠ€å·§"""
    def __init__(self, input_size, num_classes=4):
        super(ImprovedNN, self).__init__()
        
        self.fc1 = nn.Linear(input_size, 512)
        self.bn1 = nn.BatchNorm1d(512)
        self.dropout1 = nn.Dropout(0.5)
        
        self.fc2 = nn.Linear(512, 256)
        self.bn2 = nn.BatchNorm1d(256)
        self.dropout2 = nn.Dropout(0.4)
        
        self.fc3 = nn.Linear(256, 128)
        self.bn3 = nn.BatchNorm1d(128)
        self.dropout3 = nn.Dropout(0.3)
        
        self.fc4 = nn.Linear(128, 64)
        self.bn4 = nn.BatchNorm1d(64)
        self.dropout4 = nn.Dropout(0.2)
        
        self.fc5 = nn.Linear(64, num_classes)
        
        # åˆå§‹åŒ–æ¬Šé‡
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
    
    def forward(self, x):
        x = F.relu(self.bn1(self.fc1(x)))
        x = self.dropout1(x)
        
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.dropout2(x)
        
        x = F.relu(self.bn3(self.fc3(x)))
        x = self.dropout3(x)
        
        x = F.relu(self.bn4(self.fc4(x)))
        x = self.dropout4(x)
        
        x = self.fc5(x)
        return x

def train_improved_nn(X_train, X_test, y_train, y_test, epochs=100):  # epochsåœ¨é€™è£¡
    """è¨“ç·´æ”¹é€²çš„æ·±åº¦å­¸ç¿’æ¨¡å‹"""
    print("\nè¨“ç·´æ”¹é€²çš„ç¥ç¶“ç¶²è·¯...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # æ¨™æº–åŒ–
    from sklearn.preprocessing import RobustScaler
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # è½‰æ›ç‚ºå¼µé‡
    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)
    y_train_tensor = torch.LongTensor(y_train).to(device)
    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)
    
    # DataLoader
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)
    
    # å‰µå»ºæ¨¡å‹ - ä¸å‚³å…¥epochs
    model = ImprovedNN(X_train.shape[1]).to(device)
    
    # æå¤±å‡½æ•¸
    class_weights_tensor = torch.FloatTensor(list(class_weight_dict.values())).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # è¨“ç·´å¾ªç’°
    start_time = time.time()
    best_balanced_acc = 0
    
    for epoch in range(epochs):  # epochsåœ¨é€™è£¡ä½¿ç”¨
        model.train()
        total_loss = 0
        
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        # æ¯10å€‹epochè©•ä¼°ä¸€æ¬¡
        if (epoch + 1) % 10 == 0:
            model.eval()
            with torch.no_grad():
                val_outputs = model(X_test_tensor)
                _, predicted = torch.max(val_outputs, 1)
                val_balanced_acc = balanced_accuracy_score(y_test, predicted.cpu().numpy())
            
            print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}, "
                  f"Balanced Acc: {val_balanced_acc:.4f}")
            
            if val_balanced_acc > best_balanced_acc:
                best_balanced_acc = val_balanced_acc
                best_model_state = model.state_dict()
    
    # è¼‰å…¥æœ€ä½³æ¨¡å‹
    if best_balanced_acc > 0:
        model.load_state_dict(best_model_state)
    
    # æœ€çµ‚è©•ä¼°
    model.eval()
    with torch.no_grad():
        outputs = model(X_test_tensor)
        _, predicted = torch.max(outputs, 1)
        y_pred = predicted.cpu().numpy()
    
    train_time = time.time() - start_time
    
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    balanced_acc = balanced_accuracy_score(y_test, y_pred)
    
    print(f"\nè¨“ç·´æ™‚é–“: {train_time:.2f} ç§’")
    print(f"æº–ç¢ºç‡: {accuracy:.4f}")
    print(f"F1åˆ†æ•¸: {f1:.4f}")
    print(f"å¹³è¡¡æº–ç¢ºç‡: {balanced_acc:.4f}")
    
    return model, scaler, accuracy, f1, balanced_acc

# åŸ·è¡Œè¨“ç·´
nn_model, nn_scaler, nn_acc, nn_f1, nn_balanced_acc = train_improved_nn(
    X_train_balanced, X_test, y_train_balanced, y_test, epochs=50
)

# ===========================
# Cell 15: æ¨¡å‹æ¯”è¼ƒå’Œé›†æˆ
# ===========================

# æ”¶é›†æ‰€æœ‰çµæœ
results = {
    'LightGBM': {'accuracy': lgb_acc, 'f1': lgb_f1, 'balanced_acc': lgb_balanced_acc},
    'XGBoost': {'accuracy': xgb_acc, 'f1': xgb_f1, 'balanced_acc': xgb_balanced_acc},
    'CatBoost': {'accuracy': cat_acc, 'f1': cat_f1, 'balanced_acc': cat_balanced_acc},
    'Balanced_RF': {'accuracy': brf_acc, 'f1': brf_f1, 'balanced_acc': brf_balanced_acc},
    'Neural_Network': {'accuracy': nn_acc, 'f1': nn_f1, 'balanced_acc': nn_balanced_acc}
}

print("\n" + "="*70)
print("æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ")
print("="*70)
print(f"{'æ¨¡å‹':<20} {'æº–ç¢ºç‡':<10} {'F1åˆ†æ•¸':<10} {'å¹³è¡¡æº–ç¢ºç‡':<10}")
print("-"*70)

for model_name, metrics in sorted(results.items(), key=lambda x: x[1]['balanced_acc'], reverse=True):
    print(f"{model_name:<20} {metrics['accuracy']:<10.4f} {metrics['f1']:<10.4f} {metrics['balanced_acc']:<10.4f}")

# æ‰¾å‡ºæœ€ä½³æ¨¡å‹
best_model_name = max(results.items(), key=lambda x: x[1]['balanced_acc'])[0]
print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"   å¹³è¡¡æº–ç¢ºç‡: {results[best_model_name]['balanced_acc']:.4f}")

# ===========================
# Cell 16: ä¿å­˜æ¨¡å‹å’Œçµæœï¼ˆä¿®æ­£ç‰ˆï¼‰
# ===========================

def save_models_and_results(models, results, feature_names, label_encoders):
    """ä¿å­˜æ‰€æœ‰æ¨¡å‹å’Œçµæœ"""
    output_dir = './model_output/'
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹
    model_dict = {
        'LightGBM': lgb_model,
        'XGBoost': xgb_model,
        'CatBoost': cat_model,
        'Balanced_RF': brf_model
    }
    
    for name, model in model_dict.items():
        if name == 'LightGBM':
            model.save_model(f'{output_dir}{name.lower()}_model.txt')
        else:
            joblib.dump(model, f'{output_dir}{name.lower()}_model.pkl')
    
    # ä¿å­˜ç¥ç¶“ç¶²è·¯
    torch.save(nn_model.state_dict(), f'{output_dir}neural_network_model.pth')
    joblib.dump(nn_scaler, f'{output_dir}nn_scaler.pkl')
    
    # ä¿å­˜ç‰¹å¾µåç¨±å’Œç·¨ç¢¼å™¨
    joblib.dump(feature_names, f'{output_dir}feature_names.pkl')
    joblib.dump(label_encoders, f'{output_dir}label_encoders.pkl')
    
    # ä¿å­˜çµæœ
    import json
    with open(f'{output_dir}results.json', 'w') as f:
        json.dump(results, f, indent=4)
    
    # ä¿å­˜è¨“ç·´ä¿¡æ¯ - ä¿®æ­£ï¼šå°‡numpyé¡å‹è½‰æ›ç‚ºPythonåŸç”Ÿé¡å‹
    train_info = {
        'train_size': int(len(X_train)),  # è½‰æ›ç‚ºint
        'test_size': int(len(X_test)),    # è½‰æ›ç‚ºint
        'n_features': int(len(feature_names)),  # è½‰æ›ç‚ºint
        'class_distribution': {int(k): int(v) for k, v in zip(*np.unique(y, return_counts=True))},  # è½‰æ›éµå€¼
        'best_model': best_model_name,
        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open(f'{output_dir}train_info.json', 'w') as f:
        json.dump(train_info, f, indent=4)
    
    print(f"\nâœ… æ‰€æœ‰æ¨¡å‹å’Œçµæœå·²ä¿å­˜è‡³: {output_dir}")

# ä¿å­˜
save_models_and_results(
    {'nn_model': nn_model, 'nn_scaler': nn_scaler},
    results,
    feature_names,
    label_encoders
)