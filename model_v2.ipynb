{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfdf69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 1: å°å…¥å¥—ä»¶å’Œè¨­å®š\n",
    "# ===========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import gc  # åƒåœ¾å›æ”¶\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# åŸºæœ¬å¥—ä»¶\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, \n",
    "    confusion_matrix, balanced_accuracy_score, \n",
    "    cohen_kappa_score, make_scorer\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# è™•ç†ä¸å¹³è¡¡è³‡æ–™\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# Boostingæ¨¡å‹\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import torch.nn.functional as F \n",
    "\n",
    "print(\"ç’°å¢ƒæª¢æŸ¥:\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"XGBoost: {xgb.__version__}\")\n",
    "print(f\"LightGBM: {lgb.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30044c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 2: è¨˜æ†¶é«”å„ªåŒ–å‡½æ•¸\n",
    "# ===========================\n",
    "\n",
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    é€šéæ”¹è®Šæ•¸æ“šé¡å‹ä¾†æ¸›å°‘DataFrameçš„è¨˜æ†¶é«”ä½¿ç”¨\n",
    "    åƒè€ƒè‡ªKaggleçš„è¨˜æ†¶é«”å„ªåŒ–æŠ€è¡“\n",
    "    \"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f'è¨˜æ†¶é«”ä½¿ç”¨æ¸›å°‘äº† {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "        print(f'{start_mem:.2f} MB --> {end_mem:.2f} MB')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_memory():\n",
    "    \"\"\"æ¸…ç†è¨˜æ†¶é«”\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 3: è¼‰å…¥è³‡æ–™ï¼ˆå„ªåŒ–ç‰ˆï¼‰\n",
    "# ===========================\n",
    "\n",
    "def load_data_optimized(file_path, sample_frac=None, chunksize=None):\n",
    "    \"\"\"\n",
    "    å„ªåŒ–çš„è³‡æ–™è¼‰å…¥ï¼Œæ”¯æ´æ¡æ¨£å’Œåˆ†å¡Šè®€å–\n",
    "    \"\"\"\n",
    "    print(f\"è¼‰å…¥è³‡æ–™: {file_path}\")\n",
    "    \n",
    "    # å…ˆè®€å–ä¸€å°éƒ¨åˆ†ä¾†äº†è§£è³‡æ–™\n",
    "    sample_df = pd.read_csv(file_path, nrows=5)\n",
    "    print(\"è³‡æ–™æ¬„ä½é è¦½:\")\n",
    "    print(sample_df.columns.tolist())\n",
    "    \n",
    "    # å®šç¾©éœ€è¦çš„æ¬„ä½ï¼ˆæ’é™¤ä¸éœ€è¦çš„æ–‡å­—æ¬„ä½ä»¥ç¯€çœè¨˜æ†¶é«”ï¼‰\n",
    "    # æ ¹æ“šå…¶ä»–Kaggle notebookçš„ç¶“é©—ï¼Œé€™äº›æ˜¯æœ€é‡è¦çš„æ¬„ä½\n",
    "    important_cols = [\n",
    "        'Severity', 'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng',\n",
    "        'Distance(mi)', 'Temperature(F)', 'Humidity(%)', 'Pressure(in)',\n",
    "        'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)',\n",
    "        'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way',\n",
    "        'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop',\n",
    "        'Traffic_Calming', 'Traffic_Signal', 'Sunrise_Sunset', 'State',\n",
    "        'Side', 'Weather_Timestamp'\n",
    "    ]\n",
    "    \n",
    "    # éæ¿¾å­˜åœ¨çš„æ¬„ä½\n",
    "    existing_cols = [col for col in important_cols if col in sample_df.columns]\n",
    "    \n",
    "    # å®šç¾©æ•¸æ“šé¡å‹ä»¥æ¸›å°‘è¨˜æ†¶é«”\n",
    "    dtype_dict = {\n",
    "        'Severity': 'int8',\n",
    "        'Distance(mi)': 'float32',\n",
    "        'Temperature(F)': 'float32',\n",
    "        'Humidity(%)': 'float32',\n",
    "        'Pressure(in)': 'float32',\n",
    "        'Visibility(mi)': 'float32',\n",
    "        'Wind_Speed(mph)': 'float32',\n",
    "        'Precipitation(in)': 'float32',\n",
    "        'Amenity': 'bool',\n",
    "        'Bump': 'bool',\n",
    "        'Crossing': 'bool',\n",
    "        'Give_Way': 'bool',\n",
    "        'Junction': 'bool',\n",
    "        'No_Exit': 'bool',\n",
    "        'Railway': 'bool',\n",
    "        'Roundabout': 'bool',\n",
    "        'Station': 'bool',\n",
    "        'Stop': 'bool',\n",
    "        'Traffic_Calming': 'bool',\n",
    "        'Traffic_Signal': 'bool'\n",
    "    }\n",
    "    \n",
    "    # è¼‰å…¥è³‡æ–™\n",
    "    if sample_frac:\n",
    "        # éš¨æ©Ÿæ¡æ¨£\n",
    "        print(f\"è¼‰å…¥ {sample_frac*100}% çš„è³‡æ–™...\")\n",
    "        df = pd.read_csv(file_path, usecols=existing_cols, dtype=dtype_dict)\n",
    "        df = df.sample(frac=sample_frac, random_state=42)\n",
    "    elif chunksize:\n",
    "        # åˆ†å¡Šè¼‰å…¥\n",
    "        print(f\"åˆ†å¡Šè¼‰å…¥ï¼Œæ¯å¡Š {chunksize} è¡Œ...\")\n",
    "        chunks = []\n",
    "        for chunk in pd.read_csv(file_path, usecols=existing_cols, \n",
    "                                dtype=dtype_dict, chunksize=chunksize):\n",
    "            chunks.append(chunk)\n",
    "            if len(chunks) * chunksize >= 1000000:  # é™åˆ¶åœ¨100è¬è¡Œ\n",
    "                break\n",
    "        df = pd.concat(chunks, ignore_index=True)\n",
    "    else:\n",
    "        # å®Œæ•´è¼‰å…¥\n",
    "        df = pd.read_csv(file_path, usecols=existing_cols, dtype=dtype_dict)\n",
    "    \n",
    "    print(f\"è¼‰å…¥è³‡æ–™å¤§å°: {df.shape}\")\n",
    "    print(f\"è¨˜æ†¶é«”ä½¿ç”¨: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # é¡¯ç¤ºç›®æ¨™è®Šæ•¸åˆ†å¸ƒ\n",
    "    print(\"\\nç›®æ¨™è®Šæ•¸åˆ†å¸ƒ:\")\n",
    "    severity_counts = df['Severity'].value_counts().sort_index()\n",
    "    for sev, count in severity_counts.items():\n",
    "        print(f\"Severity {sev}: {count:,} ({count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# åŸ·è¡Œè¼‰å…¥ï¼ˆå»ºè­°å…ˆç”¨å°æ¨£æœ¬æ¸¬è©¦ï¼‰\n",
    "file_path = 'us-accidents/US_Accidents_March23.csv'\n",
    "\n",
    "# é¸é …1: ä½¿ç”¨éƒ¨åˆ†è³‡æ–™ï¼ˆæ¨è–¦ç”¨æ–¼æ¸¬è©¦ï¼‰\n",
    "# df = load_data_optimized(file_path, sample_frac=0.1)  # 10%è³‡æ–™\n",
    "\n",
    "# é¸é …2: åˆ†å¡Šè¼‰å…¥\n",
    "# df = load_data_optimized(file_path, chunksize=500000)  # æ¯æ¬¡50è¬è¡Œ\n",
    "\n",
    "# é¸é …3: å®Œæ•´è¼‰å…¥ï¼ˆéœ€è¦å¤§é‡è¨˜æ†¶é«”ï¼‰\n",
    "df = load_data_optimized(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dce068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 4: æ—¥æœŸæ™‚é–“è™•ç†ï¼ˆå„ªåŒ–ç‰ˆï¼‰\n",
    "# ===========================\n",
    "\n",
    "def process_datetime_features(df):\n",
    "    \"\"\"è™•ç†æ—¥æœŸæ™‚é–“ç‰¹å¾µ\"\"\"\n",
    "    print(\"\\nè™•ç†æ—¥æœŸæ™‚é–“ç‰¹å¾µ...\")\n",
    "    \n",
    "    # è½‰æ›æ—¥æœŸæ™‚é–“\n",
    "    df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\n",
    "    df['End_Time'] = pd.to_datetime(df['End_Time'], errors='coerce')\n",
    "    \n",
    "    # è¨ˆç®—æŒçºŒæ™‚é–“\n",
    "    df['Duration_minutes'] = (df['End_Time'] - df['Start_Time']).dt.total_seconds() / 60\n",
    "    \n",
    "    # éæ¿¾ç•°å¸¸å€¼ï¼ˆä½¿ç”¨æ›´å¯¬é¬†çš„ç¯„åœï¼‰\n",
    "    df = df[(df['Duration_minutes'] > 0) & (df['Duration_minutes'] < 1440*7)]  # å°æ–¼7å¤©\n",
    "    \n",
    "    # ç§»é™¤æ—¥æœŸæ™‚é–“ç‚ºç©ºçš„è¨˜éŒ„\n",
    "    df = df.dropna(subset=['Start_Time'])\n",
    "    \n",
    "    # æå–æ™‚é–“ç‰¹å¾µ\n",
    "    df['Hour'] = df['Start_Time'].dt.hour.astype('int8')\n",
    "    df['DayOfWeek'] = df['Start_Time'].dt.dayofweek.astype('int8')\n",
    "    df['Month'] = df['Start_Time'].dt.month.astype('int8')\n",
    "    df['Year'] = df['Start_Time'].dt.year.astype('int16')\n",
    "    \n",
    "    # è¡ç”Ÿç‰¹å¾µ\n",
    "    df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype('int8')\n",
    "    df['IsRushHour'] = df['Hour'].apply(\n",
    "        lambda x: 1 if (6 <= x <= 9) or (16 <= x <= 19) else 0\n",
    "    ).astype('int8')\n",
    "    \n",
    "    # æ™‚æ®µåˆ†é¡\n",
    "    df['TimeOfDay'] = pd.cut(df['Hour'], \n",
    "                            bins=[-1, 6, 12, 18, 24], \n",
    "                            labels=[0, 1, 2, 3]).astype('int8')  # è½‰æ›ç‚ºæ•¸å€¼\n",
    "    \n",
    "    # å­£ç¯€\n",
    "    df['Season'] = pd.cut(df['Month'], \n",
    "                         bins=[0, 3, 6, 9, 12], \n",
    "                         labels=[0, 1, 2, 3]).astype('int8')  # è½‰æ›ç‚ºæ•¸å€¼\n",
    "    \n",
    "    # åˆªé™¤åŸå§‹æ™‚é–“æ¬„ä½ä»¥ç¯€çœè¨˜æ†¶é«”\n",
    "    df = df.drop(['Start_Time', 'End_Time', 'Weather_Timestamp'], axis=1, errors='ignore')\n",
    "    \n",
    "    print(f\"è™•ç†å¾Œå¤§å°: {df.shape}\")\n",
    "    clean_memory()\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = process_datetime_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77396eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 5: å¤©æ°£ç‰¹å¾µè™•ç†\n",
    "# ===========================\n",
    "\n",
    "def process_weather_features(df):\n",
    "    \"\"\"è™•ç†å¤©æ°£ç›¸é—œç‰¹å¾µ\"\"\"\n",
    "    print(\"\\nè™•ç†å¤©æ°£ç‰¹å¾µ...\")\n",
    "    \n",
    "    if 'Weather_Condition' in df.columns:\n",
    "        # ç°¡åŒ–å¤©æ°£åˆ†é¡\n",
    "        def categorize_weather(condition):\n",
    "            if pd.isna(condition):\n",
    "                return 0  # Unknown\n",
    "            condition = str(condition).lower()\n",
    "            if any(word in condition for word in ['clear', 'fair']):\n",
    "                return 1  # Clear\n",
    "            elif any(word in condition for word in ['cloud', 'overcast']):\n",
    "                return 2  # Cloudy\n",
    "            elif any(word in condition for word in ['rain', 'drizzle']):\n",
    "                return 3  # Rain\n",
    "            elif any(word in condition for word in ['snow', 'sleet']):\n",
    "                return 4  # Snow\n",
    "            elif any(word in condition for word in ['fog', 'mist']):\n",
    "                return 5  # Fog\n",
    "            elif any(word in condition for word in ['storm', 'thunder']):\n",
    "                return 6  # Storm\n",
    "            else:\n",
    "                return 7  # Other\n",
    "        \n",
    "        df['Weather_Category'] = df['Weather_Condition'].apply(categorize_weather).astype('int8')\n",
    "        df = df.drop('Weather_Condition', axis=1)\n",
    "    \n",
    "    # è™•ç†å…¶ä»–å¤©æ°£æ•¸å€¼ç‰¹å¾µçš„ç¼ºå¤±å€¼\n",
    "    weather_numeric_cols = ['Temperature(F)', 'Humidity(%)', 'Pressure(in)', \n",
    "                           'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']\n",
    "    \n",
    "    for col in weather_numeric_cols:\n",
    "        if col in df.columns:\n",
    "            # ä½¿ç”¨ä¸­ä½æ•¸å¡«å……\n",
    "            median_val = df[col].median()\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "    \n",
    "    clean_memory()\n",
    "    return df\n",
    "\n",
    "df = process_weather_features(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 6: è™•ç†ç¼ºå¤±å€¼å’Œç·¨ç¢¼é¡åˆ¥è®Šæ•¸\n",
    "# ===========================\n",
    "\n",
    "def handle_missing_and_encode(df):\n",
    "    \"\"\"è™•ç†ç¼ºå¤±å€¼ä¸¦ç·¨ç¢¼é¡åˆ¥è®Šæ•¸\"\"\"\n",
    "    print(\"\\nè™•ç†ç¼ºå¤±å€¼å’Œç·¨ç¢¼...\")\n",
    "    \n",
    "    # åˆªé™¤ç¼ºå¤±å€¼éå¤šçš„æ¬„ä½\n",
    "    missing_pct = df.isnull().sum() / len(df)\n",
    "    high_missing_cols = missing_pct[missing_pct > 0.5].index.tolist()\n",
    "    \n",
    "    # ä¿ç•™Severity\n",
    "    if 'Severity' in high_missing_cols:\n",
    "        high_missing_cols.remove('Severity')\n",
    "    \n",
    "    df = df.drop(columns=high_missing_cols, errors='ignore')\n",
    "    print(f\"åˆªé™¤é«˜ç¼ºå¤±ç‡æ¬„ä½: {len(high_missing_cols)}\")\n",
    "    \n",
    "    # å°é¡åˆ¥è®Šæ•¸é€²è¡Œæ¨™ç±¤ç·¨ç¢¼\n",
    "    categorical_cols = ['State', 'Side', 'Sunrise_Sunset']\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            # å¡«å……ç¼ºå¤±å€¼\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "            # ç·¨ç¢¼\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # å¡«å……æ•¸å€¼å‹ç¼ºå¤±å€¼\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col != 'Severity':\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # ç¢ºä¿å¸ƒæ—å‹æ¬„ä½æ˜¯æ•´æ•¸\n",
    "    bool_cols = df.select_dtypes(include=['bool']).columns\n",
    "    for col in bool_cols:\n",
    "        df[col] = df[col].astype('int8')\n",
    "    \n",
    "    print(f\"è™•ç†å¾Œè³‡æ–™å¤§å°: {df.shape}\")\n",
    "    print(f\"å‰©é¤˜ç¼ºå¤±å€¼: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    clean_memory()\n",
    "    return df, label_encoders\n",
    "\n",
    "df, label_encoders = handle_missing_and_encode(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 7: ç‰¹å¾µé¸æ“‡å’Œæº–å‚™æœ€çµ‚æ•¸æ“š\n",
    "# ===========================\n",
    "\n",
    "def prepare_final_data(df):\n",
    "    \"\"\"æº–å‚™æœ€çµ‚çš„è¨“ç·´æ•¸æ“š\"\"\"\n",
    "    print(\"\\næº–å‚™æœ€çµ‚æ•¸æ“š...\")\n",
    "    \n",
    "    # åˆªé™¤ä»»ä½•ä»æœ‰ç¼ºå¤±å€¼çš„è¡Œ\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # ç¢ºä¿Severityæ˜¯æ­£ç¢ºçš„å€¼\n",
    "    df = df[df['Severity'].isin([1, 2, 3, 4])]\n",
    "    \n",
    "    # æ ¹æ“šKaggleä¸Šçš„å»ºè­°ï¼Œè€ƒæ…®åˆä½µSeverity 1å’Œ2\n",
    "    # å› ç‚ºSeverity 1çš„æ¨£æœ¬å¤ªå°‘\n",
    "    print(\"\\nåŸå§‹é¡åˆ¥åˆ†å¸ƒ:\")\n",
    "    print(df['Severity'].value_counts().sort_index())\n",
    "    \n",
    "    # é¸é …ï¼šåˆä½µé¡åˆ¥ï¼ˆå¯é¸ï¼‰\n",
    "    # df['Severity'] = df['Severity'].replace({1: 2})\n",
    "    \n",
    "    # åˆ†é›¢ç‰¹å¾µå’Œç›®æ¨™\n",
    "    feature_cols = [col for col in df.columns if col != 'Severity']\n",
    "    X = df[feature_cols].values\n",
    "    y = df['Severity'].values - 1  # è½‰æ›ç‚º0-3\n",
    "    \n",
    "    print(f\"\\næœ€çµ‚æ•¸æ“šå¤§å°: X={X.shape}, y={y.shape}\")\n",
    "    print(\"æœ€çµ‚é¡åˆ¥åˆ†å¸ƒ:\")\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for cls, cnt in zip(unique, counts):\n",
    "        print(f\"  é¡åˆ¥ {cls} (Severity {cls+1}): {cnt:,} ({cnt/len(y)*100:.2f}%)\")\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "X, y, feature_names = prepare_final_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ecf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 8: è³‡æ–™åˆ†å‰²\n",
    "# ===========================\n",
    "\n",
    "# åˆ†å±¤åˆ†å‰²\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"è¨“ç·´é›†: {X_train.shape}\")\n",
    "print(f\"æ¸¬è©¦é›†: {X_test.shape}\")\n",
    "\n",
    "# è¨ˆç®—é¡åˆ¥æ¬Šé‡\n",
    "class_weights = compute_class_weight('balanced', \n",
    "                                   classes=np.unique(y_train), \n",
    "                                   y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"\\né¡åˆ¥æ¬Šé‡:\")\n",
    "for cls, weight in class_weight_dict.items():\n",
    "    print(f\"  é¡åˆ¥ {cls}: {weight:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d040e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 9: è™•ç†ä¸å¹³è¡¡ - æ··åˆæ¡æ¨£ç­–ç•¥\n",
    "# ===========================\n",
    "\n",
    "def apply_mixed_sampling(X_train, y_train, strategy='mixed'):\n",
    "    \"\"\"\n",
    "    æ‡‰ç”¨æ··åˆæ¡æ¨£ç­–ç•¥\n",
    "    åƒè€ƒKaggleæœ€ä½³å¯¦è¸ï¼šçµåˆéæ¡æ¨£å’Œæ¬ æ¡æ¨£\n",
    "    \"\"\"\n",
    "    print(f\"\\næ‡‰ç”¨æ¡æ¨£ç­–ç•¥: {strategy}\")\n",
    "    \n",
    "    if strategy == 'none':\n",
    "        return X_train, y_train\n",
    "    \n",
    "    # è¨ˆç®—å„é¡åˆ¥æ•¸é‡\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    class_counts = dict(zip(unique, counts))\n",
    "    print(\"åŸå§‹åˆ†å¸ƒ:\", class_counts)\n",
    "    \n",
    "    if strategy == 'mixed':\n",
    "        # æ··åˆç­–ç•¥ï¼šå°å¤šæ•¸é¡æ¬ æ¡æ¨£ï¼Œå°å°‘æ•¸é¡éæ¡æ¨£\n",
    "        # ç›®æ¨™ï¼šè®“æ‰€æœ‰é¡åˆ¥æ¥è¿‘ä¸­ä½æ•¸\n",
    "        median_count = int(np.median(counts))\n",
    "        target_count = int(median_count * 1.5)  # ç›®æ¨™æ•¸é‡è¨­ç‚ºä¸­ä½æ•¸çš„1.5å€\n",
    "        \n",
    "        # ç¬¬ä¸€æ­¥ï¼šæ¬ æ¡æ¨£ - åªå°è¶…éç›®æ¨™æ•¸é‡çš„é¡åˆ¥é€²è¡Œæ¬ æ¡æ¨£\n",
    "        undersample_strategy = {}\n",
    "        for cls, cnt in class_counts.items():\n",
    "            if cnt > target_count:\n",
    "                undersample_strategy[cls] = target_count\n",
    "            else:\n",
    "                undersample_strategy[cls] = cnt  # ä¿æŒåŸæ¨£\n",
    "        \n",
    "        if len(undersample_strategy) > 0 and any(cnt < class_counts[cls] for cls, cnt in undersample_strategy.items()):\n",
    "            rus = RandomUnderSampler(sampling_strategy=undersample_strategy, random_state=42)\n",
    "            X_temp, y_temp = rus.fit_resample(X_train, y_train)\n",
    "        else:\n",
    "            X_temp, y_temp = X_train, y_train\n",
    "        \n",
    "        # ç¬¬äºŒæ­¥ï¼šéæ¡æ¨£ - åªå°å°‘æ–¼ç›®æ¨™æ•¸é‡çš„é¡åˆ¥é€²è¡Œéæ¡æ¨£\n",
    "        temp_unique, temp_counts = np.unique(y_temp, return_counts=True)\n",
    "        temp_class_counts = dict(zip(temp_unique, temp_counts))\n",
    "        \n",
    "        oversample_strategy = {}\n",
    "        for cls, cnt in temp_class_counts.items():\n",
    "            if cnt < target_count:\n",
    "                oversample_strategy[cls] = target_count\n",
    "            else:\n",
    "                oversample_strategy[cls] = cnt  # ä¿æŒåŸæ¨£\n",
    "        \n",
    "        if len(oversample_strategy) > 0 and any(cnt > temp_class_counts[cls] for cls, cnt in oversample_strategy.items()):\n",
    "            ros = RandomOverSampler(sampling_strategy=oversample_strategy, random_state=42)\n",
    "            X_resampled, y_resampled = ros.fit_resample(X_temp, y_temp)\n",
    "        else:\n",
    "            X_resampled, y_resampled = X_temp, y_temp\n",
    "            \n",
    "    elif strategy == 'smote':\n",
    "        # SMOTEç­–ç•¥ï¼šåªéæ¡æ¨£åˆ°æœ€å¤šé¡åˆ¥çš„50%\n",
    "        max_count = max(counts)\n",
    "        target_count = int(max_count * 0.5)\n",
    "        \n",
    "        # ç¢ºä¿ç›®æ¨™æ•¸é‡ä¸å°æ–¼ç•¶å‰æ•¸é‡\n",
    "        sampling_strategy = {}\n",
    "        for cls, cnt in class_counts.items():\n",
    "            if cnt < target_count:\n",
    "                sampling_strategy[cls] = target_count\n",
    "            else:\n",
    "                sampling_strategy[cls] = cnt\n",
    "        \n",
    "        smote = SMOTE(sampling_strategy=sampling_strategy, k_neighbors=5, random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    elif strategy == 'undersample_only':\n",
    "        # åªæ¬ æ¡æ¨£åˆ°æœ€å°‘é¡åˆ¥çš„2å€\n",
    "        min_count = min(counts)\n",
    "        target_count = min_count * 2\n",
    "        \n",
    "        sampling_strategy = {}\n",
    "        for cls, cnt in class_counts.items():\n",
    "            sampling_strategy[cls] = min(cnt, target_count)\n",
    "        \n",
    "        rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "        X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # é¡¯ç¤ºæ–°åˆ†å¸ƒ\n",
    "    unique_new, counts_new = np.unique(y_resampled, return_counts=True)\n",
    "    new_distribution = dict(zip(unique_new, counts_new))\n",
    "    print(\"æ¡æ¨£å¾Œåˆ†å¸ƒ:\", new_distribution)\n",
    "    \n",
    "    # é¡¯ç¤ºè®ŠåŒ–\n",
    "    print(\"\\næ¡æ¨£è®ŠåŒ–:\")\n",
    "    for cls in range(4):\n",
    "        original = class_counts.get(cls, 0)\n",
    "        new = new_distribution.get(cls, 0)\n",
    "        change = ((new - original) / original * 100) if original > 0 else 0\n",
    "        print(f\"  é¡åˆ¥ {cls}: {original:,} â†’ {new:,} ({change:+.1f}%)\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# æ‡‰ç”¨æ··åˆæ¡æ¨£\n",
    "# å¯ä»¥å˜—è©¦ä¸åŒç­–ç•¥\n",
    "X_train_balanced, y_train_balanced = apply_mixed_sampling(X_train, y_train, 'mixed')\n",
    "\n",
    "# å¦‚æœæ··åˆç­–ç•¥é‚„æ˜¯æœ‰å•é¡Œï¼Œå¯ä»¥å˜—è©¦å…¶ä»–ç­–ç•¥ï¼š\n",
    "# X_train_balanced, y_train_balanced = apply_mixed_sampling(X_train, y_train, 'undersample_only')\n",
    "# æˆ–è€…ä¸é€²è¡Œæ¡æ¨£ï¼š\n",
    "# X_train_balanced, y_train_balanced = X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 10: LightGBMæ¨¡å‹ï¼ˆå„ªåŒ–ç‰ˆï¼‰\n",
    "# ===========================\n",
    "\n",
    "def train_lightgbm_optimized(X_train, X_test, y_train, y_test, class_weights):\n",
    "    \"\"\"è¨“ç·´å„ªåŒ–çš„LightGBMæ¨¡å‹\"\"\"\n",
    "    print(\"\\nè¨“ç·´ LightGBM (å„ªåŒ–ç‰ˆ)...\")\n",
    "    \n",
    "    # å‰µå»ºæ•¸æ“šé›†\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "    \n",
    "    # åƒæ•¸è¨­ç½®ï¼ˆåŸºæ–¼Kaggleæœ€ä½³å¯¦è¸ï¼‰\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 4,\n",
    "        'metric': 'multi_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 127,\n",
    "        'max_depth': -1,\n",
    "        'learning_rate': 0.05,\n",
    "        'n_estimators': 1000,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'min_child_samples': 20,\n",
    "        'min_split_gain': 0.02,\n",
    "        'class_weight': 'balanced',\n",
    "        'device': 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # è¨“ç·´\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[valid_data],\n",
    "        num_boost_round=1000,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # é æ¸¬\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # è©•ä¼°\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nè¨“ç·´æ™‚é–“: {train_time:.2f} ç§’\")\n",
    "    print(f\"æœ€ä½³è¿­ä»£æ¬¡æ•¸: {model.best_iteration}\")\n",
    "    print(f\"æº–ç¢ºç‡: {accuracy:.4f}\")\n",
    "    print(f\"F1åˆ†æ•¸: {f1:.4f}\")\n",
    "    print(f\"å¹³è¡¡æº–ç¢ºç‡: {balanced_acc:.4f}\")\n",
    "    \n",
    "    # è©³ç´°å ±å‘Š\n",
    "    print(\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                              target_names=[f'Severity {i+1}' for i in range(4)]))\n",
    "    \n",
    "    return model, accuracy, f1, balanced_acc\n",
    "\n",
    "# è¨“ç·´æ¨¡å‹\n",
    "lgb_model, lgb_acc, lgb_f1, lgb_balanced_acc = train_lightgbm_optimized(\n",
    "    X_train_balanced, X_test, y_train_balanced, y_test, class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b648315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 11: XGBoostæ¨¡å‹ï¼ˆç©©å®šç‰ˆï¼‰\n",
    "# ===========================\n",
    "\n",
    "def train_xgboost_stable(X_train, X_test, y_train, y_test, use_sample_weight=True):\n",
    "    \"\"\"ç©©å®šç‰ˆXGBoost\"\"\"\n",
    "    print(\"\\nè¨“ç·´ XGBoost (ç©©å®šç‰ˆ)...\")\n",
    "    \n",
    "    # ä½¿ç”¨åŸå§‹çš„é¡åˆ¥æ¬Šé‡ï¼Œä½†ä¸è¦å¤ªæ¥µç«¯\n",
    "    if use_sample_weight:\n",
    "        # æº«å’Œçš„é¡åˆ¥æ¬Šé‡\n",
    "        unique, counts = np.unique(y_train, return_counts=True)\n",
    "        weight_dict = {}\n",
    "        max_count = max(counts)\n",
    "        for cls, count in zip(unique, counts):\n",
    "            # æ¬Šé‡ä¸è¶…é10å€\n",
    "            weight_dict[cls] = min(max_count / count, 10.0)\n",
    "        \n",
    "        sample_weights = np.array([weight_dict[y] for y in y_train])\n",
    "    else:\n",
    "        sample_weights = None\n",
    "    \n",
    "    # XGBooståƒæ•¸\n",
    "    params = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 4,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 300,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 5,  # å¢åŠ ä»¥é˜²æ­¢éæ“¬åˆ\n",
    "        'gamma': 0.1,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 1,\n",
    "        'tree_method': 'gpu_hist' if torch.cuda.is_available() else 'hist',\n",
    "        'random_state': 42,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'mlogloss'\n",
    "    }\n",
    "    \n",
    "    # è¨“ç·´\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        sample_weight=sample_weights,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        # early_stopping_rounds=50,\n",
    "        verbose=100\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # é æ¸¬\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # è©•ä¼°\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nè¨“ç·´æ™‚é–“: {train_time:.2f} ç§’\")\n",
    "    print(f\"æº–ç¢ºç‡: {accuracy:.4f}\")\n",
    "    print(f\"F1åˆ†æ•¸: {f1:.4f}\")\n",
    "    print(f\"å¹³è¡¡æº–ç¢ºç‡: {balanced_acc:.4f}\")\n",
    "    \n",
    "    print(\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                              target_names=[f'Severity {i+1}' for i in range(4)]))\n",
    "    \n",
    "    return model, accuracy, f1, balanced_acc\n",
    "\n",
    "# åŸ·è¡Œè¨“ç·´\n",
    "xgb_model, xgb_acc, xgb_f1, xgb_balanced_acc = train_xgboost_stable(\n",
    "    X_train_balanced, X_test, y_train_balanced, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 12: CatBoostæ¨¡å‹\n",
    "# ===========================\n",
    "\n",
    "def train_catboost_optimized(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"è¨“ç·´å„ªåŒ–çš„CatBoostæ¨¡å‹\"\"\"\n",
    "    print(\"\\nè¨“ç·´ CatBoost (å„ªåŒ–ç‰ˆ)...\")\n",
    "    \n",
    "    # CatBooståƒæ•¸\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=1000,\n",
    "        depth=8,\n",
    "        learning_rate=0.05,\n",
    "        loss_function='MultiClass',\n",
    "        eval_metric='TotalF1',\n",
    "        auto_class_weights='Balanced',\n",
    "        l2_leaf_reg=3,\n",
    "        random_strength=1,\n",
    "        bagging_temperature=1,\n",
    "        od_type='Iter',\n",
    "        od_wait=50,\n",
    "        task_type='GPU' if torch.cuda.is_available() else 'CPU',\n",
    "        devices='0',\n",
    "        random_state=42,\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    # è¨“ç·´\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_test, y_test),\n",
    "        early_stopping_rounds=50,\n",
    "        plot=False\n",
    "    )\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # é æ¸¬\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # è©•ä¼°\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nè¨“ç·´æ™‚é–“: {train_time:.2f} ç§’\")\n",
    "    print(f\"æº–ç¢ºç‡: {accuracy:.4f}\")\n",
    "    print(f\"F1åˆ†æ•¸: {f1:.4f}\")\n",
    "    print(f\"å¹³è¡¡æº–ç¢ºç‡: {balanced_acc:.4f}\")\n",
    "    \n",
    "    return model, accuracy, f1, balanced_acc\n",
    "\n",
    "# è¨“ç·´CatBoost\n",
    "cat_model, cat_acc, cat_f1, cat_balanced_acc = train_catboost_optimized(\n",
    "    X_train_balanced, X_test, y_train_balanced, y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed3f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 13: Balanced Random Forest\n",
    "# ===========================\n",
    "\n",
    "def train_balanced_rf(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"è¨“ç·´Balanced Random Forest\"\"\"\n",
    "    print(\"\\nè¨“ç·´ Balanced Random Forest...\")\n",
    "    \n",
    "    model = BalancedRandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=20,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        class_weight='balanced_subsample',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # é æ¸¬\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # è©•ä¼°\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nè¨“ç·´æ™‚é–“: {train_time:.2f} ç§’\")\n",
    "    print(f\"OOBåˆ†æ•¸: {model.oob_score_:.4f}\")\n",
    "    print(f\"æº–ç¢ºç‡: {accuracy:.4f}\")\n",
    "    print(f\"F1åˆ†æ•¸: {f1:.4f}\")\n",
    "    print(f\"å¹³è¡¡æº–ç¢ºç‡: {balanced_acc:.4f}\")\n",
    "    \n",
    "    # è©³ç´°å ±å‘Š\n",
    "    print(\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                              target_names=[f'Severity {i+1}' for i in range(4)]))\n",
    "    \n",
    "    return model, accuracy, f1, balanced_acc\n",
    "\n",
    "# è¨“ç·´Balanced RF\n",
    "brf_model, brf_acc, brf_f1, brf_balanced_acc = train_balanced_rf(\n",
    "    X_train, X_test, y_train, y_test  # ä½¿ç”¨åŸå§‹æ•¸æ“šï¼Œå› ç‚ºæ¨¡å‹å…§éƒ¨æœƒå¹³è¡¡\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0384288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# æ”¹é€²çš„æ·±åº¦å­¸ç¿’æ¨¡å‹ï¼ˆæ›¿æ› Cell 14ï¼‰\n",
    "# ===========================\n",
    "\n",
    "class ImprovedNN(nn.Module):\n",
    "    \"\"\"æ”¹é€²çš„ç¥ç¶“ç¶²è·¯ - åŠ å…¥æ›´å¤šæŠ€å·§\"\"\"\n",
    "    def __init__(self, input_size, num_classes=4):\n",
    "        super(ImprovedNN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc5 = nn.Linear(64, num_classes)\n",
    "        \n",
    "        # åˆå§‹åŒ–æ¬Šé‡\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "def train_improved_nn(X_train, X_test, y_train, y_test, epochs=100):  # epochsåœ¨é€™è£¡\n",
    "    \"\"\"è¨“ç·´æ”¹é€²çš„æ·±åº¦å­¸ç¿’æ¨¡å‹\"\"\"\n",
    "    print(\"\\nè¨“ç·´æ”¹é€²çš„ç¥ç¶“ç¶²è·¯...\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # æ¨™æº–åŒ–\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # è½‰æ›ç‚ºå¼µé‡\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    \n",
    "    # DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "    \n",
    "    # å‰µå»ºæ¨¡å‹ - ä¸å‚³å…¥epochs\n",
    "    model = ImprovedNN(X_train.shape[1]).to(device)\n",
    "    \n",
    "    # æå¤±å‡½æ•¸\n",
    "    class_weights_tensor = torch.FloatTensor(list(class_weight_dict.values())).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # è¨“ç·´å¾ªç’°\n",
    "    start_time = time.time()\n",
    "    best_balanced_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):  # epochsåœ¨é€™è£¡ä½¿ç”¨\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # æ¯10å€‹epochè©•ä¼°ä¸€æ¬¡\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_test_tensor)\n",
    "                _, predicted = torch.max(val_outputs, 1)\n",
    "                val_balanced_acc = balanced_accuracy_score(y_test, predicted.cpu().numpy())\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}, \"\n",
    "                  f\"Balanced Acc: {val_balanced_acc:.4f}\")\n",
    "            \n",
    "            if val_balanced_acc > best_balanced_acc:\n",
    "                best_balanced_acc = val_balanced_acc\n",
    "                best_model_state = model.state_dict()\n",
    "    \n",
    "    # è¼‰å…¥æœ€ä½³æ¨¡å‹\n",
    "    if best_balanced_acc > 0:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # æœ€çµ‚è©•ä¼°\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred = predicted.cpu().numpy()\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nè¨“ç·´æ™‚é–“: {train_time:.2f} ç§’\")\n",
    "    print(f\"æº–ç¢ºç‡: {accuracy:.4f}\")\n",
    "    print(f\"F1åˆ†æ•¸: {f1:.4f}\")\n",
    "    print(f\"å¹³è¡¡æº–ç¢ºç‡: {balanced_acc:.4f}\")\n",
    "    \n",
    "    return model, scaler, accuracy, f1, balanced_acc\n",
    "\n",
    "# åŸ·è¡Œè¨“ç·´\n",
    "nn_model, nn_scaler, nn_acc, nn_f1, nn_balanced_acc = train_improved_nn(\n",
    "    X_train_balanced, X_test, y_train_balanced, y_test, epochs=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ad0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 15: æ¨¡å‹æ¯”è¼ƒå’Œé›†æˆ\n",
    "# ===========================\n",
    "\n",
    "# æ”¶é›†æ‰€æœ‰çµæœ\n",
    "results = {\n",
    "    'LightGBM': {'accuracy': lgb_acc, 'f1': lgb_f1, 'balanced_acc': lgb_balanced_acc},\n",
    "    'XGBoost': {'accuracy': xgb_acc, 'f1': xgb_f1, 'balanced_acc': xgb_balanced_acc},\n",
    "    'CatBoost': {'accuracy': cat_acc, 'f1': cat_f1, 'balanced_acc': cat_balanced_acc},\n",
    "    'Balanced_RF': {'accuracy': brf_acc, 'f1': brf_f1, 'balanced_acc': brf_balanced_acc},\n",
    "    'Neural_Network': {'accuracy': nn_acc, 'f1': nn_f1, 'balanced_acc': nn_balanced_acc}\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'æ¨¡å‹':<20} {'æº–ç¢ºç‡':<10} {'F1åˆ†æ•¸':<10} {'å¹³è¡¡æº–ç¢ºç‡':<10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for model_name, metrics in sorted(results.items(), key=lambda x: x[1]['balanced_acc'], reverse=True):\n",
    "    print(f\"{model_name:<20} {metrics['accuracy']:<10.4f} {metrics['f1']:<10.4f} {metrics['balanced_acc']:<10.4f}\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³æ¨¡å‹\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['balanced_acc'])[0]\n",
    "print(f\"\\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}\")\n",
    "print(f\"   å¹³è¡¡æº–ç¢ºç‡: {results[best_model_name]['balanced_acc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47563d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 16: ä¿å­˜æ¨¡å‹å’Œçµæœï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
    "# ===========================\n",
    "\n",
    "def save_models_and_results(models, results, feature_names, label_encoders):\n",
    "    \"\"\"ä¿å­˜æ‰€æœ‰æ¨¡å‹å’Œçµæœ\"\"\"\n",
    "    output_dir = './model_output/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # ä¿å­˜æ¨¡å‹\n",
    "    model_dict = {\n",
    "        'LightGBM': lgb_model,\n",
    "        'XGBoost': xgb_model,\n",
    "        'CatBoost': cat_model,\n",
    "        'Balanced_RF': brf_model\n",
    "    }\n",
    "    \n",
    "    for name, model in model_dict.items():\n",
    "        if name == 'LightGBM':\n",
    "            model.save_model(f'{output_dir}{name.lower()}_model.txt')\n",
    "        else:\n",
    "            joblib.dump(model, f'{output_dir}{name.lower()}_model.pkl')\n",
    "    \n",
    "    # ä¿å­˜ç¥ç¶“ç¶²è·¯\n",
    "    torch.save(nn_model.state_dict(), f'{output_dir}neural_network_model.pth')\n",
    "    joblib.dump(nn_scaler, f'{output_dir}nn_scaler.pkl')\n",
    "    \n",
    "    # ä¿å­˜ç‰¹å¾µåç¨±å’Œç·¨ç¢¼å™¨\n",
    "    joblib.dump(feature_names, f'{output_dir}feature_names.pkl')\n",
    "    joblib.dump(label_encoders, f'{output_dir}label_encoders.pkl')\n",
    "    \n",
    "    # ä¿å­˜çµæœ\n",
    "    import json\n",
    "    with open(f'{output_dir}results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    # ä¿å­˜è¨“ç·´ä¿¡æ¯ - ä¿®æ­£ï¼šå°‡numpyé¡å‹è½‰æ›ç‚ºPythonåŸç”Ÿé¡å‹\n",
    "    train_info = {\n",
    "        'train_size': int(len(X_train)),  # è½‰æ›ç‚ºint\n",
    "        'test_size': int(len(X_test)),    # è½‰æ›ç‚ºint\n",
    "        'n_features': int(len(feature_names)),  # è½‰æ›ç‚ºint\n",
    "        'class_distribution': {int(k): int(v) for k, v in zip(*np.unique(y, return_counts=True))},  # è½‰æ›éµå€¼\n",
    "        'best_model': best_model_name,\n",
    "        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    with open(f'{output_dir}train_info.json', 'w') as f:\n",
    "        json.dump(train_info, f, indent=4)\n",
    "    \n",
    "    print(f\"\\nâœ… æ‰€æœ‰æ¨¡å‹å’Œçµæœå·²ä¿å­˜è‡³: {output_dir}\")\n",
    "\n",
    "# ä¿å­˜\n",
    "save_models_and_results(\n",
    "    {'nn_model': nn_model, 'nn_scaler': nn_scaler},\n",
    "    results,\n",
    "    feature_names,\n",
    "    label_encoders\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b70c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 17: æº–å‚™æ™‚ç©ºé æ¸¬æ•¸æ“š\n",
    "# ===========================\n",
    "\n",
    "# é¦–å…ˆï¼Œæˆ‘å€‘éœ€è¦é‡æ–°è¼‰å…¥åŒ…å«åœ°ç†ä½ç½®çš„åŸå§‹æ•¸æ“š\n",
    "print(\"é‡æ–°è¼‰å…¥åœ°ç†æ•¸æ“š...\")\n",
    "\n",
    "# è¼‰å…¥éœ€è¦çš„æ¬„ä½\n",
    "geo_cols = ['Start_Lat', 'Start_Lng', 'Start_Time', 'Severity', 'State', 'City']\n",
    "df_geo = pd.read_csv(file_path, usecols=geo_cols, nrows=1000000)  # å…ˆç”¨100è¬ç­†æ¸¬è©¦\n",
    "\n",
    "# è™•ç†æ™‚é–“\n",
    "df_geo['Start_Time'] = pd.to_datetime(df_geo['Start_Time'])\n",
    "df_geo = df_geo.dropna(subset=['Start_Lat', 'Start_Lng'])\n",
    "\n",
    "print(f\"åœ°ç†æ•¸æ“šå¤§å°: {df_geo.shape}\")\n",
    "print(f\"æ•¸æ“šç¯„åœ: Lat [{df_geo['Start_Lat'].min():.2f}, {df_geo['Start_Lat'].max():.2f}], \"\n",
    "      f\"Lng [{df_geo['Start_Lng'].min():.2f}, {df_geo['Start_Lng'].max():.2f}]\")\n",
    "\n",
    "# ===========================\n",
    "# Cell 18: å‰µå»ºç¶²æ ¼åŒ–åœ°åœ–æ•¸æ“š\n",
    "# ===========================\n",
    "\n",
    "def create_grid_statistics(df_geo):\n",
    "    \"\"\"å‰µå»ºç¶²æ ¼åŒ–çš„äº‹æ•…çµ±è¨ˆ\"\"\"\n",
    "    print(\"å‰µå»ºç¶²æ ¼çµ±è¨ˆ...\")\n",
    "    \n",
    "    # æå–æ™‚é–“ç‰¹å¾µ\n",
    "    df_geo['Hour'] = df_geo['Start_Time'].dt.hour\n",
    "    df_geo['DayOfWeek'] = df_geo['Start_Time'].dt.dayofweek\n",
    "    df_geo['Month'] = df_geo['Start_Time'].dt.month\n",
    "    \n",
    "    # å‰µå»ºåœ°ç†ç¶²æ ¼ï¼ˆ0.5åº¦ x 0.5åº¦ï¼‰\n",
    "    df_geo['lat_grid'] = (df_geo['Start_Lat'] // 0.5) * 0.5\n",
    "    df_geo['lng_grid'] = (df_geo['Start_Lng'] // 0.5) * 0.5\n",
    "    \n",
    "    # çµ±è¨ˆæ¯å€‹ç¶²æ ¼çš„äº‹æ•…\n",
    "    grid_stats = df_geo.groupby(['lat_grid', 'lng_grid']).agg({\n",
    "        'Severity': ['count', 'mean'],\n",
    "        'Hour': lambda x: x.mode()[0] if len(x) > 0 else 12,\n",
    "        'DayOfWeek': lambda x: x.mode()[0] if len(x) > 0 else 1\n",
    "    }).reset_index()\n",
    "    \n",
    "    # ç°¡åŒ–åˆ—å\n",
    "    grid_stats.columns = ['lat', 'lng', 'accident_count', 'avg_severity', 'common_hour', 'common_day']\n",
    "    \n",
    "    # åªä¿ç•™æœ‰è¶³å¤ äº‹æ•…çš„ç¶²æ ¼\n",
    "    grid_stats = grid_stats[grid_stats['accident_count'] >= 10]\n",
    "    \n",
    "    print(f\"ç¶²æ ¼æ•¸é‡: {len(grid_stats)}\")\n",
    "    \n",
    "    return grid_stats, df_geo\n",
    "\n",
    "grid_stats, df_geo_processed = create_grid_statistics(df_geo)\n",
    "\n",
    "# ===========================\n",
    "# Cell 19: å‰µå»ºäº’å‹•å¼åœ°åœ–ï¼ˆä½¿ç”¨Plotlyï¼‰\n",
    "# ===========================\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "def create_interactive_map(grid_stats, selected_hour=None, selected_day=None):\n",
    "    \"\"\"å‰µå»ºäº’å‹•å¼äº‹æ•…ç†±åŠ›åœ–\"\"\"\n",
    "    \n",
    "    # ç¯©é¸æ•¸æ“š\n",
    "    data = grid_stats.copy()\n",
    "    if selected_hour is not None:\n",
    "        # ç¯©é¸ç›¸ä¼¼æ™‚é–“çš„æ•¸æ“š\n",
    "        data = data[np.abs(data['common_hour'] - selected_hour) <= 3]\n",
    "    \n",
    "    # å‰µå»ºåœ°åœ–\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # æ·»åŠ ç†±åŠ›åœ–å±¤\n",
    "    fig.add_trace(go.Scattermapbox(\n",
    "        lat=data['lat'],\n",
    "        lon=data['lng'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=np.log1p(data['accident_count']) * 3,  # å°æ•¸ç¸®æ”¾\n",
    "            color=data['avg_severity'],\n",
    "            colorscale='Reds',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"å¹³å‡åš´é‡åº¦\"),\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        text=[f\"ä½ç½®: ({lat:.2f}, {lng:.2f})<br>\"\n",
    "              f\"äº‹æ•…æ•¸: {count}<br>\"\n",
    "              f\"å¹³å‡åš´é‡åº¦: {sev:.2f}\"\n",
    "              for lat, lng, count, sev in zip(\n",
    "                  data['lat'], data['lng'], \n",
    "                  data['accident_count'], data['avg_severity'])],\n",
    "        hovertemplate='%{text}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    # è¨­ç½®åœ°åœ–æ¨£å¼\n",
    "    fig.update_layout(\n",
    "        mapbox=dict(\n",
    "            style=\"open-street-map\",\n",
    "            center=dict(lat=39.8283, lon=-98.5795),  # ç¾åœ‹ä¸­å¿ƒ\n",
    "            zoom=3\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        height=600,\n",
    "        title=f\"ç¾åœ‹äº¤é€šäº‹æ•…ç†±åŠ›åœ–\" + \n",
    "              (f\" - {selected_hour}:00\" if selected_hour is not None else \"\")\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# å‰µå»ºåŸºç¤åœ°åœ–\n",
    "base_map = create_interactive_map(grid_stats)\n",
    "base_map.show()\n",
    "\n",
    "# ===========================\n",
    "# Cell 20: æ™‚é–“å‹•æ…‹åˆ†æ\n",
    "# ===========================\n",
    "\n",
    "# å‰µå»ºæŒ‰å°æ™‚çš„äº‹æ•…åˆ†å¸ƒ\n",
    "hourly_stats = df_geo_processed.groupby('Hour').agg({\n",
    "    'Severity': ['count', 'mean']\n",
    "}).reset_index()\n",
    "hourly_stats.columns = ['Hour', 'Count', 'Avg_Severity']\n",
    "\n",
    "# ç¹ªè£½æ™‚é–“åˆ†å¸ƒåœ–\n",
    "fig_time = go.Figure()\n",
    "\n",
    "# äº‹æ•…æ•¸é‡\n",
    "fig_time.add_trace(go.Bar(\n",
    "    x=hourly_stats['Hour'],\n",
    "    y=hourly_stats['Count'],\n",
    "    name='äº‹æ•…æ•¸é‡',\n",
    "    yaxis='y'\n",
    "))\n",
    "\n",
    "# å¹³å‡åš´é‡åº¦\n",
    "fig_time.add_trace(go.Scatter(\n",
    "    x=hourly_stats['Hour'],\n",
    "    y=hourly_stats['Avg_Severity'],\n",
    "    name='å¹³å‡åš´é‡åº¦',\n",
    "    yaxis='y2',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "fig_time.update_layout(\n",
    "    title='24å°æ™‚äº‹æ•…åˆ†å¸ƒ',\n",
    "    xaxis=dict(title='å°æ™‚'),\n",
    "    yaxis=dict(title='äº‹æ•…æ•¸é‡', side='left'),\n",
    "    yaxis2=dict(title='å¹³å‡åš´é‡åº¦', side='right', overlaying='y'),\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig_time.show()\n",
    "\n",
    "# ===========================\n",
    "# Cell 21: é æ¸¬å‡½æ•¸\n",
    "# ===========================\n",
    "\n",
    "def predict_accident_risk(lat, lng, hour, day_of_week, model, scaler, feature_template):\n",
    "    \"\"\"é æ¸¬ç‰¹å®šä½ç½®å’Œæ™‚é–“çš„äº‹æ•…é¢¨éšª\"\"\"\n",
    "    \n",
    "    # å‰µå»ºç‰¹å¾µå‘é‡ï¼ˆéœ€è¦åŒ¹é…è¨“ç·´æ™‚çš„ç‰¹å¾µï¼‰\n",
    "    # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›éœ€è¦å®Œæ•´çš„ç‰¹å¾µå·¥ç¨‹\n",
    "    features = np.zeros(len(feature_template))\n",
    "    \n",
    "    # å¡«å…¥åŸºæœ¬ç‰¹å¾µ\n",
    "    features[0] = hour\n",
    "    features[1] = day_of_week\n",
    "    # ... å…¶ä»–ç‰¹å¾µ\n",
    "    \n",
    "    # æ¨™æº–åŒ–\n",
    "    features_scaled = scaler.transform(features.reshape(1, -1))\n",
    "    \n",
    "    # é æ¸¬\n",
    "    prediction = model.predict(features_scaled)[0]\n",
    "    probability = model.predict_proba(features_scaled)[0]\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# ===========================\n",
    "# Cell 22: é«˜é¢¨éšªå€åŸŸè­˜åˆ¥\n",
    "# ===========================\n",
    "\n",
    "def identify_high_risk_areas(grid_stats, threshold_percentile=90):\n",
    "    \"\"\"è­˜åˆ¥é«˜é¢¨éšªå€åŸŸ\"\"\"\n",
    "    \n",
    "    # è¨ˆç®—é¢¨éšªåˆ†æ•¸ï¼ˆçµåˆäº‹æ•…æ•¸é‡å’Œåš´é‡åº¦ï¼‰\n",
    "    grid_stats['risk_score'] = (\n",
    "        grid_stats['accident_count'] * 0.3 + \n",
    "        grid_stats['avg_severity'] * 100 * 0.7\n",
    "    )\n",
    "    \n",
    "    # æ‰¾å‡ºé«˜é¢¨éšªå€åŸŸ\n",
    "    threshold = np.percentile(grid_stats['risk_score'], threshold_percentile)\n",
    "    high_risk = grid_stats[grid_stats['risk_score'] >= threshold].copy()\n",
    "    \n",
    "    # æ’åº\n",
    "    high_risk = high_risk.sort_values('risk_score', ascending=False)\n",
    "    \n",
    "    print(f\"è­˜åˆ¥å‡º {len(high_risk)} å€‹é«˜é¢¨éšªå€åŸŸ\")\n",
    "    print(\"\\nTop 10 é«˜é¢¨éšªå€åŸŸ:\")\n",
    "    for idx, row in high_risk.head(10).iterrows():\n",
    "        print(f\"  ({row['lat']:.2f}, {row['lng']:.2f}) - \"\n",
    "              f\"é¢¨éšªåˆ†æ•¸: {row['risk_score']:.2f}, \"\n",
    "              f\"äº‹æ•…æ•¸: {row['accident_count']}\")\n",
    "    \n",
    "    return high_risk\n",
    "\n",
    "high_risk_areas = identify_high_risk_areas(grid_stats)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
