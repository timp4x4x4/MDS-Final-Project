{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfdf69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 1: 導入套件和設定\n",
    "# ===========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import gc  # 垃圾回收\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 基本套件\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, \n",
    "    confusion_matrix, balanced_accuracy_score, \n",
    "    cohen_kappa_score, make_scorer\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# 處理不平衡資料\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# Boosting模型\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import torch.nn.functional as F \n",
    "\n",
    "print(\"環境檢查:\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"XGBoost: {xgb.__version__}\")\n",
    "print(f\"LightGBM: {lgb.__version__}\")\n",
    "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30044c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 2: 記憶體優化函數\n",
    "# ===========================\n",
    "\n",
    "def reduce_memory_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    通過改變數據類型來減少DataFrame的記憶體使用\n",
    "    參考自Kaggle的記憶體優化技術\n",
    "    \"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f'記憶體使用減少了 {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "        print(f'{start_mem:.2f} MB --> {end_mem:.2f} MB')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_memory():\n",
    "    \"\"\"清理記憶體\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 3: 載入資料（優化版）\n",
    "# ===========================\n",
    "\n",
    "def load_data_optimized(file_path, sample_frac=None, chunksize=None):\n",
    "    \"\"\"\n",
    "    優化的資料載入，支援採樣和分塊讀取\n",
    "    \"\"\"\n",
    "    print(f\"載入資料: {file_path}\")\n",
    "    \n",
    "    # 先讀取一小部分來了解資料\n",
    "    sample_df = pd.read_csv(file_path, nrows=5)\n",
    "    print(\"資料欄位預覽:\")\n",
    "    print(sample_df.columns.tolist())\n",
    "    \n",
    "    # 定義需要的欄位（排除不需要的文字欄位以節省記憶體）\n",
    "    # 根據其他Kaggle notebook的經驗，這些是最重要的欄位\n",
    "    important_cols = [\n",
    "        'Severity', 'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng',\n",
    "        'Distance(mi)', 'Temperature(F)', 'Humidity(%)', 'Pressure(in)',\n",
    "        'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)',\n",
    "        'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way',\n",
    "        'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop',\n",
    "        'Traffic_Calming', 'Traffic_Signal', 'Sunrise_Sunset', 'State',\n",
    "        'Side', 'Weather_Timestamp'\n",
    "    ]\n",
    "    \n",
    "    # 過濾存在的欄位\n",
    "    existing_cols = [col for col in important_cols if col in sample_df.columns]\n",
    "    \n",
    "    # 定義數據類型以減少記憶體\n",
    "    dtype_dict = {\n",
    "        'Severity': 'int8',\n",
    "        'Distance(mi)': 'float32',\n",
    "        'Temperature(F)': 'float32',\n",
    "        'Humidity(%)': 'float32',\n",
    "        'Pressure(in)': 'float32',\n",
    "        'Visibility(mi)': 'float32',\n",
    "        'Wind_Speed(mph)': 'float32',\n",
    "        'Precipitation(in)': 'float32',\n",
    "        'Amenity': 'bool',\n",
    "        'Bump': 'bool',\n",
    "        'Crossing': 'bool',\n",
    "        'Give_Way': 'bool',\n",
    "        'Junction': 'bool',\n",
    "        'No_Exit': 'bool',\n",
    "        'Railway': 'bool',\n",
    "        'Roundabout': 'bool',\n",
    "        'Station': 'bool',\n",
    "        'Stop': 'bool',\n",
    "        'Traffic_Calming': 'bool',\n",
    "        'Traffic_Signal': 'bool'\n",
    "    }\n",
    "    \n",
    "    # 載入資料\n",
    "    if sample_frac:\n",
    "        # 隨機採樣\n",
    "        print(f\"載入 {sample_frac*100}% 的資料...\")\n",
    "        df = pd.read_csv(file_path, usecols=existing_cols, dtype=dtype_dict)\n",
    "        df = df.sample(frac=sample_frac, random_state=42)\n",
    "    elif chunksize:\n",
    "        # 分塊載入\n",
    "        print(f\"分塊載入，每塊 {chunksize} 行...\")\n",
    "        chunks = []\n",
    "        for chunk in pd.read_csv(file_path, usecols=existing_cols, \n",
    "                                dtype=dtype_dict, chunksize=chunksize):\n",
    "            chunks.append(chunk)\n",
    "            if len(chunks) * chunksize >= 1000000:  # 限制在100萬行\n",
    "                break\n",
    "        df = pd.concat(chunks, ignore_index=True)\n",
    "    else:\n",
    "        # 完整載入\n",
    "        df = pd.read_csv(file_path, usecols=existing_cols, dtype=dtype_dict)\n",
    "    \n",
    "    print(f\"載入資料大小: {df.shape}\")\n",
    "    print(f\"記憶體使用: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # 顯示目標變數分布\n",
    "    print(\"\\n目標變數分布:\")\n",
    "    severity_counts = df['Severity'].value_counts().sort_index()\n",
    "    for sev, count in severity_counts.items():\n",
    "        print(f\"Severity {sev}: {count:,} ({count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 執行載入（建議先用小樣本測試）\n",
    "file_path = 'us-accidents/US_Accidents_March23.csv'\n",
    "\n",
    "# 選項1: 使用部分資料（推薦用於測試）\n",
    "# df = load_data_optimized(file_path, sample_frac=0.1)  # 10%資料\n",
    "\n",
    "# 選項2: 分塊載入\n",
    "# df = load_data_optimized(file_path, chunksize=500000)  # 每次50萬行\n",
    "\n",
    "# 選項3: 完整載入（需要大量記憶體）\n",
    "df = load_data_optimized(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dce068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 4: 日期時間處理（優化版）\n",
    "# ===========================\n",
    "\n",
    "def process_datetime_features(df):\n",
    "    \"\"\"處理日期時間特徵\"\"\"\n",
    "    print(\"\\n處理日期時間特徵...\")\n",
    "    \n",
    "    # 轉換日期時間\n",
    "    df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\n",
    "    df['End_Time'] = pd.to_datetime(df['End_Time'], errors='coerce')\n",
    "    \n",
    "    # 計算持續時間\n",
    "    df['Duration_minutes'] = (df['End_Time'] - df['Start_Time']).dt.total_seconds() / 60\n",
    "    \n",
    "    # 過濾異常值（使用更寬鬆的範圍）\n",
    "    df = df[(df['Duration_minutes'] > 0) & (df['Duration_minutes'] < 1440*7)]  # 小於7天\n",
    "    \n",
    "    # 移除日期時間為空的記錄\n",
    "    df = df.dropna(subset=['Start_Time'])\n",
    "    \n",
    "    # 提取時間特徵\n",
    "    df['Hour'] = df['Start_Time'].dt.hour.astype('int8')\n",
    "    df['DayOfWeek'] = df['Start_Time'].dt.dayofweek.astype('int8')\n",
    "    df['Month'] = df['Start_Time'].dt.month.astype('int8')\n",
    "    df['Year'] = df['Start_Time'].dt.year.astype('int16')\n",
    "    \n",
    "    # 衍生特徵\n",
    "    df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype('int8')\n",
    "    df['IsRushHour'] = df['Hour'].apply(\n",
    "        lambda x: 1 if (6 <= x <= 9) or (16 <= x <= 19) else 0\n",
    "    ).astype('int8')\n",
    "    \n",
    "    # 時段分類\n",
    "    df['TimeOfDay'] = pd.cut(df['Hour'], \n",
    "                            bins=[-1, 6, 12, 18, 24], \n",
    "                            labels=[0, 1, 2, 3]).astype('int8')  # 轉換為數值\n",
    "    \n",
    "    # 季節\n",
    "    df['Season'] = pd.cut(df['Month'], \n",
    "                         bins=[0, 3, 6, 9, 12], \n",
    "                         labels=[0, 1, 2, 3]).astype('int8')  # 轉換為數值\n",
    "    \n",
    "    # 刪除原始時間欄位以節省記憶體\n",
    "    df = df.drop(['Start_Time', 'End_Time', 'Weather_Timestamp'], axis=1, errors='ignore')\n",
    "    \n",
    "    print(f\"處理後大小: {df.shape}\")\n",
    "    clean_memory()\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = process_datetime_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77396eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 5: 天氣特徵處理\n",
    "# ===========================\n",
    "\n",
    "def process_weather_features(df):\n",
    "    \"\"\"處理天氣相關特徵\"\"\"\n",
    "    print(\"\\n處理天氣特徵...\")\n",
    "    \n",
    "    if 'Weather_Condition' in df.columns:\n",
    "        # 簡化天氣分類\n",
    "        def categorize_weather(condition):\n",
    "            if pd.isna(condition):\n",
    "                return 0  # Unknown\n",
    "            condition = str(condition).lower()\n",
    "            if any(word in condition for word in ['clear', 'fair']):\n",
    "                return 1  # Clear\n",
    "            elif any(word in condition for word in ['cloud', 'overcast']):\n",
    "                return 2  # Cloudy\n",
    "            elif any(word in condition for word in ['rain', 'drizzle']):\n",
    "                return 3  # Rain\n",
    "            elif any(word in condition for word in ['snow', 'sleet']):\n",
    "                return 4  # Snow\n",
    "            elif any(word in condition for word in ['fog', 'mist']):\n",
    "                return 5  # Fog\n",
    "            elif any(word in condition for word in ['storm', 'thunder']):\n",
    "                return 6  # Storm\n",
    "            else:\n",
    "                return 7  # Other\n",
    "        \n",
    "        df['Weather_Category'] = df['Weather_Condition'].apply(categorize_weather).astype('int8')\n",
    "        df = df.drop('Weather_Condition', axis=1)\n",
    "    \n",
    "    # 處理其他天氣數值特徵的缺失值\n",
    "    weather_numeric_cols = ['Temperature(F)', 'Humidity(%)', 'Pressure(in)', \n",
    "                           'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']\n",
    "    \n",
    "    for col in weather_numeric_cols:\n",
    "        if col in df.columns:\n",
    "            # 使用中位數填充\n",
    "            median_val = df[col].median()\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "    \n",
    "    clean_memory()\n",
    "    return df\n",
    "\n",
    "df = process_weather_features(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 6: 處理缺失值和編碼類別變數\n",
    "# ===========================\n",
    "\n",
    "def handle_missing_and_encode(df):\n",
    "    \"\"\"處理缺失值並編碼類別變數\"\"\"\n",
    "    print(\"\\n處理缺失值和編碼...\")\n",
    "    \n",
    "    # 刪除缺失值過多的欄位\n",
    "    missing_pct = df.isnull().sum() / len(df)\n",
    "    high_missing_cols = missing_pct[missing_pct > 0.5].index.tolist()\n",
    "    \n",
    "    # 保留Severity\n",
    "    if 'Severity' in high_missing_cols:\n",
    "        high_missing_cols.remove('Severity')\n",
    "    \n",
    "    df = df.drop(columns=high_missing_cols, errors='ignore')\n",
    "    print(f\"刪除高缺失率欄位: {len(high_missing_cols)}\")\n",
    "    \n",
    "    # 對類別變數進行標籤編碼\n",
    "    categorical_cols = ['State', 'Side', 'Sunrise_Sunset']\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            # 填充缺失值\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "            # 編碼\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # 填充數值型缺失值\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col != 'Severity':\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # 確保布林型欄位是整數\n",
    "    bool_cols = df.select_dtypes(include=['bool']).columns\n",
    "    for col in bool_cols:\n",
    "        df[col] = df[col].astype('int8')\n",
    "    \n",
    "    print(f\"處理後資料大小: {df.shape}\")\n",
    "    print(f\"剩餘缺失值: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    clean_memory()\n",
    "    return df, label_encoders\n",
    "\n",
    "df, label_encoders = handle_missing_and_encode(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 7: 特徵選擇和準備最終數據\n",
    "# ===========================\n",
    "\n",
    "def prepare_final_data(df):\n",
    "    \"\"\"準備最終的訓練數據\"\"\"\n",
    "    print(\"\\n準備最終數據...\")\n",
    "    \n",
    "    # 刪除任何仍有缺失值的行\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # 確保Severity是正確的值\n",
    "    df = df[df['Severity'].isin([1, 2, 3, 4])]\n",
    "    \n",
    "    # 根據Kaggle上的建議，考慮合併Severity 1和2\n",
    "    # 因為Severity 1的樣本太少\n",
    "    print(\"\\n原始類別分布:\")\n",
    "    print(df['Severity'].value_counts().sort_index())\n",
    "    \n",
    "    # 選項：合併類別（可選）\n",
    "    # df['Severity'] = df['Severity'].replace({1: 2})\n",
    "    \n",
    "    # 分離特徵和目標\n",
    "    feature_cols = [col for col in df.columns if col != 'Severity']\n",
    "    X = df[feature_cols].values\n",
    "    y = df['Severity'].values - 1  # 轉換為0-3\n",
    "    \n",
    "    print(f\"\\n最終數據大小: X={X.shape}, y={y.shape}\")\n",
    "    print(\"最終類別分布:\")\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for cls, cnt in zip(unique, counts):\n",
    "        print(f\"  類別 {cls} (Severity {cls+1}): {cnt:,} ({cnt/len(y)*100:.2f}%)\")\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "X, y, feature_names = prepare_final_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ecf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 8: 資料分割\n",
    "# ===========================\n",
    "\n",
    "# 分層分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"訓練集: {X_train.shape}\")\n",
    "print(f\"測試集: {X_test.shape}\")\n",
    "\n",
    "# 計算類別權重\n",
    "class_weights = compute_class_weight('balanced', \n",
    "                                   classes=np.unique(y_train), \n",
    "                                   y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"\\n類別權重:\")\n",
    "for cls, weight in class_weight_dict.items():\n",
    "    print(f\"  類別 {cls}: {weight:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d040e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 9: 處理不平衡 - 混合採樣策略\n",
    "# ===========================\n",
    "\n",
    "def apply_mixed_sampling(X_train, y_train, strategy='mixed'):\n",
    "    \"\"\"\n",
    "    應用混合採樣策略\n",
    "    參考Kaggle最佳實踐：結合過採樣和欠採樣\n",
    "    \"\"\"\n",
    "    print(f\"\\n應用採樣策略: {strategy}\")\n",
    "    \n",
    "    if strategy == 'none':\n",
    "        return X_train, y_train\n",
    "    \n",
    "    # 計算各類別數量\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    class_counts = dict(zip(unique, counts))\n",
    "    print(\"原始分布:\", class_counts)\n",
    "    \n",
    "    if strategy == 'mixed':\n",
    "        # 混合策略：對多數類欠採樣，對少數類過採樣\n",
    "        # 目標：讓所有類別接近中位數\n",
    "        median_count = int(np.median(counts))\n",
    "        target_count = int(median_count * 1.5)  # 目標數量設為中位數的1.5倍\n",
    "        \n",
    "        # 第一步：欠採樣 - 只對超過目標數量的類別進行欠採樣\n",
    "        undersample_strategy = {}\n",
    "        for cls, cnt in class_counts.items():\n",
    "            if cnt > target_count:\n",
    "                undersample_strategy[cls] = target_count\n",
    "            else:\n",
    "                undersample_strategy[cls] = cnt  # 保持原樣\n",
    "        \n",
    "        if len(undersample_strategy) > 0 and any(cnt < class_counts[cls] for cls, cnt in undersample_strategy.items()):\n",
    "            rus = RandomUnderSampler(sampling_strategy=undersample_strategy, random_state=42)\n",
    "            X_temp, y_temp = rus.fit_resample(X_train, y_train)\n",
    "        else:\n",
    "            X_temp, y_temp = X_train, y_train\n",
    "        \n",
    "        # 第二步：過採樣 - 只對少於目標數量的類別進行過採樣\n",
    "        temp_unique, temp_counts = np.unique(y_temp, return_counts=True)\n",
    "        temp_class_counts = dict(zip(temp_unique, temp_counts))\n",
    "        \n",
    "        oversample_strategy = {}\n",
    "        for cls, cnt in temp_class_counts.items():\n",
    "            if cnt < target_count:\n",
    "                oversample_strategy[cls] = target_count\n",
    "            else:\n",
    "                oversample_strategy[cls] = cnt  # 保持原樣\n",
    "        \n",
    "        if len(oversample_strategy) > 0 and any(cnt > temp_class_counts[cls] for cls, cnt in oversample_strategy.items()):\n",
    "            ros = RandomOverSampler(sampling_strategy=oversample_strategy, random_state=42)\n",
    "            X_resampled, y_resampled = ros.fit_resample(X_temp, y_temp)\n",
    "        else:\n",
    "            X_resampled, y_resampled = X_temp, y_temp\n",
    "            \n",
    "    elif strategy == 'smote':\n",
    "        # SMOTE策略：只過採樣到最多類別的50%\n",
    "        max_count = max(counts)\n",
    "        target_count = int(max_count * 0.5)\n",
    "        \n",
    "        # 確保目標數量不小於當前數量\n",
    "        sampling_strategy = {}\n",
    "        for cls, cnt in class_counts.items():\n",
    "            if cnt < target_count:\n",
    "                sampling_strategy[cls] = target_count\n",
    "            else:\n",
    "                sampling_strategy[cls] = cnt\n",
    "        \n",
    "        smote = SMOTE(sampling_strategy=sampling_strategy, k_neighbors=5, random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    elif strategy == 'undersample_only':\n",
    "        # 只欠採樣到最少類別的2倍\n",
    "        min_count = min(counts)\n",
    "        target_count = min_count * 2\n",
    "        \n",
    "        sampling_strategy = {}\n",
    "        for cls, cnt in class_counts.items():\n",
    "            sampling_strategy[cls] = min(cnt, target_count)\n",
    "        \n",
    "        rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "        X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # 顯示新分布\n",
    "    unique_new, counts_new = np.unique(y_resampled, return_counts=True)\n",
    "    new_distribution = dict(zip(unique_new, counts_new))\n",
    "    print(\"採樣後分布:\", new_distribution)\n",
    "    \n",
    "    # 顯示變化\n",
    "    print(\"\\n採樣變化:\")\n",
    "    for cls in range(4):\n",
    "        original = class_counts.get(cls, 0)\n",
    "        new = new_distribution.get(cls, 0)\n",
    "        change = ((new - original) / original * 100) if original > 0 else 0\n",
    "        print(f\"  類別 {cls}: {original:,} → {new:,} ({change:+.1f}%)\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# 應用混合採樣\n",
    "# 可以嘗試不同策略\n",
    "X_train_balanced, y_train_balanced = apply_mixed_sampling(X_train, y_train, 'mixed')\n",
    "\n",
    "# 如果混合策略還是有問題，可以嘗試其他策略：\n",
    "# X_train_balanced, y_train_balanced = apply_mixed_sampling(X_train, y_train, 'undersample_only')\n",
    "# 或者不進行採樣：\n",
    "# X_train_balanced, y_train_balanced = X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 10: LightGBM模型（優化版）\n",
    "# ===========================\n",
    "\n",
    "def train_lightgbm_optimized(X_train, X_test, y_train, y_test, class_weights):\n",
    "    \"\"\"訓練優化的LightGBM模型\"\"\"\n",
    "    print(\"\\n訓練 LightGBM (優化版)...\")\n",
    "    \n",
    "    # 創建數據集\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "    \n",
    "    # 參數設置（基於Kaggle最佳實踐）\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 4,\n",
    "        'metric': 'multi_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 127,\n",
    "        'max_depth': -1,\n",
    "        'learning_rate': 0.05,\n",
    "        'n_estimators': 1000,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'min_child_samples': 20,\n",
    "        'min_split_gain': 0.02,\n",
    "        'class_weight': 'balanced',\n",
    "        'device': 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # 訓練\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[valid_data],\n",
    "        num_boost_round=1000,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 預測\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # 評估\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n訓練時間: {train_time:.2f} 秒\")\n",
    "    print(f\"最佳迭代次數: {model.best_iteration}\")\n",
    "    print(f\"準確率: {accuracy:.4f}\")\n",
    "    print(f\"F1分數: {f1:.4f}\")\n",
    "    print(f\"平衡準確率: {balanced_acc:.4f}\")\n",
    "    \n",
    "    # 詳細報告\n",
    "    print(\"\\n分類報告:\")\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                              target_names=[f'Severity {i+1}' for i in range(4)]))\n",
    "    \n",
    "    return model, accuracy, f1, balanced_acc\n",
    "\n",
    "# 訓練模型\n",
    "lgb_model, lgb_acc, lgb_f1, lgb_balanced_acc = train_lightgbm_optimized(\n",
    "    X_train_balanced, X_test, y_train_balanced, y_test, class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b648315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 11: XGBoost模型（穩定版）\n",
    "# ===========================\n",
    "\n",
    "def train_xgboost_stable(X_train, X_test, y_train, y_test, use_sample_weight=True):\n",
    "    \"\"\"穩定版XGBoost\"\"\"\n",
    "    print(\"\\n訓練 XGBoost (穩定版)...\")\n",
    "    \n",
    "    # 使用原始的類別權重，但不要太極端\n",
    "    if use_sample_weight:\n",
    "        # 溫和的類別權重\n",
    "        unique, counts = np.unique(y_train, return_counts=True)\n",
    "        weight_dict = {}\n",
    "        max_count = max(counts)\n",
    "        for cls, count in zip(unique, counts):\n",
    "            # 權重不超過10倍\n",
    "            weight_dict[cls] = min(max_count / count, 10.0)\n",
    "        \n",
    "        sample_weights = np.array([weight_dict[y] for y in y_train])\n",
    "    else:\n",
    "        sample_weights = None\n",
    "    \n",
    "    # XGBoost參數\n",
    "    params = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 4,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 300,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 5,  # 增加以防止過擬合\n",
    "        'gamma': 0.1,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 1,\n",
    "        'tree_method': 'gpu_hist' if torch.cuda.is_available() else 'hist',\n",
    "        'random_state': 42,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'mlogloss'\n",
    "    }\n",
    "    \n",
    "    # 訓練\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        sample_weight=sample_weights,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        # early_stopping_rounds=50,\n",
    "        verbose=100\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 預測\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 評估\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n訓練時間: {train_time:.2f} 秒\")\n",
    "    print(f\"準確率: {accuracy:.4f}\")\n",
    "    print(f\"F1分數: {f1:.4f}\")\n",
    "    print(f\"平衡準確率: {balanced_acc:.4f}\")\n",
    "    \n",
    "    print(\"\\n分類報告:\")\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                              target_names=[f'Severity {i+1}' for i in range(4)]))\n",
    "    \n",
    "    return model, accuracy, f1, balanced_acc\n",
    "\n",
    "# 執行訓練\n",
    "xgb_model, xgb_acc, xgb_f1, xgb_balanced_acc = train_xgboost_stable(\n",
    "    X_train_balanced, X_test, y_train_balanced, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 12: CatBoost模型\n",
    "# ===========================\n",
    "\n",
    "def train_catboost_optimized(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"訓練優化的CatBoost模型\"\"\"\n",
    "    print(\"\\n訓練 CatBoost (優化版)...\")\n",
    "    \n",
    "    # CatBoost參數\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=1000,\n",
    "        depth=8,\n",
    "        learning_rate=0.05,\n",
    "        loss_function='MultiClass',\n",
    "        eval_metric='TotalF1',\n",
    "        auto_class_weights='Balanced',\n",
    "        l2_leaf_reg=3,\n",
    "        random_strength=1,\n",
    "        bagging_temperature=1,\n",
    "        od_type='Iter',\n",
    "        od_wait=50,\n",
    "        task_type='GPU' if torch.cuda.is_available() else 'CPU',\n",
    "        devices='0',\n",
    "        random_state=42,\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    # 訓練\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_test, y_test),\n",
    "        early_stopping_rounds=50,\n",
    "        plot=False\n",
    "    )\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 預測\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 評估\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n訓練時間: {train_time:.2f} 秒\")\n",
    "    print(f\"準確率: {accuracy:.4f}\")\n",
    "    print(f\"F1分數: {f1:.4f}\")\n",
    "    print(f\"平衡準確率: {balanced_acc:.4f}\")\n",
    "    \n",
    "    return model, accuracy, f1, balanced_acc\n",
    "\n",
    "# 訓練CatBoost\n",
    "cat_model, cat_acc, cat_f1, cat_balanced_acc = train_catboost_optimized(\n",
    "    X_train_balanced, X_test, y_train_balanced, y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed3f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 13: Balanced Random Forest\n",
    "# ===========================\n",
    "\n",
    "def train_balanced_rf(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"訓練Balanced Random Forest\"\"\"\n",
    "    print(\"\\n訓練 Balanced Random Forest...\")\n",
    "    \n",
    "    model = BalancedRandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=20,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        class_weight='balanced_subsample',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 預測\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 評估\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n訓練時間: {train_time:.2f} 秒\")\n",
    "    print(f\"OOB分數: {model.oob_score_:.4f}\")\n",
    "    print(f\"準確率: {accuracy:.4f}\")\n",
    "    print(f\"F1分數: {f1:.4f}\")\n",
    "    print(f\"平衡準確率: {balanced_acc:.4f}\")\n",
    "    \n",
    "    # 詳細報告\n",
    "    print(\"\\n分類報告:\")\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                              target_names=[f'Severity {i+1}' for i in range(4)]))\n",
    "    \n",
    "    return model, accuracy, f1, balanced_acc\n",
    "\n",
    "# 訓練Balanced RF\n",
    "brf_model, brf_acc, brf_f1, brf_balanced_acc = train_balanced_rf(\n",
    "    X_train, X_test, y_train, y_test  # 使用原始數據，因為模型內部會平衡\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0384288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 改進的深度學習模型（替換 Cell 14）\n",
    "# ===========================\n",
    "\n",
    "class ImprovedNN(nn.Module):\n",
    "    \"\"\"改進的神經網路 - 加入更多技巧\"\"\"\n",
    "    def __init__(self, input_size, num_classes=4):\n",
    "        super(ImprovedNN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc5 = nn.Linear(64, num_classes)\n",
    "        \n",
    "        # 初始化權重\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "def train_improved_nn(X_train, X_test, y_train, y_test, epochs=100):  # epochs在這裡\n",
    "    \"\"\"訓練改進的深度學習模型\"\"\"\n",
    "    print(\"\\n訓練改進的神經網路...\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 標準化\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 轉換為張量\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    \n",
    "    # DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "    \n",
    "    # 創建模型 - 不傳入epochs\n",
    "    model = ImprovedNN(X_train.shape[1]).to(device)\n",
    "    \n",
    "    # 損失函數\n",
    "    class_weights_tensor = torch.FloatTensor(list(class_weight_dict.values())).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # 訓練循環\n",
    "    start_time = time.time()\n",
    "    best_balanced_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):  # epochs在這裡使用\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # 每10個epoch評估一次\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_test_tensor)\n",
    "                _, predicted = torch.max(val_outputs, 1)\n",
    "                val_balanced_acc = balanced_accuracy_score(y_test, predicted.cpu().numpy())\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}, \"\n",
    "                  f\"Balanced Acc: {val_balanced_acc:.4f}\")\n",
    "            \n",
    "            if val_balanced_acc > best_balanced_acc:\n",
    "                best_balanced_acc = val_balanced_acc\n",
    "                best_model_state = model.state_dict()\n",
    "    \n",
    "    # 載入最佳模型\n",
    "    if best_balanced_acc > 0:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # 最終評估\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred = predicted.cpu().numpy()\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n訓練時間: {train_time:.2f} 秒\")\n",
    "    print(f\"準確率: {accuracy:.4f}\")\n",
    "    print(f\"F1分數: {f1:.4f}\")\n",
    "    print(f\"平衡準確率: {balanced_acc:.4f}\")\n",
    "    \n",
    "    return model, scaler, accuracy, f1, balanced_acc\n",
    "\n",
    "# 執行訓練\n",
    "nn_model, nn_scaler, nn_acc, nn_f1, nn_balanced_acc = train_improved_nn(\n",
    "    X_train_balanced, X_test, y_train_balanced, y_test, epochs=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ad0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 15: 模型比較和集成\n",
    "# ===========================\n",
    "\n",
    "# 收集所有結果\n",
    "results = {\n",
    "    'LightGBM': {'accuracy': lgb_acc, 'f1': lgb_f1, 'balanced_acc': lgb_balanced_acc},\n",
    "    'XGBoost': {'accuracy': xgb_acc, 'f1': xgb_f1, 'balanced_acc': xgb_balanced_acc},\n",
    "    'CatBoost': {'accuracy': cat_acc, 'f1': cat_f1, 'balanced_acc': cat_balanced_acc},\n",
    "    'Balanced_RF': {'accuracy': brf_acc, 'f1': brf_f1, 'balanced_acc': brf_balanced_acc},\n",
    "    'Neural_Network': {'accuracy': nn_acc, 'f1': nn_f1, 'balanced_acc': nn_balanced_acc}\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"模型性能比較\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'模型':<20} {'準確率':<10} {'F1分數':<10} {'平衡準確率':<10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for model_name, metrics in sorted(results.items(), key=lambda x: x[1]['balanced_acc'], reverse=True):\n",
    "    print(f\"{model_name:<20} {metrics['accuracy']:<10.4f} {metrics['f1']:<10.4f} {metrics['balanced_acc']:<10.4f}\")\n",
    "\n",
    "# 找出最佳模型\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['balanced_acc'])[0]\n",
    "print(f\"\\n🏆 最佳模型: {best_model_name}\")\n",
    "print(f\"   平衡準確率: {results[best_model_name]['balanced_acc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47563d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 16: 保存模型和結果（修正版）\n",
    "# ===========================\n",
    "\n",
    "def save_models_and_results(models, results, feature_names, label_encoders):\n",
    "    \"\"\"保存所有模型和結果\"\"\"\n",
    "    output_dir = './model_output/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 保存模型\n",
    "    model_dict = {\n",
    "        'LightGBM': lgb_model,\n",
    "        'XGBoost': xgb_model,\n",
    "        'CatBoost': cat_model,\n",
    "        'Balanced_RF': brf_model\n",
    "    }\n",
    "    \n",
    "    for name, model in model_dict.items():\n",
    "        if name == 'LightGBM':\n",
    "            model.save_model(f'{output_dir}{name.lower()}_model.txt')\n",
    "        else:\n",
    "            joblib.dump(model, f'{output_dir}{name.lower()}_model.pkl')\n",
    "    \n",
    "    # 保存神經網路\n",
    "    torch.save(nn_model.state_dict(), f'{output_dir}neural_network_model.pth')\n",
    "    joblib.dump(nn_scaler, f'{output_dir}nn_scaler.pkl')\n",
    "    \n",
    "    # 保存特徵名稱和編碼器\n",
    "    joblib.dump(feature_names, f'{output_dir}feature_names.pkl')\n",
    "    joblib.dump(label_encoders, f'{output_dir}label_encoders.pkl')\n",
    "    \n",
    "    # 保存結果\n",
    "    import json\n",
    "    with open(f'{output_dir}results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    # 保存訓練信息 - 修正：將numpy類型轉換為Python原生類型\n",
    "    train_info = {\n",
    "        'train_size': int(len(X_train)),  # 轉換為int\n",
    "        'test_size': int(len(X_test)),    # 轉換為int\n",
    "        'n_features': int(len(feature_names)),  # 轉換為int\n",
    "        'class_distribution': {int(k): int(v) for k, v in zip(*np.unique(y, return_counts=True))},  # 轉換鍵值\n",
    "        'best_model': best_model_name,\n",
    "        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    with open(f'{output_dir}train_info.json', 'w') as f:\n",
    "        json.dump(train_info, f, indent=4)\n",
    "    \n",
    "    print(f\"\\n✅ 所有模型和結果已保存至: {output_dir}\")\n",
    "\n",
    "# 保存\n",
    "save_models_and_results(\n",
    "    {'nn_model': nn_model, 'nn_scaler': nn_scaler},\n",
    "    results,\n",
    "    feature_names,\n",
    "    label_encoders\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b70c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 17: 準備時空預測數據\n",
    "# ===========================\n",
    "\n",
    "# 首先，我們需要重新載入包含地理位置的原始數據\n",
    "print(\"重新載入地理數據...\")\n",
    "\n",
    "# 載入需要的欄位\n",
    "geo_cols = ['Start_Lat', 'Start_Lng', 'Start_Time', 'Severity', 'State', 'City']\n",
    "df_geo = pd.read_csv(file_path, usecols=geo_cols, nrows=1000000)  # 先用100萬筆測試\n",
    "\n",
    "# 處理時間\n",
    "df_geo['Start_Time'] = pd.to_datetime(df_geo['Start_Time'])\n",
    "df_geo = df_geo.dropna(subset=['Start_Lat', 'Start_Lng'])\n",
    "\n",
    "print(f\"地理數據大小: {df_geo.shape}\")\n",
    "print(f\"數據範圍: Lat [{df_geo['Start_Lat'].min():.2f}, {df_geo['Start_Lat'].max():.2f}], \"\n",
    "      f\"Lng [{df_geo['Start_Lng'].min():.2f}, {df_geo['Start_Lng'].max():.2f}]\")\n",
    "\n",
    "# ===========================\n",
    "# Cell 18: 創建網格化地圖數據\n",
    "# ===========================\n",
    "\n",
    "def create_grid_statistics(df_geo):\n",
    "    \"\"\"創建網格化的事故統計\"\"\"\n",
    "    print(\"創建網格統計...\")\n",
    "    \n",
    "    # 提取時間特徵\n",
    "    df_geo['Hour'] = df_geo['Start_Time'].dt.hour\n",
    "    df_geo['DayOfWeek'] = df_geo['Start_Time'].dt.dayofweek\n",
    "    df_geo['Month'] = df_geo['Start_Time'].dt.month\n",
    "    \n",
    "    # 創建地理網格（0.5度 x 0.5度）\n",
    "    df_geo['lat_grid'] = (df_geo['Start_Lat'] // 0.5) * 0.5\n",
    "    df_geo['lng_grid'] = (df_geo['Start_Lng'] // 0.5) * 0.5\n",
    "    \n",
    "    # 統計每個網格的事故\n",
    "    grid_stats = df_geo.groupby(['lat_grid', 'lng_grid']).agg({\n",
    "        'Severity': ['count', 'mean'],\n",
    "        'Hour': lambda x: x.mode()[0] if len(x) > 0 else 12,\n",
    "        'DayOfWeek': lambda x: x.mode()[0] if len(x) > 0 else 1\n",
    "    }).reset_index()\n",
    "    \n",
    "    # 簡化列名\n",
    "    grid_stats.columns = ['lat', 'lng', 'accident_count', 'avg_severity', 'common_hour', 'common_day']\n",
    "    \n",
    "    # 只保留有足夠事故的網格\n",
    "    grid_stats = grid_stats[grid_stats['accident_count'] >= 10]\n",
    "    \n",
    "    print(f\"網格數量: {len(grid_stats)}\")\n",
    "    \n",
    "    return grid_stats, df_geo\n",
    "\n",
    "grid_stats, df_geo_processed = create_grid_statistics(df_geo)\n",
    "\n",
    "# ===========================\n",
    "# Cell 19: 創建互動式地圖（使用Plotly）\n",
    "# ===========================\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "def create_interactive_map(grid_stats, selected_hour=None, selected_day=None):\n",
    "    \"\"\"創建互動式事故熱力圖\"\"\"\n",
    "    \n",
    "    # 篩選數據\n",
    "    data = grid_stats.copy()\n",
    "    if selected_hour is not None:\n",
    "        # 篩選相似時間的數據\n",
    "        data = data[np.abs(data['common_hour'] - selected_hour) <= 3]\n",
    "    \n",
    "    # 創建地圖\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # 添加熱力圖層\n",
    "    fig.add_trace(go.Scattermapbox(\n",
    "        lat=data['lat'],\n",
    "        lon=data['lng'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=np.log1p(data['accident_count']) * 3,  # 對數縮放\n",
    "            color=data['avg_severity'],\n",
    "            colorscale='Reds',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"平均嚴重度\"),\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        text=[f\"位置: ({lat:.2f}, {lng:.2f})<br>\"\n",
    "              f\"事故數: {count}<br>\"\n",
    "              f\"平均嚴重度: {sev:.2f}\"\n",
    "              for lat, lng, count, sev in zip(\n",
    "                  data['lat'], data['lng'], \n",
    "                  data['accident_count'], data['avg_severity'])],\n",
    "        hovertemplate='%{text}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    # 設置地圖樣式\n",
    "    fig.update_layout(\n",
    "        mapbox=dict(\n",
    "            style=\"open-street-map\",\n",
    "            center=dict(lat=39.8283, lon=-98.5795),  # 美國中心\n",
    "            zoom=3\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        height=600,\n",
    "        title=f\"美國交通事故熱力圖\" + \n",
    "              (f\" - {selected_hour}:00\" if selected_hour is not None else \"\")\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# 創建基礎地圖\n",
    "base_map = create_interactive_map(grid_stats)\n",
    "base_map.show()\n",
    "\n",
    "# ===========================\n",
    "# Cell 20: 時間動態分析\n",
    "# ===========================\n",
    "\n",
    "# 創建按小時的事故分布\n",
    "hourly_stats = df_geo_processed.groupby('Hour').agg({\n",
    "    'Severity': ['count', 'mean']\n",
    "}).reset_index()\n",
    "hourly_stats.columns = ['Hour', 'Count', 'Avg_Severity']\n",
    "\n",
    "# 繪製時間分布圖\n",
    "fig_time = go.Figure()\n",
    "\n",
    "# 事故數量\n",
    "fig_time.add_trace(go.Bar(\n",
    "    x=hourly_stats['Hour'],\n",
    "    y=hourly_stats['Count'],\n",
    "    name='事故數量',\n",
    "    yaxis='y'\n",
    "))\n",
    "\n",
    "# 平均嚴重度\n",
    "fig_time.add_trace(go.Scatter(\n",
    "    x=hourly_stats['Hour'],\n",
    "    y=hourly_stats['Avg_Severity'],\n",
    "    name='平均嚴重度',\n",
    "    yaxis='y2',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "fig_time.update_layout(\n",
    "    title='24小時事故分布',\n",
    "    xaxis=dict(title='小時'),\n",
    "    yaxis=dict(title='事故數量', side='left'),\n",
    "    yaxis2=dict(title='平均嚴重度', side='right', overlaying='y'),\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig_time.show()\n",
    "\n",
    "# ===========================\n",
    "# Cell 21: 預測函數\n",
    "# ===========================\n",
    "\n",
    "def predict_accident_risk(lat, lng, hour, day_of_week, model, scaler, feature_template):\n",
    "    \"\"\"預測特定位置和時間的事故風險\"\"\"\n",
    "    \n",
    "    # 創建特徵向量（需要匹配訓練時的特徵）\n",
    "    # 這裡簡化處理，實際需要完整的特徵工程\n",
    "    features = np.zeros(len(feature_template))\n",
    "    \n",
    "    # 填入基本特徵\n",
    "    features[0] = hour\n",
    "    features[1] = day_of_week\n",
    "    # ... 其他特徵\n",
    "    \n",
    "    # 標準化\n",
    "    features_scaled = scaler.transform(features.reshape(1, -1))\n",
    "    \n",
    "    # 預測\n",
    "    prediction = model.predict(features_scaled)[0]\n",
    "    probability = model.predict_proba(features_scaled)[0]\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# ===========================\n",
    "# Cell 22: 高風險區域識別\n",
    "# ===========================\n",
    "\n",
    "def identify_high_risk_areas(grid_stats, threshold_percentile=90):\n",
    "    \"\"\"識別高風險區域\"\"\"\n",
    "    \n",
    "    # 計算風險分數（結合事故數量和嚴重度）\n",
    "    grid_stats['risk_score'] = (\n",
    "        grid_stats['accident_count'] * 0.3 + \n",
    "        grid_stats['avg_severity'] * 100 * 0.7\n",
    "    )\n",
    "    \n",
    "    # 找出高風險區域\n",
    "    threshold = np.percentile(grid_stats['risk_score'], threshold_percentile)\n",
    "    high_risk = grid_stats[grid_stats['risk_score'] >= threshold].copy()\n",
    "    \n",
    "    # 排序\n",
    "    high_risk = high_risk.sort_values('risk_score', ascending=False)\n",
    "    \n",
    "    print(f\"識別出 {len(high_risk)} 個高風險區域\")\n",
    "    print(\"\\nTop 10 高風險區域:\")\n",
    "    for idx, row in high_risk.head(10).iterrows():\n",
    "        print(f\"  ({row['lat']:.2f}, {row['lng']:.2f}) - \"\n",
    "              f\"風險分數: {row['risk_score']:.2f}, \"\n",
    "              f\"事故數: {row['accident_count']}\")\n",
    "    \n",
    "    return high_risk\n",
    "\n",
    "high_risk_areas = identify_high_risk_areas(grid_stats)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
