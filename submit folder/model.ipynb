{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63100b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 1: å°å…¥å¥—ä»¶\n",
    "# ===========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ç¢ºä¿å·²å®‰è£æ‰€éœ€å¥—ä»¶\n",
    "required_packages = {\n",
    "    'pandas': pd,\n",
    "    'numpy': np,\n",
    "    'matplotlib': matplotlib,\n",
    "    'seaborn': sns,\n",
    "}\n",
    "\n",
    "print(\"æª¢æŸ¥å¥—ä»¶å®‰è£ç‹€æ…‹...\")\n",
    "for package_name, package in required_packages.items():\n",
    "    try:\n",
    "        print(f\"âœ… {package_name} ç‰ˆæœ¬: {package.__version__}\")\n",
    "    except:\n",
    "        print(f\"âŒ {package_name} æœªå®‰è£\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc2a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 2: å°å…¥æ©Ÿå™¨å­¸ç¿’ç›¸é—œå¥—ä»¶\n",
    "# ===========================\n",
    "try:\n",
    "    # Scikit-learn\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "    print(\"âœ… Scikit-learn å¥—ä»¶è¼‰å…¥æˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Scikit-learn è¼‰å…¥å¤±æ•—: {e}\")\n",
    "\n",
    "try:\n",
    "    # è™•ç†ä¸å¹³è¡¡è³‡æ–™\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from imblearn.combine import SMOTEENN\n",
    "    print(\"âœ… Imbalanced-learn å¥—ä»¶è¼‰å…¥æˆåŠŸ\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Imbalanced-learn æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install imbalanced-learn\")\n",
    "\n",
    "try:\n",
    "    # GPUåŠ é€Ÿçš„æ¨¡å‹\n",
    "    import xgboost as xgb\n",
    "    print(f\"âœ… XGBoost ç‰ˆæœ¬: {xgb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ XGBoost æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    print(f\"âœ… LightGBM ç‰ˆæœ¬: {lgb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ LightGBM æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install lightgbm\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    print(\"âœ… CatBoost è¼‰å…¥æˆåŠŸ\")\n",
    "except ImportError:\n",
    "    print(\"âŒ CatBoost æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install catboost\")\n",
    "\n",
    "try:\n",
    "    # PyTorch\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    print(f\"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "    print(f\"   CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ PyTorch æœªå®‰è£ï¼Œè«‹åƒè€ƒ https://pytorch.org å®‰è£\")\n",
    "\n",
    "# å…¶ä»–å·¥å…·\n",
    "import joblib\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735a0244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 3: è¼‰å…¥è³‡æ–™\n",
    "# ===========================\n",
    "def load_data(file_path):\n",
    "    \"\"\"è¼‰å…¥è³‡æ–™ä¸¦é¡¯ç¤ºåŸºæœ¬è³‡è¨Š\"\"\"\n",
    "    print(f\"æ­£åœ¨è¼‰å…¥è³‡æ–™: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # å…ˆæª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æª”æ¡ˆ {file_path}\")\n",
    "            return None\n",
    "            \n",
    "        # è¼‰å…¥è³‡æ–™\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        print(f\"âœ… è³‡æ–™è¼‰å…¥æˆåŠŸ!\")\n",
    "        print(f\"   è³‡æ–™é›†å¤§å°: {df.shape}\")\n",
    "        print(f\"   è¨˜æ†¶é«”ä½¿ç”¨: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # é¡¯ç¤ºå‰å¹¾ç­†è³‡æ–™\n",
    "        print(\"\\nå‰5ç­†è³‡æ–™:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # é¡¯ç¤ºæ¬„ä½è³‡è¨Š\n",
    "        print(f\"\\næ¬„ä½ç¸½æ•¸: {len(df.columns)}\")\n",
    "        print(\"æ¬„ä½åˆ—è¡¨:\")\n",
    "        for i, col in enumerate(df.columns):\n",
    "            if i < 10:  # åªé¡¯ç¤ºå‰10å€‹\n",
    "                print(f\"   {i+1}. {col}\")\n",
    "        print(\"   ...\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¼‰å…¥è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        return None\n",
    "\n",
    "# åŸ·è¡Œè¼‰å…¥ (è«‹ä¿®æ”¹ç‚ºä½ çš„æª”æ¡ˆè·¯å¾‘)\n",
    "file_path = 'us-accidents/US_Accidents_March23.csv'\n",
    "df = load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba9284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 4: è³‡æ–™åŸºæœ¬æ¢ç´¢\n",
    "# ===========================\n",
    "if df is not None:\n",
    "    print(\"=== è³‡æ–™åŸºæœ¬çµ±è¨ˆ ===\")\n",
    "    print(f\"\\nç›®æ¨™è®Šæ•¸ (Severity) åˆ†å¸ƒ:\")\n",
    "    print(df['Severity'].value_counts().sort_index())\n",
    "    \n",
    "    # æª¢æŸ¥è³‡æ–™å‹æ…‹\n",
    "    print(\"\\nè³‡æ–™å‹æ…‹çµ±è¨ˆ:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    # æ•¸å€¼å‹æ¬„ä½çµ±è¨ˆ\n",
    "    print(\"\\næ•¸å€¼å‹æ¬„ä½æè¿°çµ±è¨ˆ:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print(f\"æ•¸å€¼å‹æ¬„ä½æ•¸é‡: {len(numeric_cols)}\")\n",
    "    \n",
    "    # æª¢æŸ¥ç¼ºå¤±å€¼\n",
    "    print(\"\\nç¼ºå¤±å€¼æ¦‚è¦½:\")\n",
    "    missing_summary = df.isnull().sum()\n",
    "    missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "    print(f\"æœ‰ç¼ºå¤±å€¼çš„æ¬„ä½æ•¸: {len(missing_summary)}\")\n",
    "    print(missing_summary.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edd9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 5: æ—¥æœŸæ™‚é–“è™•ç†å‡½æ•¸\n",
    "# ===========================\n",
    "def process_datetime_columns(df):\n",
    "    \"\"\"è™•ç†æ—¥æœŸæ™‚é–“æ¬„ä½\"\"\"\n",
    "    if df is None:\n",
    "        print(\"âŒ è³‡æ–™æ¡†ç‚ºç©º\")\n",
    "        return None\n",
    "        \n",
    "    print(\"\\n=== è™•ç†æ—¥æœŸæ™‚é–“æ¬„ä½ ===\")\n",
    "    df_copy = df.copy()  # é¿å…ä¿®æ”¹åŸå§‹è³‡æ–™\n",
    "    \n",
    "    # æ—¥æœŸæ™‚é–“æ¬„ä½\n",
    "    date_columns = ['Start_Time', 'End_Time', 'Weather_Timestamp']\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if col in df_copy.columns:\n",
    "            print(f\"\\nè™•ç† {col}...\")\n",
    "            \n",
    "            # é¡¯ç¤ºåŸå§‹è³‡æ–™ç¯„ä¾‹\n",
    "            print(f\"åŸå§‹è³‡æ–™ç¯„ä¾‹: {df_copy[col].iloc[0]}\")\n",
    "            \n",
    "            # è½‰æ›ç‚ºæ—¥æœŸæ™‚é–“\n",
    "            df_copy[col] = pd.to_datetime(df_copy[col], errors='coerce')\n",
    "            \n",
    "            # æª¢æŸ¥è½‰æ›çµæœ\n",
    "            null_count = df_copy[col].isnull().sum()\n",
    "            if null_count > 0:\n",
    "                print(f\"âš ï¸  è­¦å‘Š: {col} æœ‰ {null_count} ç­†ç„¡æ³•è§£æçš„æ—¥æœŸ\")\n",
    "            else:\n",
    "                print(f\"âœ… {col} è½‰æ›æˆåŠŸ\")\n",
    "                print(f\"   æ—¥æœŸç¯„åœ: {df_copy[col].min()} åˆ° {df_copy[col].max()}\")\n",
    "    \n",
    "    # è¨ˆç®—æŒçºŒæ™‚é–“\n",
    "    if 'Start_Time' in df_copy.columns and 'End_Time' in df_copy.columns:\n",
    "        print(\"\\nè¨ˆç®—äº‹æ•…æŒçºŒæ™‚é–“...\")\n",
    "        df_copy['Duration_minutes'] = (df_copy['End_Time'] - df_copy['Start_Time']).dt.total_seconds() / 60\n",
    "        \n",
    "        # é¡¯ç¤ºçµ±è¨ˆ\n",
    "        print(f\"æŒçºŒæ™‚é–“çµ±è¨ˆ:\")\n",
    "        print(f\"  æœ€å°å€¼: {df_copy['Duration_minutes'].min():.2f} åˆ†é˜\")\n",
    "        print(f\"  æœ€å¤§å€¼: {df_copy['Duration_minutes'].max():.2f} åˆ†é˜\") \n",
    "        print(f\"  å¹³å‡å€¼: {df_copy['Duration_minutes'].mean():.2f} åˆ†é˜\")\n",
    "        print(f\"  ä¸­ä½æ•¸: {df_copy['Duration_minutes'].median():.2f} åˆ†é˜\")\n",
    "        \n",
    "        # éæ¿¾ç•°å¸¸å€¼\n",
    "        original_len = len(df_copy)\n",
    "        df_copy = df_copy[(df_copy['Duration_minutes'] > 0) & (df_copy['Duration_minutes'] < 1440)]\n",
    "        filtered_len = len(df_copy)\n",
    "        print(f\"\\néæ¿¾ç•°å¸¸æŒçºŒæ™‚é–“: {original_len} â†’ {filtered_len} (ç§»é™¤ {original_len - filtered_len} ç­†)\")\n",
    "    \n",
    "    print(f\"\\nè™•ç†å¾Œè³‡æ–™å¤§å°: {df_copy.shape}\")\n",
    "    return df_copy\n",
    "\n",
    "# åŸ·è¡Œæ—¥æœŸè™•ç†\n",
    "df_processed = process_datetime_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 6: ç‰¹å¾µå·¥ç¨‹\n",
    "# ===========================\n",
    "def feature_engineering(df):\n",
    "    \"\"\"å‰µå»ºæ–°ç‰¹å¾µ\"\"\"\n",
    "    if df is None:\n",
    "        print(\"âŒ è³‡æ–™æ¡†ç‚ºç©º\")\n",
    "        return None\n",
    "        \n",
    "    print(\"\\n=== åŸ·è¡Œç‰¹å¾µå·¥ç¨‹ ===\")\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # æ™‚é–“ç›¸é—œç‰¹å¾µ\n",
    "    if 'Start_Time' in df_copy.columns:\n",
    "        # ç¢ºä¿Start_Timeä¸ç‚ºnull\n",
    "        df_copy = df_copy[df_copy['Start_Time'].notna()]\n",
    "        print(f\"ç§»é™¤Start_Timeç‚ºnullçš„è¨˜éŒ„å¾Œ: {len(df_copy)} ç­†\")\n",
    "        \n",
    "        # æå–æ™‚é–“ç‰¹å¾µ\n",
    "        print(\"å‰µå»ºæ™‚é–“ç‰¹å¾µ...\")\n",
    "        df_copy['Hour'] = df_copy['Start_Time'].dt.hour\n",
    "        df_copy['DayOfWeek'] = df_copy['Start_Time'].dt.dayofweek\n",
    "        df_copy['Month'] = df_copy['Start_Time'].dt.month\n",
    "        df_copy['Year'] = df_copy['Start_Time'].dt.year\n",
    "        df_copy['IsWeekend'] = (df_copy['DayOfWeek'] >= 5).astype(int)\n",
    "        \n",
    "        # æ™‚æ®µåˆ†é¡\n",
    "        df_copy['TimeOfDay'] = pd.cut(df_copy['Hour'], \n",
    "                                      bins=[-1, 6, 12, 18, 24], \n",
    "                                      labels=['Night', 'Morning', 'Afternoon', 'Evening'])\n",
    "        \n",
    "        # å­£ç¯€\n",
    "        df_copy['Season'] = pd.cut(df_copy['Month'], \n",
    "                                   bins=[0, 3, 6, 9, 12], \n",
    "                                   labels=['Winter', 'Spring', 'Summer', 'Fall'])\n",
    "        \n",
    "        print(\"âœ… æ™‚é–“ç‰¹å¾µå‰µå»ºå®Œæˆ\")\n",
    "    \n",
    "    # å¤©æ°£æ¢ä»¶ç°¡åŒ–\n",
    "    if 'Weather_Condition' in df_copy.columns:\n",
    "        print(\"\\nè™•ç†å¤©æ°£æ¢ä»¶...\")\n",
    "        \n",
    "        weather_keywords = {\n",
    "            'Clear': ['Clear', 'Fair'],\n",
    "            'Cloudy': ['Cloud', 'Overcast'],\n",
    "            'Rain': ['Rain', 'Drizzle', 'Shower'],\n",
    "            'Snow': ['Snow', 'Sleet', 'Hail'],\n",
    "            'Fog': ['Fog', 'Mist'],\n",
    "            'Storm': ['Storm', 'Thunder']\n",
    "        }\n",
    "        \n",
    "        def categorize_weather(condition):\n",
    "            if pd.isna(condition):\n",
    "                return 'Unknown'\n",
    "            condition = str(condition)\n",
    "            for category, keywords in weather_keywords.items():\n",
    "                if any(keyword in condition for keyword in keywords):\n",
    "                    return category\n",
    "            return 'Other'\n",
    "        \n",
    "        df_copy['Weather_Category'] = df_copy['Weather_Condition'].apply(categorize_weather)\n",
    "        \n",
    "        # é¡¯ç¤ºå¤©æ°£åˆ†é¡çµæœ\n",
    "        print(\"å¤©æ°£é¡åˆ¥åˆ†å¸ƒ:\")\n",
    "        print(df_copy['Weather_Category'].value_counts())\n",
    "    \n",
    "    print(f\"\\nç‰¹å¾µå·¥ç¨‹å®Œæˆï¼Œè³‡æ–™å¤§å°: {df_copy.shape}\")\n",
    "    return df_copy\n",
    "\n",
    "# åŸ·è¡Œç‰¹å¾µå·¥ç¨‹\n",
    "df_featured = feature_engineering(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c832aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 7: ç¼ºå¤±å€¼è™•ç†\n",
    "# ===========================\n",
    "def handle_missing_values(df, missing_threshold=60):\n",
    "    \"\"\"è™•ç†ç¼ºå¤±å€¼\"\"\"\n",
    "    if df is None:\n",
    "        print(\"âŒ è³‡æ–™æ¡†ç‚ºç©º\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"\\n=== è™•ç†ç¼ºå¤±å€¼ (é–¾å€¼: {missing_threshold}%) ===\")\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # è¨ˆç®—ç¼ºå¤±å€¼ç™¾åˆ†æ¯”\n",
    "    missing_percentage = (df_copy.isnull().sum() / len(df_copy)) * 100\n",
    "    \n",
    "    # æ‰¾å‡ºé«˜ç¼ºå¤±ç‡æ¬„ä½\n",
    "    high_missing_cols = missing_percentage[missing_percentage > missing_threshold].index.tolist()\n",
    "    \n",
    "    if len(high_missing_cols) > 0:\n",
    "        print(f\"\\nåˆªé™¤é«˜ç¼ºå¤±ç‡æ¬„ä½ ({len(high_missing_cols)} å€‹):\")\n",
    "        for col in high_missing_cols[:10]:  # é¡¯ç¤ºå‰10å€‹\n",
    "            print(f\"  - {col}: {missing_percentage[col]:.1f}%\")\n",
    "        if len(high_missing_cols) > 10:\n",
    "            print(f\"  ... é‚„æœ‰ {len(high_missing_cols) - 10} å€‹æ¬„ä½\")\n",
    "        \n",
    "        df_copy = df_copy.drop(columns=high_missing_cols)\n",
    "    \n",
    "    # å¡«è£œå‰©é¤˜ç¼ºå¤±å€¼\n",
    "    print(\"\\nå¡«è£œå‰©é¤˜ç¼ºå¤±å€¼...\")\n",
    "    \n",
    "    # æ•¸å€¼å‹æ¬„ä½ç”¨ä¸­ä½æ•¸å¡«è£œ\n",
    "    numeric_cols = df_copy.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_copy[col].isnull().sum() > 0:\n",
    "            median_value = df_copy[col].median()\n",
    "            df_copy[col].fillna(median_value, inplace=True)\n",
    "            print(f\"  {col}: ç”¨ä¸­ä½æ•¸ {median_value:.2f} å¡«è£œ\")\n",
    "    \n",
    "    # é¡åˆ¥å‹æ¬„ä½ç”¨çœ¾æ•¸å¡«è£œ\n",
    "    categorical_cols = df_copy.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if df_copy[col].isnull().sum() > 0:\n",
    "            mode_value = df_copy[col].mode()[0] if len(df_copy[col].mode()) > 0 else 'Unknown'\n",
    "            df_copy[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"  {col}: ç”¨çœ¾æ•¸ '{mode_value}' å¡«è£œ\")\n",
    "    \n",
    "    print(f\"\\nè™•ç†å¾Œè³‡æ–™å¤§å°: {df_copy.shape}\")\n",
    "    print(f\"å‰©é¤˜ç¼ºå¤±å€¼ç¸½æ•¸: {df_copy.isnull().sum().sum()}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# åŸ·è¡Œç¼ºå¤±å€¼è™•ç†\n",
    "df_cleaned = handle_missing_values(df_featured, missing_threshold=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e2c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 8: ç‰¹å¾µé¸æ“‡èˆ‡ç·¨ç¢¼\n",
    "# ===========================\n",
    "def select_and_encode_features(df):\n",
    "    \"\"\"é¸æ“‡ç‰¹å¾µä¸¦é€²è¡Œç·¨ç¢¼\"\"\"\n",
    "    if df is None:\n",
    "        print(\"âŒ è³‡æ–™æ¡†ç‚ºç©º\")\n",
    "        return None, None, None\n",
    "        \n",
    "    print(\"\\n=== ç‰¹å¾µé¸æ“‡èˆ‡ç·¨ç¢¼ ===\")\n",
    "    \n",
    "    # å®šç¾©ç‰¹å¾µé¡å‹\n",
    "    numeric_features = [\n",
    "        'Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)',\n",
    "        'Wind_Speed(mph)', 'Distance(mi)', 'Hour', 'DayOfWeek', 'Month', \n",
    "        'Year', 'IsWeekend'\n",
    "    ]\n",
    "    \n",
    "    categorical_features = [\n",
    "        'Side', 'State', 'Weather_Category', 'TimeOfDay', 'Season',\n",
    "        'Sunrise_Sunset'\n",
    "    ]\n",
    "    \n",
    "    boolean_features = [\n",
    "        'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit',\n",
    "        'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', \n",
    "        'Traffic_Signal'\n",
    "    ]\n",
    "    \n",
    "    # åŠ å…¥Duration_minuteså¦‚æœå­˜åœ¨\n",
    "    if 'Duration_minutes' in df.columns:\n",
    "        numeric_features.append('Duration_minutes')\n",
    "    \n",
    "    # æª¢æŸ¥ä¸¦éæ¿¾å­˜åœ¨çš„ç‰¹å¾µ\n",
    "    print(\"\\næª¢æŸ¥ç‰¹å¾µå­˜åœ¨æ€§...\")\n",
    "    numeric_features = [f for f in numeric_features if f in df.columns]\n",
    "    categorical_features = [f for f in categorical_features if f in df.columns]\n",
    "    boolean_features = [f for f in boolean_features if f in df.columns]\n",
    "    \n",
    "    print(f\"æ•¸å€¼å‹ç‰¹å¾µ: {len(numeric_features)} å€‹\")\n",
    "    print(f\"é¡åˆ¥å‹ç‰¹å¾µ: {len(categorical_features)} å€‹\")\n",
    "    print(f\"å¸ƒæ—å‹ç‰¹å¾µ: {len(boolean_features)} å€‹\")\n",
    "    \n",
    "    # è¤‡è£½è³‡æ–™\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # è™•ç†å¸ƒæ—ç‰¹å¾µ\n",
    "    print(\"\\nè™•ç†å¸ƒæ—ç‰¹å¾µ...\")\n",
    "    for col in boolean_features:\n",
    "        df_encoded[col] = df_encoded[col].astype(int)\n",
    "    \n",
    "    # ç·¨ç¢¼é¡åˆ¥ç‰¹å¾µ\n",
    "    print(\"\\nç·¨ç¢¼é¡åˆ¥ç‰¹å¾µ...\")\n",
    "    label_encoders = {}\n",
    "    encoded_features = numeric_features + boolean_features\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        print(f\"  ç·¨ç¢¼ {col}...\")\n",
    "        le = LabelEncoder()\n",
    "        encoded_col_name = col + '_encoded'\n",
    "        \n",
    "        # è™•ç†å¯èƒ½çš„nullå€¼\n",
    "        df_encoded[col] = df_encoded[col].fillna('Unknown').astype(str)\n",
    "        df_encoded[encoded_col_name] = le.fit_transform(df_encoded[col])\n",
    "        \n",
    "        label_encoders[col] = le\n",
    "        encoded_features.append(encoded_col_name)\n",
    "        \n",
    "        print(f\"    é¡åˆ¥æ•¸: {len(le.classes_)}\")\n",
    "    \n",
    "    print(f\"\\nç¸½ç‰¹å¾µæ•¸: {len(encoded_features)}\")\n",
    "    \n",
    "    return df_encoded, encoded_features, label_encoders\n",
    "\n",
    "# åŸ·è¡Œç‰¹å¾µç·¨ç¢¼\n",
    "df_encoded, selected_features, label_encoders = select_and_encode_features(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd271be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 9: æº–å‚™è¨“ç·´è³‡æ–™\n",
    "# ===========================\n",
    "def prepare_training_data(df, features, target_col='Severity'):\n",
    "    \"\"\"æº–å‚™æœ€çµ‚çš„è¨“ç·´è³‡æ–™\"\"\"\n",
    "    if df is None or features is None:\n",
    "        print(\"âŒ è³‡æ–™æˆ–ç‰¹å¾µç‚ºç©º\")\n",
    "        return None, None\n",
    "        \n",
    "    print(\"\\n=== æº–å‚™è¨“ç·´è³‡æ–™ ===\")\n",
    "    \n",
    "    # æª¢æŸ¥ç›®æ¨™è®Šæ•¸\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"âŒ æ‰¾ä¸åˆ°ç›®æ¨™è®Šæ•¸ '{target_col}'\")\n",
    "        return None, None\n",
    "    \n",
    "    # æª¢æŸ¥æ‰€æœ‰ç‰¹å¾µæ˜¯å¦å­˜åœ¨\n",
    "    missing_features = [f for f in features if f not in df.columns]\n",
    "    if missing_features:\n",
    "        print(f\"âŒ ç¼ºå°‘ä»¥ä¸‹ç‰¹å¾µ: {missing_features}\")\n",
    "        return None, None\n",
    "    \n",
    "    # é¸æ“‡éœ€è¦çš„æ¬„ä½\n",
    "    required_columns = features + [target_col]\n",
    "    df_subset = df[required_columns].copy()\n",
    "    \n",
    "    # ç§»é™¤å«æœ‰ç¼ºå¤±å€¼çš„è¡Œ\n",
    "    print(f\"è™•ç†å‰è³‡æ–™å¤§å°: {df_subset.shape}\")\n",
    "    df_clean = df_subset.dropna()\n",
    "    print(f\"ç§»é™¤ç¼ºå¤±å€¼å¾Œå¤§å°: {df_clean.shape}\")\n",
    "    \n",
    "    if len(df_clean) == 0:\n",
    "        print(\"âŒ éŒ¯èª¤: ç§»é™¤ç¼ºå¤±å€¼å¾Œæ²’æœ‰å‰©é¤˜è³‡æ–™!\")\n",
    "        return None, None\n",
    "    \n",
    "    # æº–å‚™Xå’Œy\n",
    "    X = df_clean[features]\n",
    "    y = df_clean[target_col]\n",
    "    \n",
    "    # å°‡åš´é‡åº¦å¾1-4è½‰æ›ç‚º0-3 (sklearnè¦æ±‚)\n",
    "    y = y - 1\n",
    "    \n",
    "    # é¡¯ç¤ºç›®æ¨™è®Šæ•¸åˆ†å¸ƒ\n",
    "    print(\"\\nç›®æ¨™è®Šæ•¸åˆ†å¸ƒ:\")\n",
    "    value_counts = pd.Series(y).value_counts().sort_index()\n",
    "    for severity, count in value_counts.items():\n",
    "        print(f\"  åš´é‡åº¦ {severity+1}: {count} ({count/len(y)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\næœ€çµ‚è¨“ç·´è³‡æ–™å¤§å°: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# æº–å‚™è¨“ç·´è³‡æ–™\n",
    "X, y = prepare_training_data(df_encoded, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe0b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 10: åˆ†å‰²è³‡æ–™é›†\n",
    "# ===========================\n",
    "if X is not None and y is not None:\n",
    "    print(\"\\n=== åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›† ===\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"è¨“ç·´é›†å¤§å°: {X_train.shape}\")\n",
    "    print(f\"æ¸¬è©¦é›†å¤§å°: {X_test.shape}\")\n",
    "    \n",
    "    # æª¢æŸ¥åˆ†å‰²å¾Œçš„é¡åˆ¥åˆ†å¸ƒ\n",
    "    print(\"\\nè¨“ç·´é›†é¡åˆ¥åˆ†å¸ƒ:\")\n",
    "    train_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "    for severity, count in train_dist.items():\n",
    "        print(f\"  åš´é‡åº¦ {severity+1}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\næ¸¬è©¦é›†é¡åˆ¥åˆ†å¸ƒ:\")\n",
    "    test_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "    for severity, count in test_dist.items():\n",
    "        print(f\"  åš´é‡åº¦ {severity+1}: {count} ({count/len(y_test)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"âŒ ç„¡æ³•åˆ†å‰²è³‡æ–™é›†ï¼Œè«‹æª¢æŸ¥å‰é¢çš„æ­¥é©Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 11: å®šç¾©æ¨¡å‹è¨“ç·´å‡½æ•¸\n",
    "# ===========================\n",
    "def train_random_forest(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"è¨“ç·´éš¨æ©Ÿæ£®æ—æ¨¡å‹\"\"\"\n",
    "    print(\"\\nè¨“ç·´ Random Forest...\")\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # é æ¸¬\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # è©•ä¼°\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"è¨“ç·´æ™‚é–“: {train_time:.2f} ç§’\")\n",
    "    print(f\"æº–ç¢ºç‡: {accuracy:.4f}\")\n",
    "    print(f\"F1åˆ†æ•¸: {f1:.4f}\")\n",
    "    \n",
    "    # é¡¯ç¤ºåˆ†é¡å ±å‘Š\n",
    "    print(\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                              target_names=[f'Severity {i+1}' for i in range(4)]))\n",
    "    \n",
    "    return model, accuracy, f1\n",
    "\n",
    "def train_xgboost(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"è¨“ç·´XGBoostæ¨¡å‹\"\"\"\n",
    "    print(\"\\nè¨“ç·´ XGBoost...\")\n",
    "    \n",
    "    # æª¢æŸ¥æ˜¯å¦æœ‰GPU\n",
    "    use_gpu = torch.cuda.is_available() if 'torch' in globals() else False\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        objective='multi:softprob',\n",
    "        n_jobs=-1,\n",
    "        tree_method='gpu_hist' if use_gpu else 'hist',\n",
    "        predictor='gpu_predictor' if use_gpu else 'cpu_predictor',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # é æ¸¬\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # è©•ä¼°\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"ä½¿ç”¨GPU: {use_gpu}\")\n",
    "    print(f\"è¨“ç·´æ™‚é–“: {train_time:.2f} ç§’\")\n",
    "    print(f\"æº–ç¢ºç‡: {accuracy:.4f}\")\n",
    "    print(f\"F1åˆ†æ•¸: {f1:.4f}\")\n",
    "    \n",
    "    return model, accuracy, f1\n",
    "\n",
    "def train_lightgbm(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"è¨“ç·´LightGBMæ¨¡å‹\"\"\"\n",
    "    print(\"\\nè¨“ç·´ LightGBM...\")\n",
    "    \n",
    "    # æª¢æŸ¥æ˜¯å¦æœ‰GPU\n",
    "    use_gpu = torch.cuda.is_available() if 'torch' in globals() else False\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 4,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'multi_logloss',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 100,\n",
    "        'device': 'gpu' if use_gpu else 'cpu',\n",
    "        'random_state': 42,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    if use_gpu:\n",
    "        params.update({\n",
    "            'gpu_platform_id': 0,\n",
    "            'gpu_device_id': 0\n",
    "        })\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # é æ¸¬\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # è©•ä¼°\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"ä½¿ç”¨GPU: {use_gpu}\")\n",
    "    print(f\"è¨“ç·´æ™‚é–“: {train_time:.2f} ç§’\")\n",
    "    print(f\"æº–ç¢ºç‡: {accuracy:.4f}\")\n",
    "    print(f\"F1åˆ†æ•¸: {f1:.4f}\")\n",
    "    \n",
    "    return model, accuracy, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 12: è¨“ç·´æ¨¡å‹\n",
    "# ===========================\n",
    "if 'X_train' in globals() and X_train is not None:\n",
    "    results = {}\n",
    "    models = {}\n",
    "    \n",
    "    # 1. Random Forest\n",
    "    try:\n",
    "        rf_model, rf_acc, rf_f1 = train_random_forest(X_train, X_test, y_train, y_test)\n",
    "        results['Random_Forest'] = {'accuracy': rf_acc, 'f1': rf_f1}\n",
    "        models['Random_Forest'] = rf_model\n",
    "    except Exception as e:\n",
    "        print(f\"Random Forest è¨“ç·´å¤±æ•—: {e}\")\n",
    "    \n",
    "    # 2. XGBoost\n",
    "    try:\n",
    "        if 'xgb' in globals():\n",
    "            xgb_model, xgb_acc, xgb_f1 = train_xgboost(X_train, X_test, y_train, y_test)\n",
    "            results['XGBoost'] = {'accuracy': xgb_acc, 'f1': xgb_f1}\n",
    "            models['XGBoost'] = xgb_model\n",
    "    except Exception as e:\n",
    "        print(f\"XGBoost è¨“ç·´å¤±æ•—: {e}\")\n",
    "    \n",
    "    # 3. LightGBM\n",
    "    try:\n",
    "        if 'lgb' in globals():\n",
    "            lgb_model, lgb_acc, lgb_f1 = train_lightgbm(X_train, X_test, y_train, y_test)\n",
    "            results['LightGBM'] = {'accuracy': lgb_acc, 'f1': lgb_f1}\n",
    "            models['LightGBM'] = lgb_model\n",
    "    except Exception as e:\n",
    "        print(f\"LightGBM è¨“ç·´å¤±æ•—: {e}\")\n",
    "else:\n",
    "    print(\"âŒ è«‹å…ˆåŸ·è¡Œå‰é¢çš„è³‡æ–™æº–å‚™æ­¥é©Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630fb8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 13: æ¨¡å‹æ¯”è¼ƒ\n",
    "# ===========================\n",
    "if 'results' in globals() and results:\n",
    "    print(\"\\n=== æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ ===\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'æ¨¡å‹':<15} {'æº–ç¢ºç‡':<10} {'F1åˆ†æ•¸':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"{model_name:<15} {metrics['accuracy']:<10.4f} {metrics['f1']:<10.4f}\")\n",
    "    \n",
    "    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹\n",
    "    best_model_name = max(results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "    print(f\"\\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}\")\n",
    "    print(f\"   æº–ç¢ºç‡: {results[best_model_name]['accuracy']:.4f}\")\n",
    "    print(f\"   F1åˆ†æ•¸: {results[best_model_name]['f1']:.4f}\")\n",
    "    \n",
    "    best_model = models[best_model_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daffed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 14: ç‰¹å¾µé‡è¦æ€§åˆ†æ\n",
    "# ===========================\n",
    "if 'best_model' in globals() and hasattr(best_model, 'feature_importances_'):\n",
    "    print(\"\\n=== ç‰¹å¾µé‡è¦æ€§åˆ†æ ===\")\n",
    "    \n",
    "    # å–å¾—ç‰¹å¾µé‡è¦æ€§\n",
    "    importances = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # é¡¯ç¤ºå‰20å€‹é‡è¦ç‰¹å¾µ\n",
    "    print(\"\\nTop 20 é‡è¦ç‰¹å¾µ:\")\n",
    "    print(importances.head(20))\n",
    "    \n",
    "    # è¦–è¦ºåŒ–\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_n = 20\n",
    "    plt.barh(importances.head(top_n)['feature'][::-1], \n",
    "             importances.head(top_n)['importance'][::-1])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top {top_n} Feature Importances ({best_model_name})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ab737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 15: å„²å­˜æ¨¡å‹\n",
    "# ===========================\n",
    "def save_model_and_components(model, model_name, label_encoders, feature_names, output_dir='./model_output/'):\n",
    "    \"\"\"å„²å­˜æ¨¡å‹å’Œç›¸é—œå…ƒä»¶\"\"\"\n",
    "    print(f\"\\n=== å„²å­˜æ¨¡å‹: {model_name} ===\")\n",
    "    \n",
    "    # å»ºç«‹è¼¸å‡ºç›®éŒ„\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # å„²å­˜æ¨¡å‹\n",
    "    model_path = os.path.join(output_dir, f'{model_name.lower()}_model.pkl')\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"âœ… æ¨¡å‹å·²å„²å­˜è‡³: {model_path}\")\n",
    "    \n",
    "    # å„²å­˜æ¨™ç±¤ç·¨ç¢¼å™¨\n",
    "    encoders_path = os.path.join(output_dir, 'label_encoders.pkl')\n",
    "    joblib.dump(label_encoders, encoders_path)\n",
    "    print(f\"âœ… æ¨™ç±¤ç·¨ç¢¼å™¨å·²å„²å­˜è‡³: {encoders_path}\")\n",
    "    \n",
    "    # å„²å­˜ç‰¹å¾µåç¨±\n",
    "    features_path = os.path.join(output_dir, 'feature_names.pkl')\n",
    "    joblib.dump(feature_names, features_path)\n",
    "    print(f\"âœ… ç‰¹å¾µåç¨±å·²å„²å­˜è‡³: {features_path}\")\n",
    "    \n",
    "    # å„²å­˜æ¨¡å‹è³‡è¨Š\n",
    "    model_info = {\n",
    "        'model_name': model_name,\n",
    "        'n_features': len(feature_names),\n",
    "        'feature_names': feature_names,\n",
    "        'accuracy': results[model_name]['accuracy'],\n",
    "        'f1_score': results[model_name]['f1'],\n",
    "        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    info_path = os.path.join(output_dir, 'model_info.json')\n",
    "    import json\n",
    "    with open(info_path, 'w') as f:\n",
    "        json.dump(model_info, f, indent=4)\n",
    "    print(f\"âœ… æ¨¡å‹è³‡è¨Šå·²å„²å­˜è‡³: {info_path}\")\n",
    "\n",
    "# å„²å­˜æœ€ä½³æ¨¡å‹\n",
    "if 'best_model' in globals() and 'best_model_name' in globals():\n",
    "    save_model_and_components(best_model, best_model_name, label_encoders, selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86cb565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 16: å®šç¾©é æ¸¬å‡½æ•¸\n",
    "# ===========================\n",
    "def load_model_and_predict(model_path, encoders_path, features_path, input_data):\n",
    "    \"\"\"è¼‰å…¥æ¨¡å‹ä¸¦é€²è¡Œé æ¸¬\"\"\"\n",
    "    print(\"\\n=== è¼‰å…¥æ¨¡å‹ä¸¦é æ¸¬ ===\")\n",
    "    \n",
    "    # è¼‰å…¥æ¨¡å‹å’Œå…ƒä»¶\n",
    "    model = joblib.load(model_path)\n",
    "    label_encoders = joblib.load(encoders_path)\n",
    "    feature_names = joblib.load(features_path)\n",
    "    \n",
    "    print(f\"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ\")\n",
    "    print(f\"   ç‰¹å¾µæ•¸é‡: {len(feature_names)}\")\n",
    "    \n",
    "    # æª¢æŸ¥è¼¸å…¥è³‡æ–™\n",
    "    missing_features = [f for f in feature_names if f not in input_data]\n",
    "    if missing_features:\n",
    "        print(f\"âŒ ç¼ºå°‘ä»¥ä¸‹ç‰¹å¾µ: {missing_features}\")\n",
    "        return None\n",
    "    \n",
    "    # æº–å‚™ç‰¹å¾µå‘é‡\n",
    "    X_new = pd.DataFrame([input_data])[feature_names]\n",
    "    \n",
    "    # é€²è¡Œé æ¸¬\n",
    "    prediction = model.predict(X_new)[0]\n",
    "    prediction_proba = model.predict_proba(X_new)[0]\n",
    "    \n",
    "    # è½‰æ›å›åŸå§‹çš„åš´é‡åº¦ç´šåˆ¥\n",
    "    severity_level = prediction + 1\n",
    "    \n",
    "    print(f\"\\né æ¸¬çµæœ:\")\n",
    "    print(f\"ğŸš¨ åš´é‡åº¦ç´šåˆ¥: {severity_level}\")\n",
    "    print(f\"\\nå„ç´šåˆ¥æ©Ÿç‡:\")\n",
    "    for i, prob in enumerate(prediction_proba):\n",
    "        bar = 'â–ˆ' * int(prob * 20)\n",
    "        print(f\"  ç´šåˆ¥ {i+1}: {prob:6.2%} {bar}\")\n",
    "    \n",
    "    return severity_level, prediction_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯„ä¾‹é æ¸¬ï¼ˆå–æ¶ˆè¨»è§£ä»¥ä½¿ç”¨ï¼‰\n",
    "# example_input = {\n",
    "#     'Temperature(F)': 70.0,\n",
    "#     'Humidity(%)': 80.0,\n",
    "#     'Pressure(in)': 29.95,\n",
    "#     'Visibility(mi)': 10.0,\n",
    "#     'Wind_Speed(mph)': 5.0,\n",
    "#     'Distance(mi)': 1.0,\n",
    "#     'Hour': 14,\n",
    "#     'DayOfWeek': 1,\n",
    "#     'Month': 6,\n",
    "#     'Year': 2023,\n",
    "#     'IsWeekend': 0,\n",
    "#     'Duration_minutes': 30.0,\n",
    "#     # ... å…¶ä»–ç·¨ç¢¼å¾Œçš„ç‰¹å¾µ\n",
    "# }\n",
    "# \n",
    "# severity, proba = load_model_and_predict(\n",
    "#     './model_output/random_forest_model.pkl',\n",
    "#     './model_output/label_encoders.pkl',\n",
    "#     './model_output/feature_names.pkl',\n",
    "#     example_input\n",
    "# )\n",
    "\n",
    "print(\"\\nâœ… å®Œæˆ! æ¨¡å‹è¨“ç·´å’Œå„²å­˜æˆåŠŸã€‚\")\n",
    "print(\"   æ‚¨å¯ä»¥ä½¿ç”¨ load_model_and_predict å‡½æ•¸ä¾†é€²è¡Œæ–°çš„é æ¸¬ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
