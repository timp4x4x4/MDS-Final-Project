{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63100b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 1: 導入套件\n",
    "# ===========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 確保已安裝所需套件\n",
    "required_packages = {\n",
    "    'pandas': pd,\n",
    "    'numpy': np,\n",
    "    'matplotlib': matplotlib,\n",
    "    'seaborn': sns,\n",
    "}\n",
    "\n",
    "print(\"檢查套件安裝狀態...\")\n",
    "for package_name, package in required_packages.items():\n",
    "    try:\n",
    "        print(f\"✅ {package_name} 版本: {package.__version__}\")\n",
    "    except:\n",
    "        print(f\"❌ {package_name} 未安裝\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc2a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 2: 導入機器學習相關套件\n",
    "# ===========================\n",
    "try:\n",
    "    # Scikit-learn\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "    print(\"✅ Scikit-learn 套件載入成功\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Scikit-learn 載入失敗: {e}\")\n",
    "\n",
    "try:\n",
    "    # 處理不平衡資料\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from imblearn.combine import SMOTEENN\n",
    "    print(\"✅ Imbalanced-learn 套件載入成功\")\n",
    "except ImportError:\n",
    "    print(\"❌ Imbalanced-learn 未安裝，請執行: pip install imbalanced-learn\")\n",
    "\n",
    "try:\n",
    "    # GPU加速的模型\n",
    "    import xgboost as xgb\n",
    "    print(f\"✅ XGBoost 版本: {xgb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"❌ XGBoost 未安裝，請執行: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    print(f\"✅ LightGBM 版本: {lgb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"❌ LightGBM 未安裝，請執行: pip install lightgbm\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    print(\"✅ CatBoost 載入成功\")\n",
    "except ImportError:\n",
    "    print(\"❌ CatBoost 未安裝，請執行: pip install catboost\")\n",
    "\n",
    "try:\n",
    "    # PyTorch\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    print(f\"✅ PyTorch 版本: {torch.__version__}\")\n",
    "    print(f\"   CUDA 可用: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError:\n",
    "    print(\"❌ PyTorch 未安裝，請參考 https://pytorch.org 安裝\")\n",
    "\n",
    "# 其他工具\n",
    "import joblib\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735a0244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 3: 載入資料\n",
    "# ===========================\n",
    "def load_data(file_path):\n",
    "    \"\"\"載入資料並顯示基本資訊\"\"\"\n",
    "    print(f\"正在載入資料: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # 先檢查檔案是否存在\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"❌ 錯誤: 找不到檔案 {file_path}\")\n",
    "            return None\n",
    "            \n",
    "        # 載入資料\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        print(f\"✅ 資料載入成功!\")\n",
    "        print(f\"   資料集大小: {df.shape}\")\n",
    "        print(f\"   記憶體使用: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # 顯示前幾筆資料\n",
    "        print(\"\\n前5筆資料:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # 顯示欄位資訊\n",
    "        print(f\"\\n欄位總數: {len(df.columns)}\")\n",
    "        print(\"欄位列表:\")\n",
    "        for i, col in enumerate(df.columns):\n",
    "            if i < 10:  # 只顯示前10個\n",
    "                print(f\"   {i+1}. {col}\")\n",
    "        print(\"   ...\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 載入資料時發生錯誤: {e}\")\n",
    "        return None\n",
    "\n",
    "# 執行載入 (請修改為你的檔案路徑)\n",
    "file_path = 'us-accidents/US_Accidents_March23.csv'\n",
    "df = load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba9284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 4: 資料基本探索\n",
    "# ===========================\n",
    "if df is not None:\n",
    "    print(\"=== 資料基本統計 ===\")\n",
    "    print(f\"\\n目標變數 (Severity) 分布:\")\n",
    "    print(df['Severity'].value_counts().sort_index())\n",
    "    \n",
    "    # 檢查資料型態\n",
    "    print(\"\\n資料型態統計:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    # 數值型欄位統計\n",
    "    print(\"\\n數值型欄位描述統計:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print(f\"數值型欄位數量: {len(numeric_cols)}\")\n",
    "    \n",
    "    # 檢查缺失值\n",
    "    print(\"\\n缺失值概覽:\")\n",
    "    missing_summary = df.isnull().sum()\n",
    "    missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "    print(f\"有缺失值的欄位數: {len(missing_summary)}\")\n",
    "    print(missing_summary.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edd9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 5: 日期時間處理函數\n",
    "# ===========================\n",
    "def process_datetime_columns(df):\n",
    "    \"\"\"處理日期時間欄位\"\"\"\n",
    "    if df is None:\n",
    "        print(\"❌ 資料框為空\")\n",
    "        return None\n",
    "        \n",
    "    print(\"\\n=== 處理日期時間欄位 ===\")\n",
    "    df_copy = df.copy()  # 避免修改原始資料\n",
    "    \n",
    "    # 日期時間欄位\n",
    "    date_columns = ['Start_Time', 'End_Time', 'Weather_Timestamp']\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if col in df_copy.columns:\n",
    "            print(f\"\\n處理 {col}...\")\n",
    "            \n",
    "            # 顯示原始資料範例\n",
    "            print(f\"原始資料範例: {df_copy[col].iloc[0]}\")\n",
    "            \n",
    "            # 轉換為日期時間\n",
    "            df_copy[col] = pd.to_datetime(df_copy[col], errors='coerce')\n",
    "            \n",
    "            # 檢查轉換結果\n",
    "            null_count = df_copy[col].isnull().sum()\n",
    "            if null_count > 0:\n",
    "                print(f\"⚠️  警告: {col} 有 {null_count} 筆無法解析的日期\")\n",
    "            else:\n",
    "                print(f\"✅ {col} 轉換成功\")\n",
    "                print(f\"   日期範圍: {df_copy[col].min()} 到 {df_copy[col].max()}\")\n",
    "    \n",
    "    # 計算持續時間\n",
    "    if 'Start_Time' in df_copy.columns and 'End_Time' in df_copy.columns:\n",
    "        print(\"\\n計算事故持續時間...\")\n",
    "        df_copy['Duration_minutes'] = (df_copy['End_Time'] - df_copy['Start_Time']).dt.total_seconds() / 60\n",
    "        \n",
    "        # 顯示統計\n",
    "        print(f\"持續時間統計:\")\n",
    "        print(f\"  最小值: {df_copy['Duration_minutes'].min():.2f} 分鐘\")\n",
    "        print(f\"  最大值: {df_copy['Duration_minutes'].max():.2f} 分鐘\") \n",
    "        print(f\"  平均值: {df_copy['Duration_minutes'].mean():.2f} 分鐘\")\n",
    "        print(f\"  中位數: {df_copy['Duration_minutes'].median():.2f} 分鐘\")\n",
    "        \n",
    "        # 過濾異常值\n",
    "        original_len = len(df_copy)\n",
    "        df_copy = df_copy[(df_copy['Duration_minutes'] > 0) & (df_copy['Duration_minutes'] < 1440)]\n",
    "        filtered_len = len(df_copy)\n",
    "        print(f\"\\n過濾異常持續時間: {original_len} → {filtered_len} (移除 {original_len - filtered_len} 筆)\")\n",
    "    \n",
    "    print(f\"\\n處理後資料大小: {df_copy.shape}\")\n",
    "    return df_copy\n",
    "\n",
    "# 執行日期處理\n",
    "df_processed = process_datetime_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 6: 特徵工程\n",
    "# ===========================\n",
    "def feature_engineering(df):\n",
    "    \"\"\"創建新特徵\"\"\"\n",
    "    if df is None:\n",
    "        print(\"❌ 資料框為空\")\n",
    "        return None\n",
    "        \n",
    "    print(\"\\n=== 執行特徵工程 ===\")\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # 時間相關特徵\n",
    "    if 'Start_Time' in df_copy.columns:\n",
    "        # 確保Start_Time不為null\n",
    "        df_copy = df_copy[df_copy['Start_Time'].notna()]\n",
    "        print(f\"移除Start_Time為null的記錄後: {len(df_copy)} 筆\")\n",
    "        \n",
    "        # 提取時間特徵\n",
    "        print(\"創建時間特徵...\")\n",
    "        df_copy['Hour'] = df_copy['Start_Time'].dt.hour\n",
    "        df_copy['DayOfWeek'] = df_copy['Start_Time'].dt.dayofweek\n",
    "        df_copy['Month'] = df_copy['Start_Time'].dt.month\n",
    "        df_copy['Year'] = df_copy['Start_Time'].dt.year\n",
    "        df_copy['IsWeekend'] = (df_copy['DayOfWeek'] >= 5).astype(int)\n",
    "        \n",
    "        # 時段分類\n",
    "        df_copy['TimeOfDay'] = pd.cut(df_copy['Hour'], \n",
    "                                      bins=[-1, 6, 12, 18, 24], \n",
    "                                      labels=['Night', 'Morning', 'Afternoon', 'Evening'])\n",
    "        \n",
    "        # 季節\n",
    "        df_copy['Season'] = pd.cut(df_copy['Month'], \n",
    "                                   bins=[0, 3, 6, 9, 12], \n",
    "                                   labels=['Winter', 'Spring', 'Summer', 'Fall'])\n",
    "        \n",
    "        print(\"✅ 時間特徵創建完成\")\n",
    "    \n",
    "    # 天氣條件簡化\n",
    "    if 'Weather_Condition' in df_copy.columns:\n",
    "        print(\"\\n處理天氣條件...\")\n",
    "        \n",
    "        weather_keywords = {\n",
    "            'Clear': ['Clear', 'Fair'],\n",
    "            'Cloudy': ['Cloud', 'Overcast'],\n",
    "            'Rain': ['Rain', 'Drizzle', 'Shower'],\n",
    "            'Snow': ['Snow', 'Sleet', 'Hail'],\n",
    "            'Fog': ['Fog', 'Mist'],\n",
    "            'Storm': ['Storm', 'Thunder']\n",
    "        }\n",
    "        \n",
    "        def categorize_weather(condition):\n",
    "            if pd.isna(condition):\n",
    "                return 'Unknown'\n",
    "            condition = str(condition)\n",
    "            for category, keywords in weather_keywords.items():\n",
    "                if any(keyword in condition for keyword in keywords):\n",
    "                    return category\n",
    "            return 'Other'\n",
    "        \n",
    "        df_copy['Weather_Category'] = df_copy['Weather_Condition'].apply(categorize_weather)\n",
    "        \n",
    "        # 顯示天氣分類結果\n",
    "        print(\"天氣類別分布:\")\n",
    "        print(df_copy['Weather_Category'].value_counts())\n",
    "    \n",
    "    print(f\"\\n特徵工程完成，資料大小: {df_copy.shape}\")\n",
    "    return df_copy\n",
    "\n",
    "# 執行特徵工程\n",
    "df_featured = feature_engineering(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c832aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 7: 缺失值處理\n",
    "# ===========================\n",
    "def handle_missing_values(df, missing_threshold=60):\n",
    "    \"\"\"處理缺失值\"\"\"\n",
    "    if df is None:\n",
    "        print(\"❌ 資料框為空\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"\\n=== 處理缺失值 (閾值: {missing_threshold}%) ===\")\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # 計算缺失值百分比\n",
    "    missing_percentage = (df_copy.isnull().sum() / len(df_copy)) * 100\n",
    "    \n",
    "    # 找出高缺失率欄位\n",
    "    high_missing_cols = missing_percentage[missing_percentage > missing_threshold].index.tolist()\n",
    "    \n",
    "    if len(high_missing_cols) > 0:\n",
    "        print(f\"\\n刪除高缺失率欄位 ({len(high_missing_cols)} 個):\")\n",
    "        for col in high_missing_cols[:10]:  # 顯示前10個\n",
    "            print(f\"  - {col}: {missing_percentage[col]:.1f}%\")\n",
    "        if len(high_missing_cols) > 10:\n",
    "            print(f\"  ... 還有 {len(high_missing_cols) - 10} 個欄位\")\n",
    "        \n",
    "        df_copy = df_copy.drop(columns=high_missing_cols)\n",
    "    \n",
    "    # 填補剩餘缺失值\n",
    "    print(\"\\n填補剩餘缺失值...\")\n",
    "    \n",
    "    # 數值型欄位用中位數填補\n",
    "    numeric_cols = df_copy.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_copy[col].isnull().sum() > 0:\n",
    "            median_value = df_copy[col].median()\n",
    "            df_copy[col].fillna(median_value, inplace=True)\n",
    "            print(f\"  {col}: 用中位數 {median_value:.2f} 填補\")\n",
    "    \n",
    "    # 類別型欄位用眾數填補\n",
    "    categorical_cols = df_copy.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if df_copy[col].isnull().sum() > 0:\n",
    "            mode_value = df_copy[col].mode()[0] if len(df_copy[col].mode()) > 0 else 'Unknown'\n",
    "            df_copy[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"  {col}: 用眾數 '{mode_value}' 填補\")\n",
    "    \n",
    "    print(f\"\\n處理後資料大小: {df_copy.shape}\")\n",
    "    print(f\"剩餘缺失值總數: {df_copy.isnull().sum().sum()}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# 執行缺失值處理\n",
    "df_cleaned = handle_missing_values(df_featured, missing_threshold=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e2c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 8: 特徵選擇與編碼\n",
    "# ===========================\n",
    "def select_and_encode_features(df):\n",
    "    \"\"\"選擇特徵並進行編碼\"\"\"\n",
    "    if df is None:\n",
    "        print(\"❌ 資料框為空\")\n",
    "        return None, None, None\n",
    "        \n",
    "    print(\"\\n=== 特徵選擇與編碼 ===\")\n",
    "    \n",
    "    # 定義特徵類型\n",
    "    numeric_features = [\n",
    "        'Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)',\n",
    "        'Wind_Speed(mph)', 'Distance(mi)', 'Hour', 'DayOfWeek', 'Month', \n",
    "        'Year', 'IsWeekend'\n",
    "    ]\n",
    "    \n",
    "    categorical_features = [\n",
    "        'Side', 'State', 'Weather_Category', 'TimeOfDay', 'Season',\n",
    "        'Sunrise_Sunset'\n",
    "    ]\n",
    "    \n",
    "    boolean_features = [\n",
    "        'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit',\n",
    "        'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', \n",
    "        'Traffic_Signal'\n",
    "    ]\n",
    "    \n",
    "    # 加入Duration_minutes如果存在\n",
    "    if 'Duration_minutes' in df.columns:\n",
    "        numeric_features.append('Duration_minutes')\n",
    "    \n",
    "    # 檢查並過濾存在的特徵\n",
    "    print(\"\\n檢查特徵存在性...\")\n",
    "    numeric_features = [f for f in numeric_features if f in df.columns]\n",
    "    categorical_features = [f for f in categorical_features if f in df.columns]\n",
    "    boolean_features = [f for f in boolean_features if f in df.columns]\n",
    "    \n",
    "    print(f\"數值型特徵: {len(numeric_features)} 個\")\n",
    "    print(f\"類別型特徵: {len(categorical_features)} 個\")\n",
    "    print(f\"布林型特徵: {len(boolean_features)} 個\")\n",
    "    \n",
    "    # 複製資料\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # 處理布林特徵\n",
    "    print(\"\\n處理布林特徵...\")\n",
    "    for col in boolean_features:\n",
    "        df_encoded[col] = df_encoded[col].astype(int)\n",
    "    \n",
    "    # 編碼類別特徵\n",
    "    print(\"\\n編碼類別特徵...\")\n",
    "    label_encoders = {}\n",
    "    encoded_features = numeric_features + boolean_features\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        print(f\"  編碼 {col}...\")\n",
    "        le = LabelEncoder()\n",
    "        encoded_col_name = col + '_encoded'\n",
    "        \n",
    "        # 處理可能的null值\n",
    "        df_encoded[col] = df_encoded[col].fillna('Unknown').astype(str)\n",
    "        df_encoded[encoded_col_name] = le.fit_transform(df_encoded[col])\n",
    "        \n",
    "        label_encoders[col] = le\n",
    "        encoded_features.append(encoded_col_name)\n",
    "        \n",
    "        print(f\"    類別數: {len(le.classes_)}\")\n",
    "    \n",
    "    print(f\"\\n總特徵數: {len(encoded_features)}\")\n",
    "    \n",
    "    return df_encoded, encoded_features, label_encoders\n",
    "\n",
    "# 執行特徵編碼\n",
    "df_encoded, selected_features, label_encoders = select_and_encode_features(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd271be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 9: 準備訓練資料\n",
    "# ===========================\n",
    "def prepare_training_data(df, features, target_col='Severity'):\n",
    "    \"\"\"準備最終的訓練資料\"\"\"\n",
    "    if df is None or features is None:\n",
    "        print(\"❌ 資料或特徵為空\")\n",
    "        return None, None\n",
    "        \n",
    "    print(\"\\n=== 準備訓練資料 ===\")\n",
    "    \n",
    "    # 檢查目標變數\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"❌ 找不到目標變數 '{target_col}'\")\n",
    "        return None, None\n",
    "    \n",
    "    # 檢查所有特徵是否存在\n",
    "    missing_features = [f for f in features if f not in df.columns]\n",
    "    if missing_features:\n",
    "        print(f\"❌ 缺少以下特徵: {missing_features}\")\n",
    "        return None, None\n",
    "    \n",
    "    # 選擇需要的欄位\n",
    "    required_columns = features + [target_col]\n",
    "    df_subset = df[required_columns].copy()\n",
    "    \n",
    "    # 移除含有缺失值的行\n",
    "    print(f\"處理前資料大小: {df_subset.shape}\")\n",
    "    df_clean = df_subset.dropna()\n",
    "    print(f\"移除缺失值後大小: {df_clean.shape}\")\n",
    "    \n",
    "    if len(df_clean) == 0:\n",
    "        print(\"❌ 錯誤: 移除缺失值後沒有剩餘資料!\")\n",
    "        return None, None\n",
    "    \n",
    "    # 準備X和y\n",
    "    X = df_clean[features]\n",
    "    y = df_clean[target_col]\n",
    "    \n",
    "    # 將嚴重度從1-4轉換為0-3 (sklearn要求)\n",
    "    y = y - 1\n",
    "    \n",
    "    # 顯示目標變數分布\n",
    "    print(\"\\n目標變數分布:\")\n",
    "    value_counts = pd.Series(y).value_counts().sort_index()\n",
    "    for severity, count in value_counts.items():\n",
    "        print(f\"  嚴重度 {severity+1}: {count} ({count/len(y)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n最終訓練資料大小: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 準備訓練資料\n",
    "X, y = prepare_training_data(df_encoded, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe0b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 10: 分割資料集\n",
    "# ===========================\n",
    "if X is not None and y is not None:\n",
    "    print(\"\\n=== 分割訓練集和測試集 ===\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"訓練集大小: {X_train.shape}\")\n",
    "    print(f\"測試集大小: {X_test.shape}\")\n",
    "    \n",
    "    # 檢查分割後的類別分布\n",
    "    print(\"\\n訓練集類別分布:\")\n",
    "    train_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "    for severity, count in train_dist.items():\n",
    "        print(f\"  嚴重度 {severity+1}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n測試集類別分布:\")\n",
    "    test_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "    for severity, count in test_dist.items():\n",
    "        print(f\"  嚴重度 {severity+1}: {count} ({count/len(y_test)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"❌ 無法分割資料集，請檢查前面的步驟\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 11: 定義模型訓練函數\n",
    "# ===========================\n",
    "def train_random_forest(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"訓練隨機森林模型\"\"\"\n",
    "    print(\"\\n訓練 Random Forest...\")\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 預測\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 評估\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"訓練時間: {train_time:.2f} 秒\")\n",
    "    print(f\"準確率: {accuracy:.4f}\")\n",
    "    print(f\"F1分數: {f1:.4f}\")\n",
    "    \n",
    "    # 顯示分類報告\n",
    "    print(\"\\n分類報告:\")\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                              target_names=[f'Severity {i+1}' for i in range(4)]))\n",
    "    \n",
    "    return model, accuracy, f1\n",
    "\n",
    "def train_xgboost(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"訓練XGBoost模型\"\"\"\n",
    "    print(\"\\n訓練 XGBoost...\")\n",
    "    \n",
    "    # 檢查是否有GPU\n",
    "    use_gpu = torch.cuda.is_available() if 'torch' in globals() else False\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        objective='multi:softprob',\n",
    "        n_jobs=-1,\n",
    "        tree_method='gpu_hist' if use_gpu else 'hist',\n",
    "        predictor='gpu_predictor' if use_gpu else 'cpu_predictor',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 預測\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 評估\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"使用GPU: {use_gpu}\")\n",
    "    print(f\"訓練時間: {train_time:.2f} 秒\")\n",
    "    print(f\"準確率: {accuracy:.4f}\")\n",
    "    print(f\"F1分數: {f1:.4f}\")\n",
    "    \n",
    "    return model, accuracy, f1\n",
    "\n",
    "def train_lightgbm(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"訓練LightGBM模型\"\"\"\n",
    "    print(\"\\n訓練 LightGBM...\")\n",
    "    \n",
    "    # 檢查是否有GPU\n",
    "    use_gpu = torch.cuda.is_available() if 'torch' in globals() else False\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 4,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'multi_logloss',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 100,\n",
    "        'device': 'gpu' if use_gpu else 'cpu',\n",
    "        'random_state': 42,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    if use_gpu:\n",
    "        params.update({\n",
    "            'gpu_platform_id': 0,\n",
    "            'gpu_device_id': 0\n",
    "        })\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 預測\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 評估\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"使用GPU: {use_gpu}\")\n",
    "    print(f\"訓練時間: {train_time:.2f} 秒\")\n",
    "    print(f\"準確率: {accuracy:.4f}\")\n",
    "    print(f\"F1分數: {f1:.4f}\")\n",
    "    \n",
    "    return model, accuracy, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 12: 訓練模型\n",
    "# ===========================\n",
    "if 'X_train' in globals() and X_train is not None:\n",
    "    results = {}\n",
    "    models = {}\n",
    "    \n",
    "    # 1. Random Forest\n",
    "    try:\n",
    "        rf_model, rf_acc, rf_f1 = train_random_forest(X_train, X_test, y_train, y_test)\n",
    "        results['Random_Forest'] = {'accuracy': rf_acc, 'f1': rf_f1}\n",
    "        models['Random_Forest'] = rf_model\n",
    "    except Exception as e:\n",
    "        print(f\"Random Forest 訓練失敗: {e}\")\n",
    "    \n",
    "    # 2. XGBoost\n",
    "    try:\n",
    "        if 'xgb' in globals():\n",
    "            xgb_model, xgb_acc, xgb_f1 = train_xgboost(X_train, X_test, y_train, y_test)\n",
    "            results['XGBoost'] = {'accuracy': xgb_acc, 'f1': xgb_f1}\n",
    "            models['XGBoost'] = xgb_model\n",
    "    except Exception as e:\n",
    "        print(f\"XGBoost 訓練失敗: {e}\")\n",
    "    \n",
    "    # 3. LightGBM\n",
    "    try:\n",
    "        if 'lgb' in globals():\n",
    "            lgb_model, lgb_acc, lgb_f1 = train_lightgbm(X_train, X_test, y_train, y_test)\n",
    "            results['LightGBM'] = {'accuracy': lgb_acc, 'f1': lgb_f1}\n",
    "            models['LightGBM'] = lgb_model\n",
    "    except Exception as e:\n",
    "        print(f\"LightGBM 訓練失敗: {e}\")\n",
    "else:\n",
    "    print(\"❌ 請先執行前面的資料準備步驟\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630fb8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 13: 模型比較\n",
    "# ===========================\n",
    "if 'results' in globals() and results:\n",
    "    print(\"\\n=== 模型性能比較 ===\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'模型':<15} {'準確率':<10} {'F1分數':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"{model_name:<15} {metrics['accuracy']:<10.4f} {metrics['f1']:<10.4f}\")\n",
    "    \n",
    "    # 找出最佳模型\n",
    "    best_model_name = max(results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "    print(f\"\\n🏆 最佳模型: {best_model_name}\")\n",
    "    print(f\"   準確率: {results[best_model_name]['accuracy']:.4f}\")\n",
    "    print(f\"   F1分數: {results[best_model_name]['f1']:.4f}\")\n",
    "    \n",
    "    best_model = models[best_model_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daffed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 14: 特徵重要性分析\n",
    "# ===========================\n",
    "if 'best_model' in globals() and hasattr(best_model, 'feature_importances_'):\n",
    "    print(\"\\n=== 特徵重要性分析 ===\")\n",
    "    \n",
    "    # 取得特徵重要性\n",
    "    importances = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # 顯示前20個重要特徵\n",
    "    print(\"\\nTop 20 重要特徵:\")\n",
    "    print(importances.head(20))\n",
    "    \n",
    "    # 視覺化\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_n = 20\n",
    "    plt.barh(importances.head(top_n)['feature'][::-1], \n",
    "             importances.head(top_n)['importance'][::-1])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top {top_n} Feature Importances ({best_model_name})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ab737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 15: 儲存模型\n",
    "# ===========================\n",
    "def save_model_and_components(model, model_name, label_encoders, feature_names, output_dir='./model_output/'):\n",
    "    \"\"\"儲存模型和相關元件\"\"\"\n",
    "    print(f\"\\n=== 儲存模型: {model_name} ===\")\n",
    "    \n",
    "    # 建立輸出目錄\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 儲存模型\n",
    "    model_path = os.path.join(output_dir, f'{model_name.lower()}_model.pkl')\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"✅ 模型已儲存至: {model_path}\")\n",
    "    \n",
    "    # 儲存標籤編碼器\n",
    "    encoders_path = os.path.join(output_dir, 'label_encoders.pkl')\n",
    "    joblib.dump(label_encoders, encoders_path)\n",
    "    print(f\"✅ 標籤編碼器已儲存至: {encoders_path}\")\n",
    "    \n",
    "    # 儲存特徵名稱\n",
    "    features_path = os.path.join(output_dir, 'feature_names.pkl')\n",
    "    joblib.dump(feature_names, features_path)\n",
    "    print(f\"✅ 特徵名稱已儲存至: {features_path}\")\n",
    "    \n",
    "    # 儲存模型資訊\n",
    "    model_info = {\n",
    "        'model_name': model_name,\n",
    "        'n_features': len(feature_names),\n",
    "        'feature_names': feature_names,\n",
    "        'accuracy': results[model_name]['accuracy'],\n",
    "        'f1_score': results[model_name]['f1'],\n",
    "        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    info_path = os.path.join(output_dir, 'model_info.json')\n",
    "    import json\n",
    "    with open(info_path, 'w') as f:\n",
    "        json.dump(model_info, f, indent=4)\n",
    "    print(f\"✅ 模型資訊已儲存至: {info_path}\")\n",
    "\n",
    "# 儲存最佳模型\n",
    "if 'best_model' in globals() and 'best_model_name' in globals():\n",
    "    save_model_and_components(best_model, best_model_name, label_encoders, selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86cb565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Cell 16: 定義預測函數\n",
    "# ===========================\n",
    "def load_model_and_predict(model_path, encoders_path, features_path, input_data):\n",
    "    \"\"\"載入模型並進行預測\"\"\"\n",
    "    print(\"\\n=== 載入模型並預測 ===\")\n",
    "    \n",
    "    # 載入模型和元件\n",
    "    model = joblib.load(model_path)\n",
    "    label_encoders = joblib.load(encoders_path)\n",
    "    feature_names = joblib.load(features_path)\n",
    "    \n",
    "    print(f\"✅ 模型載入成功\")\n",
    "    print(f\"   特徵數量: {len(feature_names)}\")\n",
    "    \n",
    "    # 檢查輸入資料\n",
    "    missing_features = [f for f in feature_names if f not in input_data]\n",
    "    if missing_features:\n",
    "        print(f\"❌ 缺少以下特徵: {missing_features}\")\n",
    "        return None\n",
    "    \n",
    "    # 準備特徵向量\n",
    "    X_new = pd.DataFrame([input_data])[feature_names]\n",
    "    \n",
    "    # 進行預測\n",
    "    prediction = model.predict(X_new)[0]\n",
    "    prediction_proba = model.predict_proba(X_new)[0]\n",
    "    \n",
    "    # 轉換回原始的嚴重度級別\n",
    "    severity_level = prediction + 1\n",
    "    \n",
    "    print(f\"\\n預測結果:\")\n",
    "    print(f\"🚨 嚴重度級別: {severity_level}\")\n",
    "    print(f\"\\n各級別機率:\")\n",
    "    for i, prob in enumerate(prediction_proba):\n",
    "        bar = '█' * int(prob * 20)\n",
    "        print(f\"  級別 {i+1}: {prob:6.2%} {bar}\")\n",
    "    \n",
    "    return severity_level, prediction_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 範例預測（取消註解以使用）\n",
    "# example_input = {\n",
    "#     'Temperature(F)': 70.0,\n",
    "#     'Humidity(%)': 80.0,\n",
    "#     'Pressure(in)': 29.95,\n",
    "#     'Visibility(mi)': 10.0,\n",
    "#     'Wind_Speed(mph)': 5.0,\n",
    "#     'Distance(mi)': 1.0,\n",
    "#     'Hour': 14,\n",
    "#     'DayOfWeek': 1,\n",
    "#     'Month': 6,\n",
    "#     'Year': 2023,\n",
    "#     'IsWeekend': 0,\n",
    "#     'Duration_minutes': 30.0,\n",
    "#     # ... 其他編碼後的特徵\n",
    "# }\n",
    "# \n",
    "# severity, proba = load_model_and_predict(\n",
    "#     './model_output/random_forest_model.pkl',\n",
    "#     './model_output/label_encoders.pkl',\n",
    "#     './model_output/feature_names.pkl',\n",
    "#     example_input\n",
    "# )\n",
    "\n",
    "print(\"\\n✅ 完成! 模型訓練和儲存成功。\")\n",
    "print(\"   您可以使用 load_model_and_predict 函數來進行新的預測。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
